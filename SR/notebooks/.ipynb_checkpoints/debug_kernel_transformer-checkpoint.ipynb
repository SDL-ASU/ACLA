{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import DropPath\n",
    "import torch.nn.functional as F\n",
    "# two arguments are added compared to vanilla attention from ./models/token_transformer.py\n",
    "# num_tokens: number of tokens\n",
    "# head_separate: If True, we define separate weight matrices for different heads.\n",
    "#                Else, we define one single weight matrix for different heads.\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "def calculate_local_3x3_index_list(token_map_height, token_map_width):\n",
    "    num_tokens = token_map_height * token_map_width\n",
    "    local_pos_list = []\n",
    "    all_local_index_list = []\n",
    "\n",
    "    for row in range(token_map_height):\n",
    "        for col in range(token_map_width):\n",
    "            local_pos = []\n",
    "            for i in [-1, 0, 1]:\n",
    "                for j in [-1, 0, 1]:\n",
    "                    local_pos.append([row + i, col + j])\n",
    "            local_pos_list.append(local_pos)\n",
    "\n",
    "    for index in range(num_tokens):\n",
    "        each_local_index_list = []\n",
    "        for local_pos in local_pos_list[index]:\n",
    "            if local_pos[0] in range(token_map_height) and local_pos[1] in range(token_map_width):\n",
    "                local_index = local_pos[0] * token_map_width + local_pos[1]\n",
    "                each_local_index_list.append(local_index)\n",
    "            else:\n",
    "                local_index = num_tokens\n",
    "                each_local_index_list.append(local_index)\n",
    "        all_local_index_list.append(each_local_index_list)\n",
    "\n",
    "    return all_local_index_list\n",
    "\n",
    "\n",
    "class Kernel_3x3_Convolutional_Attention_t2t(nn.Module):\n",
    "    def __init__(self, dim, num_tokens, token_map_height, token_map_width, head_separate=False, abs_kernel_size=9, num_heads=8, in_dim=None, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.num_tokens = num_tokens\n",
    "        self.token_map_height = token_map_height\n",
    "        self.token_map_width = token_map_width\n",
    "        self.in_dim = in_dim\n",
    "        self.head_sep = head_separate\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.abs_kernel_size = 9\n",
    "        self.qkv = nn.Linear(dim, in_dim * 3, bias=qkv_bias)\n",
    "        self.local_attn_drop = nn.Dropout(attn_drop)\n",
    "        self.global_attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(in_dim, in_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        # we can initialize the mask here by replacing torch.ones\n",
    "        self.local_3x3_kernel_weight = nn.Parameter(torch.ones(num_tokens, abs_kernel_size), requires_grad=False)\n",
    "        self.calculate_local_3x3_index_tensor()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.in_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        # shape of Q, K and V (B, self.num_heads, N, self.in_dim)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        padded_attn = F.pad(input=attn, pad=[0, 1], mode='constant', value=0)\n",
    "\n",
    "        filterd_attention_map = self.batch_attention_filter(padded_attn)\n",
    "\n",
    "        weighted_filterd_attention_map = torch.mul(filterd_attention_map, self.local_3x3_kernel_weight)\n",
    "\n",
    "        index_expand = self.local_index.unsqueeze(0).unsqueeze(0).expand(B, self.num_heads, self.num_tokens, self.abs_kernel_size)\n",
    "\n",
    "        local_attn = torch.zeros((B, self.num_heads, self.num_tokens, self.num_tokens+1)).scatter_(-1, index_expand.to(torch.int64), weighted_filterd_attention_map)\n",
    "\n",
    "        local_attn = local_attn[:, :, :, :-1]\n",
    "\n",
    "        local_attn = local_attn.softmax(dim=-1)\n",
    "        global_attn = attn.softmax(dim=-1)\n",
    "        # shape of attn : B, num_heads, num_tokens, num_tokens\n",
    "        local_attn = self.local_attn_drop(local_attn)\n",
    "        global_attn = self.global_attn_drop(global_attn)\n",
    "\n",
    "        # filtered_v = self.v_filter(v)\n",
    "\n",
    "        x_local = (local_attn @ v).transpose(1, 2).reshape(B, N, self.in_dim)\n",
    "        x_global = (global_attn @ v).transpose(1, 2).reshape(B, N, self.in_dim)\n",
    "        x = x_local + x_global\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        # skip connection\n",
    "        x = v.squeeze(1) + x   # because the original x has different size with current x, use v to do skip connection\n",
    "\n",
    "        return x\n",
    "\n",
    "    def batch_attention_filter(self, attention_map):\n",
    "        local_index = self.local_index\n",
    "        num_batch, num_heads, N1, N2 = attention_map.shape\n",
    "        index_expand = local_index.unsqueeze(0).expand(num_batch * num_heads, local_index.shape[0], local_index.shape[1])\n",
    "        filterd_attention_map = torch.gather(attention_map.view(num_batch * num_heads, N1, N2), 2, index_expand.to(torch.int64))\n",
    "        return filterd_attention_map.view(num_batch, num_heads, filterd_attention_map.shape[-2], filterd_attention_map.shape[-1])\n",
    "\n",
    "    def calculate_local_3x3_index_tensor(self):\n",
    "        local_index_tensor = torch.zeros([self.num_tokens, self.abs_kernel_size])\n",
    "\n",
    "        all_local_index_list = calculate_local_3x3_index_list(self.token_map_height, self.token_map_width)\n",
    "\n",
    "        for id in range(0, self.num_tokens):\n",
    "            for local_id in range(self.abs_kernel_size):\n",
    "                local_index_tensor[id, local_id] = all_local_index_list[id][local_id]\n",
    "\n",
    "        self.local_index = local_index_tensor\n",
    "\n",
    "\n",
    "    def calculate_local_3x3_index_list(self):\n",
    "        num_tokens = self.token_map_height * self.token_map_width\n",
    "        local_pos_list = []\n",
    "        all_local_index_list = []\n",
    "\n",
    "        for row in range(self.token_map_height):\n",
    "            for col in range(self.token_map_width):\n",
    "                local_pos = []\n",
    "                for i in [-1, 0, 1]:\n",
    "                    for j in [-1, 0, 1]:\n",
    "                        local_pos.append([row + i, col + j])\n",
    "                local_pos_list.append(local_pos)\n",
    "\n",
    "        for index in range(num_tokens):\n",
    "            each_local_index_list = []\n",
    "            for local_pos in local_pos_list[index]:\n",
    "                if local_pos[0] in range(self.token_map_height) and local_pos[1] in range(self.token_map_width):\n",
    "                    local_index = local_pos[0] * self.token_map_width + local_pos[1]\n",
    "                    each_local_index_list.append(local_index)\n",
    "\n",
    "            all_local_index_list.append(each_local_index_list)\n",
    "\n",
    "        return all_local_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dim = 64\n",
    "k_attention_1 = Kernel_3x3_Convolutional_Attention_t2t(dim=3 * 7 * 7,\n",
    "                                             num_tokens=56 * 56, token_map_height=56, token_map_width=56, \n",
    "                                             head_separate=False,in_dim=token_dim, num_heads=1)\n",
    "soft_split0 = nn.Unfold(kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
    "soft_split1 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "soft_split2 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 147, 3136])\n",
      "torch.Size([10, 3136, 147])\n",
      "torch.Size([10, 3136, 64])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(10, 3, 224, 224)\n",
    "x=soft_split0(x)\n",
    "print(x.shape)\n",
    "x=x.transpose(1, 2)\n",
    "print(x.shape)\n",
    "x=k_attention_1(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "data = torch.ones(4, 4)\n",
    "# pad(left, right, top, bottom)\n",
    "new_data = F.pad(input=data, pad=[0, 1, 0, 0], mode='constant', value=0)\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(new_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "a= torch.tensor([1,2,3,4])\n",
    "print(a[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
