Loading a binary file
Making model...
===> model summary
=========================================================================================================================================================================================
Layer (type:depth-idx)                                       Input Shape               Output Shape              Kernel Shape              Param #                   Mult-Adds
=========================================================================================================================================================================================
├─MeanShift: 1-1                                             [-1, 3, 48, 48]           [-1, 3, 48, 48]           [3, 3, 1, 1]              12                        9
├─Sequential: 1-2                                            [-1, 3, 48, 48]           [-1, 64, 48, 48]          --                        --                        --
|    └─Conv2d: 2-1                                           [-1, 3, 48, 48]           [-1, 64, 48, 48]          [3, 64, 3, 3]             1,792                     3,981,312
├─ModuleList: 1                                              []                        []                        --                        --                        --
|    └─ResidualGroup: 2-2                                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    └─Sequential: 3-1                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    └─RCAB: 4-1                                   [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-1                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-1                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-2                         [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-3                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-4                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-1       [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-2              [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-1             [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-2               [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-3             [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-4            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-2                                   [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-2                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-5                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-6                         [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-7                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-8                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-3       [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-4              [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-5             [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-6               [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-7             [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-8            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-3                                   [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-3                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-9                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-10                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-11                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-12                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-5       [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-6              [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-9             [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-10              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-11            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-12           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-4                                   [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-4                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-13                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-14                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-15                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-16                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-7       [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-8              [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-13            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-14              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-15            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-16           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-5                                   [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-5                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-17                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-18                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-19                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-20                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-9       [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-10             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-17            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-18              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-19            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-20           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-6                                   [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-6                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-21                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-22                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-23                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-24                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-11      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-12             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-21            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-22              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-23            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-24           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-7                                   [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-7                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-25                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-26                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-27                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-28                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-13      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-14             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-25            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-26              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-27            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-28           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-8                                   [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-8                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-29                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-30                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-31                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-32                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-15      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-16             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-29            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-30              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-31            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-32           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-9                                   [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-9                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-33                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-34                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-35                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-36                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-17      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-18             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-33            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-34              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-35            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-36           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-10                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-10                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-37                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-38                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-39                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-40                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-19      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-20             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-37            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-38              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-39            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-40           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-11                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-11                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-41                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-42                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-43                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-44                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-21      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-22             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-41            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-42              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-43            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-44           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-12                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-12                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-45                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-46                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-47                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-48                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-23      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-24             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-45            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-46              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-47            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-48           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-13                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-13                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-49                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-50                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-51                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-52                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-25      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-26             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-49            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-50              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-51            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-52           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-14                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-14                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-53                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-54                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-55                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-56                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-27      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-28             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-53            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-54              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-55            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-56           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-15                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-15                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-57                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-58                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-59                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-60                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-29      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-30             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-57            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-58              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-59            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-60           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-16                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-16                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-61                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-62                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-63                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-64                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-31      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-32             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-61            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-62              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-63            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-64           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-17                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-17                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-65                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-66                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-67                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-68                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-33      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-34             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-65            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-66              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-67            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-68           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-18                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-18                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-69                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-70                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-71                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-72                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-35      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-36             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-69            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-70              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-71            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-72           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-19                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-19                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-73                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-74                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-75                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-76                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-37      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-38             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-73            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-74              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-75            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-76           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-20                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-20                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-77                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-78                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-79                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-80                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-39      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-40             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-77            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-78              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-79            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-80           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─Conv2d: 4-21                                [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    └─ResidualGroup: 2-3                                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    └─Sequential: 3-2                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    └─RCAB: 4-22                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-21                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-81                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-82                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-83                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-84                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-41      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-42             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-81            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-82              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-83            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-84           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-23                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-22                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-85                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-86                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-87                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-88                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-43      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-44             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-85            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-86              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-87            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-88           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-24                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-23                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-89                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-90                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-91                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-92                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-45      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-46             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-89            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-90              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-91            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-92           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-25                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-24                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-93                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-94                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-95                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-96                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-47      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-48             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-93            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-94              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-95            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-96           [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-26                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-25                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-97                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-98                        [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-99                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-100                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-49      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-50             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-97            [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-98              [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-99            [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-100          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-27                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-26                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-101                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-102                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-103                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-104                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-51      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-52             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-101           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-102             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-103           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-104          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-28                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-27                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-105                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-106                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-107                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-108                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-53      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-54             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-105           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-106             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-107           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-108          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-29                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-28                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-109                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-110                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-111                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-112                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-55      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-56             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-109           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-110             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-111           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-112          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-30                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-29                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-113                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-114                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-115                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-116                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-57      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-58             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-113           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-114             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-115           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-116          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-31                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-30                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-117                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-118                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-119                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-120                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-59      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-60             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-117           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-118             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-119           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-120          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-32                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-31                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-121                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-122                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-123                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-124                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-61      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-62             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-121           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-122             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-123           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-124          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-33                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-32                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-125                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-126                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-127                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-128                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-63      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-64             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-125           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-126             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-127           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-128          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-34                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-33                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-129                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-130                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-131                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-132                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-65      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-66             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-129           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-130             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-131           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-132          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-35                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-34                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-133                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-134                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-135                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-136                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-67      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-68             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-133           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-134             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-135           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-136          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-36                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-35                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-137                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-138                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-139                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-140                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-69      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-70             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-137           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-138             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-139           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-140          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-37                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-36                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-141                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-142                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-143                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-144                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-71      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-72             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-141           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-142             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-143           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-144          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-38                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-37                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-145                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-146                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-147                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-148                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-73      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-74             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-145           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-146             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-147           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-148          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-39                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-38                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-149                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-150                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-151                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-152                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-75      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-76             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-149           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-150             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-151           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-152          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-40                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-39                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-153                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-154                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-155                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-156                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-77      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-78             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-153           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-154             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-155           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-156          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-41                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-40                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-157                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-158                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-159                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-160                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-79      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-80             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-157           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-158             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-159           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-160          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─Conv2d: 4-42                                [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    └─ResidualGroup: 2-4                                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    └─Sequential: 3-3                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    └─RCAB: 4-43                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-41                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-161                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-162                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-163                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-164                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-81      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-82             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-161           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-162             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-163           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-164          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-44                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-42                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-165                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-166                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-167                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-168                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-83      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-84             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-165           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-166             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-167           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-168          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-45                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-43                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-169                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-170                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-171                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-172                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-85      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-86             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-169           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-170             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-171           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-172          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-46                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-44                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-173                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-174                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-175                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-176                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-87      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-88             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-173           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-174             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-175           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-176          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-47                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-45                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-177                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-178                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-179                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-180                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-89      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-90             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-177           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-178             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-179           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-180          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-48                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-46                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-181                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-182                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-183                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-184                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-91      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-92             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-181           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-182             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-183           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-184          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-49                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-47                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-185                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-186                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-187                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-188                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-93      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-94             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-185           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-186             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-187           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-188          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-50                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-48                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-189                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-190                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-191                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-192                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-95      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-96             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-189           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-190             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-191           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-192          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-51                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-49                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-193                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-194                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-195                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-196                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-97      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-98             [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-193           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-194             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-195           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-196          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-52                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-50                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-197                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-198                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-199                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-200                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-99      [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-100            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-197           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-198             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-199           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-200          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-53                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-51                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-201                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-202                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-203                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-204                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-101     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-102            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-201           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-202             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-203           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-204          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-54                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-52                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-205                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-206                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-207                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-208                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-103     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-104            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-205           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-206             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-207           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-208          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-55                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-53                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-209                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-210                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-211                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-212                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-105     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-106            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-209           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-210             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-211           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-212          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-56                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-54                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-213                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-214                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-215                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-216                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-107     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-108            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-213           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-214             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-215           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-216          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-57                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-55                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-217                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-218                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-219                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-220                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-109     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-110            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-217           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-218             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-219           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-220          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-58                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-56                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-221                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-222                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-223                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-224                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-111     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-112            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-221           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-222             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-223           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-224          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-59                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-57                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-225                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-226                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-227                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-228                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-113     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-114            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-225           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-226             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-227           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-228          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-60                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-58                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-229                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-230                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-231                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-232                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-115     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-116            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-229           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-230             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-231           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-232          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-61                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-59                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-233                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-234                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-235                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-236                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-117     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-118            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-233           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-234             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-235           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-236          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-62                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-60                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-237                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-238                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-239                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-240                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-119     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-120            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-237           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-238             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-239           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-240          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─Conv2d: 4-63                                [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    └─ResidualGroup: 2-5                                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    └─Sequential: 3-4                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    └─RCAB: 4-64                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-61                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-241                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-242                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-243                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-244                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-121     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-122            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-241           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-242             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-243           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-244          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-65                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-62                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-245                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-246                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-247                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-248                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-123     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-124            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-245           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-246             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-247           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-248          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-66                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-63                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-249                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-250                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-251                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-252                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-125     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-126            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-249           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-250             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-251           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-252          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-67                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-64                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-253                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-254                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-255                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-256                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-127     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-128            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-253           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-254             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-255           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-256          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-68                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-65                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-257                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-258                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-259                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-260                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-129     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-130            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-257           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-258             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-259           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-260          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-69                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-66                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-261                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-262                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-263                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-264                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-131     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-132            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-261           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-262             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-263           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-264          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-70                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-67                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-265                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-266                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-267                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-268                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-133     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-134            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-265           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-266             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-267           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-268          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-71                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-68                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-269                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-270                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-271                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-272                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-135     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-136            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-269           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-270             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-271           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-272          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-72                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-69                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-273                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-274                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-275                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-276                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-137     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-138            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-273           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-274             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-275           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-276          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-73                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-70                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-277                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-278                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-279                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-280                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-139     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-140            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-277           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-278             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-279           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-280          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-74                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-71                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-281                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-282                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-283                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-284                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-141     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-142            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-281           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-282             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-283           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-284          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-75                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-72                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-285                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-286                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-287                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-288                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-143     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-144            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-285           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-286             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-287           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-288          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-76                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-73                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-289                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-290                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-291                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-292                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-145     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-146            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-289           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-290             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-291           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-292          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-77                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-74                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-293                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-294                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-295                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-296                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-147     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-148            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-293           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-294             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-295           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-296          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-78                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-75                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-297                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-298                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-299                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-300                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-149     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-150            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-297           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-298             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-299           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-300          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-79                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-76                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-301                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-302                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-303                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-304                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-151     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-152            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-301           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-302             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-303           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-304          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-80                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-77                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-305                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-306                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-307                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-308                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-153     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-154            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-305           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-306             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-307           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-308          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-81                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-78                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-309                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-310                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-311                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-312                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-155     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-156            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-309           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-310             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-311           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-312          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-82                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-79                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-313                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-314                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-315                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-316                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-157     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-158            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-313           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-314             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-315           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-316          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-83                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-80                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-317                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-318                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-319                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-320                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-159     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-160            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-317           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-318             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-319           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-320          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─Conv2d: 4-84                                [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    └─ResidualGroup: 2-6                                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    └─Sequential: 3-5                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    └─RCAB: 4-85                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-81                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-321                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-322                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-323                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-324                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-161     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-162            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-321           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-322             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-323           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-324          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-86                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-82                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-325                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-326                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-327                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-328                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-163     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-164            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-325           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-326             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-327           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-328          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-87                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-83                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-329                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-330                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-331                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-332                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-165     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-166            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-329           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-330             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-331           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-332          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-88                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-84                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-333                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-334                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-335                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-336                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-167     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-168            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-333           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-334             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-335           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-336          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-89                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-85                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-337                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-338                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-339                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-340                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-169     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-170            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-337           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-338             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-339           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-340          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-90                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-86                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-341                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-342                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-343                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-344                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-171     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-172            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-341           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-342             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-343           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-344          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-91                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-87                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-345                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-346                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-347                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-348                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-173     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-174            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-345           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-346             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-347           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-348          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-92                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-88                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-349                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-350                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-351                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-352                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-175     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-176            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-349           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-350             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-351           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-352          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-93                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-89                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-353                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-354                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-355                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-356                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-177     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-178            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-353           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-354             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-355           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-356          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-94                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-90                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-357                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-358                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-359                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-360                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-179     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-180            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-357           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-358             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-359           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-360          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-95                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-91                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-361                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-362                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-363                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-364                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-181     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-182            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-361           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-362             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-363           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-364          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-96                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-92                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-365                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-366                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-367                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-368                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-183     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-184            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-365           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-366             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-367           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-368          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-97                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-93                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-369                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-370                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-371                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-372                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-185     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-186            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-369           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-370             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-371           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-372          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-98                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-94                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-373                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-374                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-375                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-376                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-187     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-188            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-373           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-374             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-375           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-376          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-99                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-95                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-377                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-378                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-379                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-380                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-189     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-190            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-377           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-378             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-379           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-380          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-100                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-96                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-381                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-382                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-383                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-384                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-191     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-192            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-381           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-382             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-383           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-384          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-101                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-97                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-385                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-386                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-387                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-388                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-193     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-194            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-385           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-386             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-387           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-388          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-102                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-98                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-389                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-390                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-391                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-392                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-195     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-196            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-389           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-390             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-391           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-392          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-103                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-99                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-393                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-394                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-395                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-396                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-197     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-198            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-393           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-394             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-395           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-396          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-104                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-100                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-397                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-398                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-399                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-400                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-199     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-200            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-397           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-398             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-399           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-400          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─Conv2d: 4-105                               [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    └─ResidualGroup: 2-7                                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    └─Sequential: 3-6                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    └─RCAB: 4-106                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-101                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-401                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-402                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-403                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-404                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-201     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-202            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-401           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-402             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-403           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-404          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-107                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-102                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-405                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-406                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-407                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-408                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-203     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-204            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-405           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-406             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-407           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-408          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-108                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-103                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-409                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-410                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-411                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-412                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-205     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-206            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-409           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-410             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-411           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-412          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-109                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-104                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-413                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-414                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-415                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-416                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-207     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-208            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-413           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-414             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-415           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-416          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-110                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-105                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-417                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-418                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-419                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-420                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-209     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-210            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-417           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-418             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-419           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-420          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-111                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-106                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-421                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-422                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-423                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-424                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-211     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-212            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-421           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-422             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-423           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-424          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-112                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-107                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-425                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-426                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-427                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-428                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-213     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-214            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-425           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-426             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-427           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-428          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-113                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-108                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-429                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-430                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-431                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-432                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-215     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-216            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-429           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-430             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-431           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-432          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-114                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-109                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-433                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-434                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-435                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-436                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-217     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-218            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-433           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-434             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-435           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-436          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-115                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-110                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-437                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-438                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-439                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-440                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-219     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-220            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-437           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-438             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-439           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-440          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-116                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-111                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-441                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-442                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-443                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-444                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-221     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-222            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-441           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-442             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-443           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-444          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-117                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-112                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-445                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-446                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-447                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-448                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-223     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-224            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-445           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-446             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-447           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-448          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-118                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-113                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-449                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-450                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-451                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-452                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-225     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-226            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-449           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-450             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-451           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-452          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-119                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-114                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-453                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-454                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-455                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-456                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-227     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-228            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-453           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-454             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-455           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-456          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-120                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-115                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-457                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-458                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-459                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-460                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-229     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-230            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-457           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-458             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-459           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-460          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-121                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-116                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-461                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-462                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-463                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-464                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-231     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-232            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-461           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-462             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-463           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-464          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-122                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-117                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-465                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-466                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-467                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-468                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-233     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-234            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-465           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-466             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-467           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-468          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-123                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-118                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-469                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-470                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-471                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-472                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-235     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-236            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-469           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-470             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-471           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-472          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-124                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-119                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-473                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-474                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-475                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-476                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-237     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-238            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-473           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-474             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-475           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-476          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-125                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-120                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-477                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-478                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-479                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-480                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-239     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-240            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-477           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-478             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-479           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-480          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─Conv2d: 4-126                               [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    └─ResidualGroup: 2-8                                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    └─Sequential: 3-7                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    └─RCAB: 4-127                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-121                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-481                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-482                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-483                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-484                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-241     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-242            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-481           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-482             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-483           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-484          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-128                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-122                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-485                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-486                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-487                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-488                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-243     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-244            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-485           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-486             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-487           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-488          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-129                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-123                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-489                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-490                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-491                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-492                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-245     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-246            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-489           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-490             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-491           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-492          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-130                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-124                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-493                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-494                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-495                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-496                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-247     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-248            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-493           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-494             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-495           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-496          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-131                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-125                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-497                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-498                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-499                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-500                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-249     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-250            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-497           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-498             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-499           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-500          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-132                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-126                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-501                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-502                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-503                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-504                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-251     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-252            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-501           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-502             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-503           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-504          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-133                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-127                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-505                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-506                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-507                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-508                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-253     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-254            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-505           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-506             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-507           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-508          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-134                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-128                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-509                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-510                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-511                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-512                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-255     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-256            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-509           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-510             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-511           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-512          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-135                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-129                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-513                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-514                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-515                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-516                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-257     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-258            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-513           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-514             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-515           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-516          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-136                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-130                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-517                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-518                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-519                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-520                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-259     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-260            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-517           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-518             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-519           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-520          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-137                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-131                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-521                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-522                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-523                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-524                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-261     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-262            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-521           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-522             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-523           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-524          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-138                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-132                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-525                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-526                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-527                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-528                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-263     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-264            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-525           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-526             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-527           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-528          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-139                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-133                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-529                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-530                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-531                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-532                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-265     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-266            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-529           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-530             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-531           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-532          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-140                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-134                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-533                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-534                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-535                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-536                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-267     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-268            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-533           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-534             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-535           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-536          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-141                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-135                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-537                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-538                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-539                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-540                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-269     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-270            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-537           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-538             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-539           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-540          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-142                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-136                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-541                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-542                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-543                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-544                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-271     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-272            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-541           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-542             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-543           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-544          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-143                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-137                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-545                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-546                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-547                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-548                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-273     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-274            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-545           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-546             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-547           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-548          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-144                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-138                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-549                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-550                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-551                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-552                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-275     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-276            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-549           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-550             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-551           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-552          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-145                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-139                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-553                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-554                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-555                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-556                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-277     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-278            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-553           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-554             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-555           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-556          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-146                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-140                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-557                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-558                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-559                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-560                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-279     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-280            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-557           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-558             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-559           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-560          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─Conv2d: 4-147                               [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    └─ResidualGroup: 2-9                                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    └─Sequential: 3-8                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    └─RCAB: 4-148                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-141                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-561                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-562                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-563                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-564                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-281     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-282            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-561           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-562             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-563           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-564          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-149                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-142                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-565                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-566                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-567                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-568                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-283     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-284            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-565           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-566             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-567           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-568          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-150                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-143                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-569                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-570                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-571                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-572                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-285     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-286            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-569           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-570             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-571           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-572          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-151                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-144                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-573                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-574                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-575                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-576                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-287     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-288            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-573           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-574             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-575           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-576          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-152                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-145                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-577                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-578                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-579                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-580                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-289     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-290            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-577           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-578             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-579           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-580          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-153                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-146                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-581                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-582                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-583                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-584                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-291     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-292            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-581           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-582             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-583           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-584          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-154                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-147                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-585                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-586                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-587                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-588                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-293     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-294            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-585           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-586             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-587           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-588          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-155                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-148                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-589                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-590                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-591                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-592                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-295     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-296            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-589           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-590             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-591           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-592          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-156                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-149                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-593                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-594                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-595                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-596                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-297     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-298            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-593           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-594             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-595           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-596          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-157                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-150                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-597                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-598                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-599                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-600                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-299     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-300            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-597           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-598             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-599           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-600          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-158                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-151                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-601                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-602                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-603                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-604                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-301     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-302            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-601           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-602             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-603           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-604          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-159                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-152                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-605                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-606                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-607                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-608                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-303     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-304            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-605           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-606             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-607           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-608          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-160                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-153                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-609                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-610                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-611                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-612                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-305     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-306            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-609           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-610             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-611           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-612          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-161                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-154                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-613                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-614                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-615                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-616                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-307     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-308            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-613           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-614             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-615           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-616          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-162                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-155                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-617                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-618                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-619                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-620                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-309     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-310            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-617           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-618             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-619           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-620          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-163                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-156                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-621                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-622                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-623                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-624                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-311     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-312            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-621           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-622             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-623           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-624          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-164                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-157                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-625                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-626                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-627                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-628                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-313     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-314            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-625           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-626             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-627           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-628          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-165                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-158                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-629                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-630                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-631                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-632                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-315     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-316            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-629           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-630             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-631           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-632          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-166                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-159                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-633                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-634                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-635                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-636                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-317     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-318            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-633           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-634             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-635           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-636          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-167                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-160                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-637                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-638                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-639                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-640                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-319     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-320            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-637           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-638             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-639           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-640          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─Conv2d: 4-168                               [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    └─ResidualGroup: 2-10                                   [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    └─Sequential: 3-9                                  [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    └─RCAB: 4-169                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-161                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-641                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-642                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-643                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-644                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-321     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-322            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-641           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-642             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-643           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-644          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-170                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-162                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-645                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-646                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-647                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-648                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-323     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-324            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-645           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-646             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-647           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-648          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-171                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-163                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-649                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-650                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-651                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-652                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-325     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-326            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-649           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-650             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-651           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-652          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-172                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-164                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-653                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-654                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-655                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-656                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-327     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-328            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-653           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-654             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-655           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-656          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-173                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-165                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-657                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-658                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-659                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-660                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-329     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-330            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-657           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-658             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-659           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-660          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-174                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-166                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-661                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-662                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-663                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-664                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-331     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-332            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-661           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-662             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-663           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-664          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-175                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-167                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-665                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-666                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-667                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-668                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-333     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-334            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-665           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-666             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-667           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-668          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-176                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-168                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-669                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-670                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-671                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-672                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-335     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-336            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-669           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-670             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-671           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-672          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-177                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-169                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-673                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-674                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-675                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-676                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-337     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-338            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-673           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-674             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-675           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-676          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-178                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-170                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-677                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-678                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-679                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-680                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-339     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-340            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-677           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-678             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-679           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-680          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-179                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-171                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-681                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-682                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-683                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-684                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-341     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-342            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-681           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-682             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-683           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-684          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-180                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-172                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-685                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-686                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-687                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-688                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-343     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-344            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-685           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-686             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-687           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-688          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-181                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-173                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-689                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-690                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-691                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-692                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-345     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-346            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-689           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-690             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-691           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-692          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-182                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-174                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-693                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-694                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-695                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-696                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-347     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-348            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-693           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-694             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-695           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-696          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-183                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-175                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-697                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-698                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-699                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-700                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-349     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-350            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-697           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-698             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-699           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-700          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-184                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-176                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-701                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-702                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-703                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-704                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-351     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-352            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-701           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-702             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-703           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-704          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-185                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-177                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-705                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-706                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-707                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-708                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-353     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-354            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-705           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-706             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-707           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-708          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-186                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-178                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-709                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-710                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-711                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-712                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-355     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-356            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-709           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-710             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-711           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-712          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-187                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-179                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-713                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-714                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-715                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-716                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-357     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-358            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-713           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-714             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-715           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-716          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-188                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-180                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-717                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-718                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-719                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-720                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-359     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-360            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-717           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-718             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-719           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-720          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─Conv2d: 4-189                               [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    └─ResidualGroup: 2-11                                   [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    └─Sequential: 3-10                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    └─RCAB: 4-190                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-181                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-721                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-722                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-723                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-724                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-361     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-362            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-721           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-722             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-723           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-724          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-191                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-182                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-725                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-726                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-727                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-728                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-363     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-364            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-725           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-726             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-727           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-728          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-192                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-183                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-729                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-730                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-731                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-732                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-365     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-366            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-729           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-730             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-731           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-732          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-193                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-184                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-733                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-734                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-735                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-736                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-367     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-368            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-733           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-734             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-735           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-736          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-194                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-185                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-737                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-738                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-739                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-740                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-369     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-370            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-737           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-738             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-739           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-740          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-195                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-186                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-741                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-742                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-743                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-744                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-371     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-372            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-741           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-742             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-743           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-744          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-196                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-187                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-745                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-746                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-747                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-748                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-373     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-374            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-745           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-746             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-747           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-748          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-197                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-188                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-749                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-750                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-751                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-752                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-375     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-376            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-749           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-750             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-751           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-752          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-198                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-189                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-753                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-754                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-755                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-756                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-377     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-378            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-753           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-754             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-755           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-756          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-199                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-190                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-757                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-758                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-759                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-760                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-379     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-380            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-757           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-758             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-759           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-760          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-200                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-191                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-761                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-762                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-763                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-764                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-381     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-382            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-761           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-762             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-763           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-764          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-201                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-192                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-765                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-766                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-767                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-768                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-383     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-384            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-765           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-766             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-767           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-768          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-202                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-193                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-769                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-770                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-771                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-772                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-385     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-386            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-769           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-770             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-771           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-772          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-203                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-194                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-773                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-774                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-775                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-776                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-387     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-388            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-773           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-774             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-775           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-776          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-204                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-195                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-777                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-778                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-779                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-780                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-389     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-390            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-777           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-778             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-779           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-780          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-205                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-196                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-781                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-782                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-783                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-784                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-391     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-392            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-781           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-782             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-783           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-784          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-206                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-197                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-785                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-786                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-787                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-788                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-393     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-394            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-785           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-786             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-787           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-788          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-207                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-198                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-789                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-790                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-791                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-792                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-395     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-396            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-789           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-790             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-791           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-792          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-208                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-199                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-793                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-794                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-795                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-796                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-397     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-398            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-793           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-794             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-795           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-796          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─RCAB: 4-209                                 [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    └─Sequential: 5-200                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-797                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─ReLU: 6-798                       [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    └─Conv2d: 6-799                     [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    |    |    |    |    └─CALayer: 6-800                    [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    |    |    |    |    |    └─AdaptiveAvgPool2d: 7-399     [-1, 64, 48, 48]          [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    └─Sequential: 7-400            [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-797           [-1, 64, 1, 1]            [-1, 4, 1, 1]             [64, 4, 1, 1]             260                       256
|    |    |    |    |    |    |    └─ReLU: 8-798             [-1, 4, 1, 1]             [-1, 4, 1, 1]             --                        --                        --
|    |    |    |    |    |    |    └─Conv2d: 8-799           [-1, 4, 1, 1]             [-1, 64, 1, 1]            [4, 64, 1, 1]             320                       256
|    |    |    |    |    |    |    └─Sigmoid: 8-800          [-1, 64, 1, 1]            [-1, 64, 1, 1]            --                        --                        --
|    |    |    └─Conv2d: 4-210                               [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
|    └─Conv2d: 2-12                                          [-1, 64, 48, 48]          [-1, 64, 48, 48]          [64, 64, 3, 3]            36,928                    84,934,656
├─Layer_Attention_Module: 1-3                                [-1, 10, 64, 48, 48]      [-1, 64, 48, 48]          --                        --                        --
|    └─Softmax: 2-13                                         [-1, 10, 10]              [-1, 10, 10]              --                        --                        --
|    └─Conv2d: 2-14                                          [-1, 640, 48, 48]         [-1, 64, 48, 48]          [640, 64, 3, 3]           368,704                   849,346,560
├─Channel_Spatial_Attention_Module: 1-4                      [-1, 64, 48, 48]          [-1, 64, 48, 48]          --                        --                        --
|    └─Conv3d: 2-15                                          [-1, 1, 64, 48, 48]       [-1, 1, 64, 48, 48]       [1, 1, 3, 3, 3]           28                        3,981,312
|    └─Sigmoid: 2-16                                         [-1, 1, 64, 48, 48]       [-1, 1, 64, 48, 48]       --                        --                        --
├─Sequential: 1-5                                            [-1, 64, 48, 48]          [-1, 3, 96, 96]           --                        --                        --
|    └─Upsampler: 2-17                                       [-1, 64, 48, 48]          [-1, 64, 96, 96]          --                        --                        --
|    |    └─Conv2d: 3-11                                     [-1, 64, 48, 48]          [-1, 256, 48, 48]         [64, 256, 3, 3]           147,712                   339,738,624
|    |    └─PixelShuffle: 3-12                               [-1, 256, 48, 48]         [-1, 64, 96, 96]          --                        --                        --
|    └─Conv2d: 2-18                                          [-1, 64, 96, 96]          [-1, 3, 96, 96]           [64, 3, 3, 3]             1,731                     15,925,248
├─MeanShift: 1-6                                             [-1, 3, 96, 96]           [-1, 3, 96, 96]           [3, 3, 1, 1]              12                        9
=========================================================================================================================================================================================
Total params: 15,813,399
Trainable params: 15,813,399
Non-trainable params: 0
Total mult-adds (G): 36.18
=========================================================================================================================================================================================
Input size (MB): 0.03
Forward/backward pass size (MB): 470.83
Params size (MB): 60.32
Estimated Total Size (MB): 531.18
=========================================================================================================================================================================================
Preparing loss function:
1.000 * L1
2021-03-20 02:27:07  [Epoch 1]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 21.6599]	 timer_model: 59.29 + timer_data: 0.34s
[3200/16000]	[L1: 15.2397]	 timer_model: 58.92 + timer_data: 0.05s
[4800/16000]	[L1: 16.2694]	 timer_model: 58.92 + timer_data: 0.05s
[6400/16000]	[L1: 14.3321]	 timer_model: 58.97 + timer_data: 0.04s
[8000/16000]	[L1: 14.3034]	 timer_model: 59.53 + timer_data: 0.05s
[9600/16000]	[L1: 27.6596]	 timer_model: 58.81 + timer_data: 0.04s
[11200/16000]	[L1: 26.5448]	 timer_model: 58.87 + timer_data: 0.05s
[12800/16000]	[L1: 24.8886]	 timer_model: 59.13 + timer_data: 0.04s
[14400/16000]	[L1: 23.3847]	 timer_model: 58.97 + timer_data: 0.04s
[16000/16000]	[L1: 22.0642]	 timer_model: 59.30 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 28.907 (Best: 28.907 @epoch 1)
Total time: 3.24s
 Now time: Sat Mar 20 02:37:02 2021

2021-03-20 02:37:04  [Epoch 2]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 9.6120]	 timer_model: 58.88 + timer_data: 0.28s
[3200/16000]	[L1: 9.3834]	 timer_model: 58.84 + timer_data: 0.04s
[4800/16000]	[L1: 9.1529]	 timer_model: 58.89 + timer_data: 0.04s
[6400/16000]	[L1: 8.9718]	 timer_model: 59.13 + timer_data: 0.04s
[8000/16000]	[L1: 8.7088]	 timer_model: 59.28 + timer_data: 0.04s
[9600/16000]	[L1: 8.5243]	 timer_model: 59.34 + timer_data: 0.03s
[11200/16000]	[L1: 8.4075]	 timer_model: 58.89 + timer_data: 0.03s
[12800/16000]	[L1: 8.2615]	 timer_model: 58.91 + timer_data: 0.03s
[14400/16000]	[L1: 8.1351]	 timer_model: 59.05 + timer_data: 0.03s
[16000/16000]	[L1: 8.0319]	 timer_model: 59.29 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 32.529 (Best: 32.529 @epoch 2)
Total time: 3.09s
 Now time: Sat Mar 20 02:46:59 2021

2021-03-20 02:47:01  [Epoch 3]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 6.8344]	 timer_model: 58.98 + timer_data: 0.29s
[3200/16000]	[L1: 6.6986]	 timer_model: 58.84 + timer_data: 0.04s
[4800/16000]	[L1: 6.6833]	 timer_model: 58.88 + timer_data: 0.04s
[6400/16000]	[L1: 6.5883]	 timer_model: 59.30 + timer_data: 0.04s
[8000/16000]	[L1: 6.5938]	 timer_model: 59.38 + timer_data: 0.04s
[9600/16000]	[L1: 6.5533]	 timer_model: 59.21 + timer_data: 0.04s
[11200/16000]	[L1: 6.5257]	 timer_model: 58.89 + timer_data: 0.03s
[12800/16000]	[L1: 6.4978]	 timer_model: 58.95 + timer_data: 0.03s
[14400/16000]	[L1: 6.4553]	 timer_model: 59.19 + timer_data: 0.04s
[16000/16000]	[L1: 6.4257]	 timer_model: 58.97 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 33.287 (Best: 33.287 @epoch 3)
Total time: 2.96s
 Now time: Sat Mar 20 02:56:55 2021

2021-03-20 02:56:57  [Epoch 4]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.9573]	 timer_model: 58.86 + timer_data: 0.27s
[3200/16000]	[L1: 6.1651]	 timer_model: 58.95 + timer_data: 0.03s
[4800/16000]	[L1: 6.0762]	 timer_model: 59.39 + timer_data: 0.03s
[6400/16000]	[L1: 6.0552]	 timer_model: 59.67 + timer_data: 0.04s
[8000/16000]	[L1: 6.0082]	 timer_model: 59.38 + timer_data: 0.04s
[9600/16000]	[L1: 5.9938]	 timer_model: 59.17 + timer_data: 0.04s
[11200/16000]	[L1: 5.9700]	 timer_model: 59.03 + timer_data: 0.04s
[12800/16000]	[L1: 6.0318]	 timer_model: 59.42 + timer_data: 0.04s
[14400/16000]	[L1: 6.0299]	 timer_model: 59.66 + timer_data: 0.03s
[16000/16000]	[L1: 6.0141]	 timer_model: 59.39 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 34.076 (Best: 34.076 @epoch 4)
Total time: 3.09s
 Now time: Sat Mar 20 03:06:54 2021

2021-03-20 03:06:56  [Epoch 5]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.7083]	 timer_model: 59.17 + timer_data: 0.29s
[3200/16000]	[L1: 6.3490]	 timer_model: 58.67 + timer_data: 0.04s
[4800/16000]	[L1: 10.2040]	 timer_model: 58.79 + timer_data: 0.04s
[6400/16000]	[L1: 9.5932]	 timer_model: 58.95 + timer_data: 0.04s
[8000/16000]	[L1: 9.1194]	 timer_model: 58.95 + timer_data: 0.04s
[9600/16000]	[L1: 8.6530]	 timer_model: 59.13 + timer_data: 0.04s
[11200/16000]	[L1: 8.2836]	 timer_model: 59.00 + timer_data: 0.03s
[12800/16000]	[L1: 7.9945]	 timer_model: 58.85 + timer_data: 0.03s
[14400/16000]	[L1: 7.7556]	 timer_model: 58.95 + timer_data: 0.03s
[16000/16000]	[L1: 7.5391]	 timer_model: 59.08 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 33.879 (Best: 34.076 @epoch 4)
Total time: 3.03s
 Now time: Sat Mar 20 03:16:49 2021

2021-03-20 03:16:50  [Epoch 6]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.8834]	 timer_model: 58.95 + timer_data: 0.27s
[3200/16000]	[L1: 5.8940]	 timer_model: 59.23 + timer_data: 0.04s
[4800/16000]	[L1: 5.8020]	 timer_model: 58.81 + timer_data: 0.03s
[6400/16000]	[L1: 5.7140]	 timer_model: 58.94 + timer_data: 0.04s
[8000/16000]	[L1: 5.6652]	 timer_model: 58.92 + timer_data: 0.04s
[9600/16000]	[L1: 5.6319]	 timer_model: 59.22 + timer_data: 0.04s
[11200/16000]	[L1: 5.5794]	 timer_model: 58.80 + timer_data: 0.04s
[12800/16000]	[L1: 5.5875]	 timer_model: 58.87 + timer_data: 0.04s
[14400/16000]	[L1: 5.5435]	 timer_model: 58.85 + timer_data: 0.03s
[16000/16000]	[L1: 5.5169]	 timer_model: 59.30 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.473 (Best: 34.473 @epoch 6)
Total time: 3.25s
 Now time: Sat Mar 20 03:26:44 2021

2021-03-20 03:26:46  [Epoch 7]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.4664]	 timer_model: 59.46 + timer_data: 0.29s
[3200/16000]	[L1: 5.4477]	 timer_model: 59.23 + timer_data: 0.03s
[4800/16000]	[L1: 5.3634]	 timer_model: 59.62 + timer_data: 0.03s
[6400/16000]	[L1: 5.4270]	 timer_model: 59.71 + timer_data: 0.04s
[8000/16000]	[L1: 11.7914]	 timer_model: 60.06 + timer_data: 0.04s
[9600/16000]	[L1: 11.1413]	 timer_model: 61.68 + timer_data: 0.04s
[11200/16000]	[L1: 10.4830]	 timer_model: 61.51 + timer_data: 0.04s
[12800/16000]	[L1: 9.9246]	 timer_model: 61.63 + timer_data: 0.04s
[14400/16000]	[L1: 9.5290]	 timer_model: 60.25 + timer_data: 0.04s
[16000/16000]	[L1: 9.1686]	 timer_model: 60.08 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 33.260 (Best: 34.473 @epoch 6)
Total time: 2.99s
 Now time: Sat Mar 20 03:36:53 2021

2021-03-20 03:36:55  [Epoch 8]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.7740]	 timer_model: 59.29 + timer_data: 0.28s
[3200/16000]	[L1: 5.7455]	 timer_model: 58.97 + timer_data: 0.03s
[4800/16000]	[L1: 5.7193]	 timer_model: 59.01 + timer_data: 0.03s
[6400/16000]	[L1: 5.6379]	 timer_model: 62.76 + timer_data: 0.05s
[8000/16000]	[L1: 5.5612]	 timer_model: 60.61 + timer_data: 0.05s
[9600/16000]	[L1: 5.5227]	 timer_model: 59.05 + timer_data: 0.03s
[11200/16000]	[L1: 5.5108]	 timer_model: 58.89 + timer_data: 0.03s
[12800/16000]	[L1: 5.4854]	 timer_model: 59.05 + timer_data: 0.03s
[14400/16000]	[L1: 5.8674]	 timer_model: 59.08 + timer_data: 0.03s
[16000/16000]	[L1: 5.9601]	 timer_model: 59.25 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 33.469 (Best: 34.473 @epoch 6)
Total time: 2.94s
 Now time: Sat Mar 20 03:46:55 2021

2021-03-20 03:46:56  [Epoch 9]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.8105]	 timer_model: 60.51 + timer_data: 0.36s
[3200/16000]	[L1: 5.6637]	 timer_model: 59.25 + timer_data: 0.03s
[4800/16000]	[L1: 7.5424]	 timer_model: 59.26 + timer_data: 0.03s
[6400/16000]	[L1: 7.2541]	 timer_model: 59.73 + timer_data: 0.03s
[8000/16000]	[L1: 6.9737]	 timer_model: 59.36 + timer_data: 0.03s
[9600/16000]	[L1: 6.7718]	 timer_model: 59.19 + timer_data: 0.03s
[11200/16000]	[L1: 6.5843]	 timer_model: 59.04 + timer_data: 0.03s
[12800/16000]	[L1: 6.4476]	 timer_model: 58.89 + timer_data: 0.03s
[14400/16000]	[L1: 6.2964]	 timer_model: 59.11 + timer_data: 0.03s
[16000/16000]	[L1: 6.3922]	 timer_model: 59.14 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 33.838 (Best: 34.473 @epoch 6)
Total time: 3.00s
 Now time: Sat Mar 20 03:56:53 2021

2021-03-20 03:56:55  [Epoch 10]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.5982]	 timer_model: 59.03 + timer_data: 0.28s
[3200/16000]	[L1: 5.4233]	 timer_model: 58.83 + timer_data: 0.04s
[4800/16000]	[L1: 5.3501]	 timer_model: 58.82 + timer_data: 0.04s
[6400/16000]	[L1: 5.2748]	 timer_model: 59.04 + timer_data: 0.03s
[8000/16000]	[L1: 5.2656]	 timer_model: 58.97 + timer_data: 0.04s
[9600/16000]	[L1: 5.6780]	 timer_model: 58.94 + timer_data: 0.04s
[11200/16000]	[L1: 5.7494]	 timer_model: 59.34 + timer_data: 0.04s
[12800/16000]	[L1: 5.7192]	 timer_model: 59.06 + timer_data: 0.04s
[14400/16000]	[L1: 5.6391]	 timer_model: 59.15 + timer_data: 0.03s
[16000/16000]	[L1: 5.5835]	 timer_model: 59.21 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 34.476 (Best: 34.476 @epoch 10)
Total time: 3.09s
 Now time: Sat Mar 20 04:06:49 2021

2021-03-20 04:06:51  [Epoch 11]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.9676]	 timer_model: 58.77 + timer_data: 0.27s
[3200/16000]	[L1: 5.0875]	 timer_model: 59.30 + timer_data: 0.03s
[4800/16000]	[L1: 5.0761]	 timer_model: 58.86 + timer_data: 0.03s
[6400/16000]	[L1: 5.0414]	 timer_model: 59.11 + timer_data: 0.03s
[8000/16000]	[L1: 6.0068]	 timer_model: 59.06 + timer_data: 0.03s
[9600/16000]	[L1: 5.9490]	 timer_model: 58.94 + timer_data: 0.03s
[11200/16000]	[L1: 6.4835]	 timer_model: 59.08 + timer_data: 0.03s
[12800/16000]	[L1: 7.2573]	 timer_model: 59.23 + timer_data: 0.03s
[14400/16000]	[L1: 7.1349]	 timer_model: 59.17 + timer_data: 0.03s
[16000/16000]	[L1: 7.7191]	 timer_model: 59.45 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 31.711 (Best: 34.476 @epoch 10)
Total time: 3.25s
 Now time: Sat Mar 20 04:16:46 2021

2021-03-20 04:16:47  [Epoch 12]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 6.4529]	 timer_model: 58.82 + timer_data: 0.27s
[3200/16000]	[L1: 6.0736]	 timer_model: 58.76 + timer_data: 0.03s
[4800/16000]	[L1: 5.9515]	 timer_model: 58.83 + timer_data: 0.03s
[6400/16000]	[L1: 5.8157]	 timer_model: 58.99 + timer_data: 0.03s
[8000/16000]	[L1: 5.7374]	 timer_model: 59.02 + timer_data: 0.03s
[9600/16000]	[L1: 5.6582]	 timer_model: 58.78 + timer_data: 0.03s
[11200/16000]	[L1: 5.5597]	 timer_model: 59.15 + timer_data: 0.03s
[12800/16000]	[L1: 5.5160]	 timer_model: 58.90 + timer_data: 0.03s
[14400/16000]	[L1: 5.5973]	 timer_model: 58.89 + timer_data: 0.03s
[16000/16000]	[L1: 5.6036]	 timer_model: 59.00 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 34.285 (Best: 34.476 @epoch 10)
Total time: 3.18s
 Now time: Sat Mar 20 04:26:41 2021

2021-03-20 04:26:42  [Epoch 13]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.3928]	 timer_model: 59.04 + timer_data: 0.27s
[3200/16000]	[L1: 5.2814]	 timer_model: 59.12 + timer_data: 0.04s
[4800/16000]	[L1: 5.1864]	 timer_model: 59.35 + timer_data: 0.03s
[6400/16000]	[L1: 5.1198]	 timer_model: 59.04 + timer_data: 0.03s
[8000/16000]	[L1: 5.1259]	 timer_model: 59.05 + timer_data: 0.03s
[9600/16000]	[L1: 5.1053]	 timer_model: 58.84 + timer_data: 0.03s
[11200/16000]	[L1: 5.0885]	 timer_model: 58.95 + timer_data: 0.03s
[12800/16000]	[L1: 7.5969]	 timer_model: 59.22 + timer_data: 0.03s
[14400/16000]	[L1: 7.5497]	 timer_model: 59.03 + timer_data: 0.03s
[16000/16000]	[L1: 7.4608]	 timer_model: 59.11 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 32.825 (Best: 34.476 @epoch 10)
Total time: 3.27s
 Now time: Sat Mar 20 04:36:37 2021

2021-03-20 04:36:38  [Epoch 14]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 6.1890]	 timer_model: 59.06 + timer_data: 0.28s
[3200/16000]	[L1: 5.9960]	 timer_model: 59.21 + timer_data: 0.04s
[4800/16000]	[L1: 5.8077]	 timer_model: 59.05 + timer_data: 0.04s
[6400/16000]	[L1: 5.7085]	 timer_model: 59.02 + timer_data: 0.04s
[8000/16000]	[L1: 5.6328]	 timer_model: 59.21 + timer_data: 0.04s
[9600/16000]	[L1: 5.5919]	 timer_model: 62.30 + timer_data: 0.04s
[11200/16000]	[L1: 5.5372]	 timer_model: 60.69 + timer_data: 0.04s
[12800/16000]	[L1: 5.5183]	 timer_model: 59.07 + timer_data: 0.04s
[14400/16000]	[L1: 5.4789]	 timer_model: 59.10 + timer_data: 0.04s
[16000/16000]	[L1: 5.4648]	 timer_model: 59.11 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 33.724 (Best: 34.476 @epoch 10)
Total time: 3.15s
 Now time: Sat Mar 20 04:46:38 2021

2021-03-20 04:46:40  [Epoch 15]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.2497]	 timer_model: 59.12 + timer_data: 0.27s
[3200/16000]	[L1: 5.3267]	 timer_model: 59.06 + timer_data: 0.03s
[4800/16000]	[L1: 5.2597]	 timer_model: 58.87 + timer_data: 0.03s
[6400/16000]	[L1: 5.2309]	 timer_model: 59.09 + timer_data: 0.04s
[8000/16000]	[L1: 5.2105]	 timer_model: 59.03 + timer_data: 0.04s
[9600/16000]	[L1: 5.1985]	 timer_model: 58.84 + timer_data: 0.03s
[11200/16000]	[L1: 5.1966]	 timer_model: 58.81 + timer_data: 0.03s
[12800/16000]	[L1: 5.1840]	 timer_model: 59.06 + timer_data: 0.03s
[14400/16000]	[L1: 5.1524]	 timer_model: 58.98 + timer_data: 0.03s
[16000/16000]	[L1: 5.1329]	 timer_model: 58.98 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 34.268 (Best: 34.476 @epoch 10)
Total time: 3.18s
 Now time: Sat Mar 20 04:56:33 2021

2021-03-20 04:56:35  [Epoch 16]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.7391]	 timer_model: 59.17 + timer_data: 0.28s
[3200/16000]	[L1: 20.1839]	 timer_model: 59.34 + timer_data: 0.04s
[4800/16000]	[L1: 16.1972]	 timer_model: 59.04 + timer_data: 0.04s
[6400/16000]	[L1: 13.6082]	 timer_model: 59.15 + timer_data: 0.04s
[8000/16000]	[L1: 12.0122]	 timer_model: 59.04 + timer_data: 0.04s
[9600/16000]	[L1: 10.9062]	 timer_model: 58.95 + timer_data: 0.04s
[11200/16000]	[L1: 10.1232]	 timer_model: 59.05 + timer_data: 0.04s
[12800/16000]	[L1: 9.5147]	 timer_model: 58.95 + timer_data: 0.04s
[14400/16000]	[L1: 9.0387]	 timer_model: 59.35 + timer_data: 0.03s
[16000/16000]	[L1: 10.3424]	 timer_model: 59.09 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 31.052 (Best: 34.476 @epoch 10)
Total time: 2.88s
 Now time: Sat Mar 20 05:06:30 2021

2021-03-20 05:06:31  [Epoch 17]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 6.7184]	 timer_model: 59.24 + timer_data: 0.28s
[3200/16000]	[L1: 28.9863]	 timer_model: 58.99 + timer_data: 0.03s
[4800/16000]	[L1: 22.1727]	 timer_model: 59.05 + timer_data: 0.03s
[6400/16000]	[L1: 18.3179]	 timer_model: 59.28 + timer_data: 0.03s
[8000/16000]	[L1: 15.9625]	 timer_model: 59.02 + timer_data: 0.03s
[9600/16000]	[L1: 14.3445]	 timer_model: 59.46 + timer_data: 0.03s
[11200/16000]	[L1: 13.1525]	 timer_model: 59.52 + timer_data: 0.03s
[12800/16000]	[L1: 12.2213]	 timer_model: 59.36 + timer_data: 0.03s
[14400/16000]	[L1: 11.4828]	 timer_model: 59.47 + timer_data: 0.03s
[16000/16000]	[L1: 10.8669]	 timer_model: 59.56 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 33.489 (Best: 34.476 @epoch 10)
Total time: 3.05s
 Now time: Sat Mar 20 05:16:28 2021

2021-03-20 05:16:29  [Epoch 18]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.5982]	 timer_model: 59.54 + timer_data: 0.28s
[3200/16000]	[L1: 5.4358]	 timer_model: 59.41 + timer_data: 0.04s
[4800/16000]	[L1: 5.3388]	 timer_model: 59.61 + timer_data: 0.04s
[6400/16000]	[L1: 5.3120]	 timer_model: 59.60 + timer_data: 0.04s
[8000/16000]	[L1: 5.3053]	 timer_model: 59.57 + timer_data: 0.04s
[9600/16000]	[L1: 5.2779]	 timer_model: 59.57 + timer_data: 0.03s
[11200/16000]	[L1: 5.2745]	 timer_model: 59.65 + timer_data: 0.03s
[12800/16000]	[L1: 5.2698]	 timer_model: 59.50 + timer_data: 0.04s
[14400/16000]	[L1: 5.5880]	 timer_model: 59.68 + timer_data: 0.04s
[16000/16000]	[L1: 5.6488]	 timer_model: 59.74 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 33.877 (Best: 34.476 @epoch 10)
Total time: 3.07s
 Now time: Sat Mar 20 05:26:29 2021

2021-03-20 05:26:31  [Epoch 19]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.4839]	 timer_model: 60.63 + timer_data: 0.35s
[3200/16000]	[L1: 5.4534]	 timer_model: 59.79 + timer_data: 0.03s
[4800/16000]	[L1: 5.4517]	 timer_model: 59.51 + timer_data: 0.04s
[6400/16000]	[L1: 5.3636]	 timer_model: 59.55 + timer_data: 0.04s
[8000/16000]	[L1: 5.3345]	 timer_model: 59.69 + timer_data: 0.04s
[9600/16000]	[L1: 5.2790]	 timer_model: 59.49 + timer_data: 0.04s
[11200/16000]	[L1: 5.2528]	 timer_model: 59.43 + timer_data: 0.04s
[12800/16000]	[L1: 5.2285]	 timer_model: 59.47 + timer_data: 0.03s
[14400/16000]	[L1: 5.4229]	 timer_model: 59.46 + timer_data: 0.03s
[16000/16000]	[L1: 5.4324]	 timer_model: 59.67 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 34.421 (Best: 34.476 @epoch 10)
Total time: 2.91s
 Now time: Sat Mar 20 05:36:31 2021

2021-03-20 05:36:32  [Epoch 20]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.0562]	 timer_model: 59.64 + timer_data: 0.28s
[3200/16000]	[L1: 5.1594]	 timer_model: 59.52 + timer_data: 0.03s
[4800/16000]	[L1: 5.1911]	 timer_model: 59.39 + timer_data: 0.04s
[6400/16000]	[L1: 5.3847]	 timer_model: 59.46 + timer_data: 0.04s
[8000/16000]	[L1: 7.0091]	 timer_model: 59.72 + timer_data: 0.03s
[9600/16000]	[L1: 7.1635]	 timer_model: 59.38 + timer_data: 0.04s
[11200/16000]	[L1: 6.9877]	 timer_model: 59.50 + timer_data: 0.04s
[12800/16000]	[L1: 6.8121]	 timer_model: 59.58 + timer_data: 0.04s
[14400/16000]	[L1: 6.6641]	 timer_model: 59.61 + timer_data: 0.04s
[16000/16000]	[L1: 6.5413]	 timer_model: 59.62 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.160 (Best: 34.476 @epoch 10)
Total time: 3.54s
 Now time: Sat Mar 20 05:46:32 2021

2021-03-20 05:46:34  [Epoch 21]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.3619]	 timer_model: 59.58 + timer_data: 0.28s
[3200/16000]	[L1: 5.3065]	 timer_model: 59.67 + timer_data: 0.04s
[4800/16000]	[L1: 5.2108]	 timer_model: 59.51 + timer_data: 0.03s
[6400/16000]	[L1: 5.1502]	 timer_model: 59.57 + timer_data: 0.03s
[8000/16000]	[L1: 5.1192]	 timer_model: 59.62 + timer_data: 0.03s
[9600/16000]	[L1: 5.0928]	 timer_model: 59.66 + timer_data: 0.03s
[11200/16000]	[L1: 6.3244]	 timer_model: 59.63 + timer_data: 0.03s
[12800/16000]	[L1: 6.2714]	 timer_model: 59.57 + timer_data: 0.04s
[14400/16000]	[L1: 6.2098]	 timer_model: 59.79 + timer_data: 0.04s
[16000/16000]	[L1: 6.1227]	 timer_model: 59.65 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.268 (Best: 34.476 @epoch 10)
Total time: 3.03s
 Now time: Sat Mar 20 05:56:34 2021

2021-03-20 05:56:35  [Epoch 22]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.5980]	 timer_model: 59.72 + timer_data: 0.29s
[3200/16000]	[L1: 5.3839]	 timer_model: 59.56 + timer_data: 0.04s
[4800/16000]	[L1: 5.2483]	 timer_model: 59.42 + timer_data: 0.04s
[6400/16000]	[L1: 5.2163]	 timer_model: 59.63 + timer_data: 0.04s
[8000/16000]	[L1: 5.3852]	 timer_model: 59.77 + timer_data: 0.04s
[9600/16000]	[L1: 5.5244]	 timer_model: 59.68 + timer_data: 0.04s
[11200/16000]	[L1: 5.4502]	 timer_model: 59.60 + timer_data: 0.04s
[12800/16000]	[L1: 5.3901]	 timer_model: 59.71 + timer_data: 0.03s
[14400/16000]	[L1: 5.3279]	 timer_model: 59.71 + timer_data: 0.03s
[16000/16000]	[L1: 5.2839]	 timer_model: 59.69 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 34.625 (Best: 34.625 @epoch 22)
Total time: 3.12s
 Now time: Sat Mar 20 06:06:36 2021

2021-03-20 06:06:38  [Epoch 23]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 40.7012]	 timer_model: 59.55 + timer_data: 0.28s
[3200/16000]	[L1: 25.5545]	 timer_model: 59.51 + timer_data: 0.03s
[4800/16000]	[L1: 19.3244]	 timer_model: 59.44 + timer_data: 0.04s
[6400/16000]	[L1: 16.0151]	 timer_model: 59.62 + timer_data: 0.04s
[8000/16000]	[L1: 13.9737]	 timer_model: 59.79 + timer_data: 0.04s
[9600/16000]	[L1: 12.5635]	 timer_model: 59.56 + timer_data: 0.03s
[11200/16000]	[L1: 11.5465]	 timer_model: 59.53 + timer_data: 0.04s
[12800/16000]	[L1: 10.7866]	 timer_model: 59.58 + timer_data: 0.04s
[14400/16000]	[L1: 10.1823]	 timer_model: 59.72 + timer_data: 0.04s
[16000/16000]	[L1: 9.7022]	 timer_model: 59.86 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.020 (Best: 34.625 @epoch 22)
Total time: 3.19s
 Now time: Sat Mar 20 06:16:38 2021

2021-03-20 06:16:39  [Epoch 24]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.3816]	 timer_model: 60.92 + timer_data: 0.35s
[3200/16000]	[L1: 5.2605]	 timer_model: 59.76 + timer_data: 0.04s
[4800/16000]	[L1: 5.2724]	 timer_model: 59.66 + timer_data: 0.04s
[6400/16000]	[L1: 5.2141]	 timer_model: 62.88 + timer_data: 0.05s
[8000/16000]	[L1: 8.6850]	 timer_model: 61.19 + timer_data: 0.04s
[9600/16000]	[L1: 8.3316]	 timer_model: 60.05 + timer_data: 0.03s
[11200/16000]	[L1: 8.0048]	 timer_model: 59.80 + timer_data: 0.04s
[12800/16000]	[L1: 7.7170]	 timer_model: 59.78 + timer_data: 0.04s
[14400/16000]	[L1: 7.4805]	 timer_model: 59.82 + timer_data: 0.04s
[16000/16000]	[L1: 7.2659]	 timer_model: 59.73 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.051 (Best: 34.625 @epoch 22)
Total time: 3.02s
 Now time: Sat Mar 20 06:26:47 2021

2021-03-20 06:26:48  [Epoch 25]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.4057]	 timer_model: 59.85 + timer_data: 0.28s
[3200/16000]	[L1: 5.3860]	 timer_model: 59.87 + timer_data: 0.03s
[4800/16000]	[L1: 5.3434]	 timer_model: 59.56 + timer_data: 0.04s
[6400/16000]	[L1: 17.4363]	 timer_model: 59.54 + timer_data: 0.04s
[8000/16000]	[L1: 15.7244]	 timer_model: 59.63 + timer_data: 0.04s
[9600/16000]	[L1: 14.2383]	 timer_model: 59.61 + timer_data: 0.04s
[11200/16000]	[L1: 13.0808]	 timer_model: 59.63 + timer_data: 0.04s
[12800/16000]	[L1: 12.1783]	 timer_model: 59.81 + timer_data: 0.04s
[14400/16000]	[L1: 11.4517]	 timer_model: 59.65 + timer_data: 0.04s
[16000/16000]	[L1: 10.8653]	 timer_model: 59.51 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 33.780 (Best: 34.625 @epoch 22)
Total time: 2.98s
 Now time: Sat Mar 20 06:36:49 2021

2021-03-20 06:36:50  [Epoch 26]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.3009]	 timer_model: 59.70 + timer_data: 0.29s
[3200/16000]	[L1: 5.3457]	 timer_model: 59.45 + timer_data: 0.05s
[4800/16000]	[L1: 5.3620]	 timer_model: 59.59 + timer_data: 0.04s
[6400/16000]	[L1: 5.3243]	 timer_model: 59.58 + timer_data: 0.04s
[8000/16000]	[L1: 5.3398]	 timer_model: 59.61 + timer_data: 0.04s
[9600/16000]	[L1: 5.3205]	 timer_model: 59.89 + timer_data: 0.04s
[11200/16000]	[L1: 5.3072]	 timer_model: 59.57 + timer_data: 0.03s
[12800/16000]	[L1: 5.2788]	 timer_model: 59.59 + timer_data: 0.04s
[14400/16000]	[L1: 5.2703]	 timer_model: 59.53 + timer_data: 0.04s
[16000/16000]	[L1: 5.2536]	 timer_model: 59.78 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.493 (Best: 34.625 @epoch 22)
Total time: 2.99s
 Now time: Sat Mar 20 06:46:51 2021

2021-03-20 06:46:52  [Epoch 27]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.0538]	 timer_model: 61.33 + timer_data: 0.28s
[3200/16000]	[L1: 5.1059]	 timer_model: 61.39 + timer_data: 0.04s
[4800/16000]	[L1: 5.0870]	 timer_model: 59.55 + timer_data: 0.04s
[6400/16000]	[L1: 5.0834]	 timer_model: 59.47 + timer_data: 0.04s
[8000/16000]	[L1: 5.1198]	 timer_model: 59.54 + timer_data: 0.04s
[9600/16000]	[L1: 5.0772]	 timer_model: 59.65 + timer_data: 0.04s
[11200/16000]	[L1: 5.0774]	 timer_model: 59.83 + timer_data: 0.04s
[12800/16000]	[L1: 5.8902]	 timer_model: 60.53 + timer_data: 0.04s
[14400/16000]	[L1: 5.8975]	 timer_model: 59.53 + timer_data: 0.04s
[16000/16000]	[L1: 5.8619]	 timer_model: 59.67 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 33.937 (Best: 34.625 @epoch 22)
Total time: 3.03s
 Now time: Sat Mar 20 06:56:57 2021

2021-03-20 06:56:58  [Epoch 28]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.2835]	 timer_model: 59.55 + timer_data: 0.27s
[3200/16000]	[L1: 5.2770]	 timer_model: 59.40 + timer_data: 0.04s
[4800/16000]	[L1: 5.2924]	 timer_model: 59.50 + timer_data: 0.04s
[6400/16000]	[L1: 5.2420]	 timer_model: 59.50 + timer_data: 0.03s
[8000/16000]	[L1: 5.2017]	 timer_model: 59.73 + timer_data: 0.04s
[9600/16000]	[L1: 5.1888]	 timer_model: 59.61 + timer_data: 0.03s
[11200/16000]	[L1: 5.1609]	 timer_model: 59.61 + timer_data: 0.03s
[12800/16000]	[L1: 6.5779]	 timer_model: 59.49 + timer_data: 0.04s
[14400/16000]	[L1: 6.5818]	 timer_model: 59.54 + timer_data: 0.03s
[16000/16000]	[L1: 6.4939]	 timer_model: 59.60 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 33.975 (Best: 34.625 @epoch 22)
Total time: 3.09s
 Now time: Sat Mar 20 07:06:57 2021

2021-03-20 07:06:59  [Epoch 29]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.5818]	 timer_model: 59.60 + timer_data: 0.27s
[3200/16000]	[L1: 5.5575]	 timer_model: 59.46 + timer_data: 0.03s
[4800/16000]	[L1: 5.5411]	 timer_model: 59.58 + timer_data: 0.03s
[6400/16000]	[L1: 5.4306]	 timer_model: 59.70 + timer_data: 0.04s
[8000/16000]	[L1: 5.3534]	 timer_model: 59.60 + timer_data: 0.03s
[9600/16000]	[L1: 5.3002]	 timer_model: 59.62 + timer_data: 0.03s
[11200/16000]	[L1: 5.2589]	 timer_model: 59.49 + timer_data: 0.03s
[12800/16000]	[L1: 5.2374]	 timer_model: 59.45 + timer_data: 0.03s
[14400/16000]	[L1: 5.2133]	 timer_model: 59.50 + timer_data: 0.03s
[16000/16000]	[L1: 5.2015]	 timer_model: 59.60 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 34.373 (Best: 34.625 @epoch 22)
Total time: 2.98s
 Now time: Sat Mar 20 07:16:58 2021

2021-03-20 07:17:00  [Epoch 30]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.0209]	 timer_model: 59.61 + timer_data: 0.28s
[3200/16000]	[L1: 4.9900]	 timer_model: 59.44 + timer_data: 0.04s
[4800/16000]	[L1: 4.9843]	 timer_model: 59.50 + timer_data: 0.04s
[6400/16000]	[L1: 4.9695]	 timer_model: 59.48 + timer_data: 0.04s
[8000/16000]	[L1: 4.9605]	 timer_model: 59.53 + timer_data: 0.04s
[9600/16000]	[L1: 4.9837]	 timer_model: 59.54 + timer_data: 0.03s
[11200/16000]	[L1: 4.9884]	 timer_model: 59.62 + timer_data: 0.04s
[12800/16000]	[L1: 4.9788]	 timer_model: 59.41 + timer_data: 0.04s
[14400/16000]	[L1: 4.9967]	 timer_model: 59.56 + timer_data: 0.04s
[16000/16000]	[L1: 4.9886]	 timer_model: 59.52 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.796 (Best: 34.796 @epoch 30)
Total time: 3.09s
 Now time: Sat Mar 20 07:26:59 2021

2021-03-20 07:27:01  [Epoch 31]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.9594]	 timer_model: 59.68 + timer_data: 0.29s
[3200/16000]	[L1: 4.8978]	 timer_model: 59.68 + timer_data: 0.04s
[4800/16000]	[L1: 4.9504]	 timer_model: 59.53 + timer_data: 0.04s
[6400/16000]	[L1: 4.9174]	 timer_model: 59.56 + timer_data: 0.04s
[8000/16000]	[L1: 4.9099]	 timer_model: 59.73 + timer_data: 0.03s
[9600/16000]	[L1: 4.9184]	 timer_model: 59.53 + timer_data: 0.04s
[11200/16000]	[L1: 4.9503]	 timer_model: 59.66 + timer_data: 0.03s
[12800/16000]	[L1: 4.9415]	 timer_model: 59.67 + timer_data: 0.04s
[14400/16000]	[L1: 4.9423]	 timer_model: 59.66 + timer_data: 0.04s
[16000/16000]	[L1: 6.6245]	 timer_model: 59.69 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 28.916 (Best: 34.796 @epoch 30)
Total time: 2.91s
 Now time: Sat Mar 20 07:37:01 2021

2021-03-20 07:37:02  [Epoch 32]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 7.7970]	 timer_model: 59.51 + timer_data: 0.27s
[3200/16000]	[L1: 7.1500]	 timer_model: 59.40 + timer_data: 0.04s
[4800/16000]	[L1: 6.7911]	 timer_model: 59.50 + timer_data: 0.03s
[6400/16000]	[L1: 6.5106]	 timer_model: 59.46 + timer_data: 0.04s
[8000/16000]	[L1: 6.6002]	 timer_model: 59.67 + timer_data: 0.03s
[9600/16000]	[L1: 6.5855]	 timer_model: 59.51 + timer_data: 0.04s
[11200/16000]	[L1: 6.4342]	 timer_model: 59.49 + timer_data: 0.04s
[12800/16000]	[L1: 6.2876]	 timer_model: 59.51 + timer_data: 0.04s
[14400/16000]	[L1: 6.2136]	 timer_model: 59.77 + timer_data: 0.04s
[16000/16000]	[L1: 6.1218]	 timer_model: 59.75 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.481 (Best: 34.796 @epoch 30)
Total time: 3.44s
 Now time: Sat Mar 20 07:47:02 2021

2021-03-20 07:47:04  [Epoch 33]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.0308]	 timer_model: 59.69 + timer_data: 0.29s
[3200/16000]	[L1: 5.0790]	 timer_model: 59.60 + timer_data: 0.04s
[4800/16000]	[L1: 5.1328]	 timer_model: 59.55 + timer_data: 0.04s
[6400/16000]	[L1: 5.1114]	 timer_model: 59.59 + timer_data: 0.03s
[8000/16000]	[L1: 5.1161]	 timer_model: 59.54 + timer_data: 0.03s
[9600/16000]	[L1: 5.1080]	 timer_model: 59.61 + timer_data: 0.04s
[11200/16000]	[L1: 5.1058]	 timer_model: 59.52 + timer_data: 0.04s
[12800/16000]	[L1: 5.0788]	 timer_model: 59.61 + timer_data: 0.04s
[14400/16000]	[L1: 5.0599]	 timer_model: 59.58 + timer_data: 0.04s
[16000/16000]	[L1: 5.0273]	 timer_model: 59.66 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.702 (Best: 34.796 @epoch 30)
Total time: 3.11s
 Now time: Sat Mar 20 07:57:04 2021

2021-03-20 07:57:05  [Epoch 34]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.8271]	 timer_model: 59.52 + timer_data: 0.28s
[3200/16000]	[L1: 4.8738]	 timer_model: 59.52 + timer_data: 0.04s
[4800/16000]	[L1: 4.8575]	 timer_model: 59.71 + timer_data: 0.04s
[6400/16000]	[L1: 4.8691]	 timer_model: 59.66 + timer_data: 0.03s
[8000/16000]	[L1: 4.8547]	 timer_model: 59.72 + timer_data: 0.04s
[9600/16000]	[L1: 4.8796]	 timer_model: 59.61 + timer_data: 0.04s
[11200/16000]	[L1: 4.8834]	 timer_model: 59.61 + timer_data: 0.04s
[12800/16000]	[L1: 6.6281]	 timer_model: 59.76 + timer_data: 0.04s
[14400/16000]	[L1: 6.5941]	 timer_model: 59.63 + timer_data: 0.04s
[16000/16000]	[L1: 12.5791]	 timer_model: 59.54 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 26.817 (Best: 34.796 @epoch 30)
Total time: 2.85s
 Now time: Sat Mar 20 08:07:05 2021

2021-03-20 08:07:07  [Epoch 35]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 10.2772]	 timer_model: 59.76 + timer_data: 0.28s
[3200/16000]	[L1: 8.8928]	 timer_model: 59.83 + timer_data: 0.04s
[4800/16000]	[L1: 8.1624]	 timer_model: 59.74 + timer_data: 0.04s
[6400/16000]	[L1: 24.0765]	 timer_model: 59.67 + timer_data: 0.04s
[8000/16000]	[L1: 21.4081]	 timer_model: 59.71 + timer_data: 0.04s
[9600/16000]	[L1: 19.2187]	 timer_model: 59.70 + timer_data: 0.04s
[11200/16000]	[L1: 17.5035]	 timer_model: 59.62 + timer_data: 0.04s
[12800/16000]	[L1: 16.1718]	 timer_model: 59.68 + timer_data: 0.04s
[14400/16000]	[L1: 15.1018]	 timer_model: 59.72 + timer_data: 0.04s
[16000/16000]	[L1: 15.0845]	 timer_model: 59.74 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 30.706 (Best: 34.796 @epoch 30)
Total time: 3.14s
 Now time: Sat Mar 20 08:17:08 2021

2021-03-20 08:17:10  [Epoch 36]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 6.9966]	 timer_model: 59.73 + timer_data: 0.28s
[3200/16000]	[L1: 13.4173]	 timer_model: 59.48 + timer_data: 0.03s
[4800/16000]	[L1: 11.6419]	 timer_model: 59.52 + timer_data: 0.04s
[6400/16000]	[L1: 10.4287]	 timer_model: 59.48 + timer_data: 0.03s
[8000/16000]	[L1: 9.5710]	 timer_model: 59.59 + timer_data: 0.03s
[9600/16000]	[L1: 8.9932]	 timer_model: 59.70 + timer_data: 0.04s
[11200/16000]	[L1: 8.5531]	 timer_model: 59.61 + timer_data: 0.04s
[12800/16000]	[L1: 8.2137]	 timer_model: 59.63 + timer_data: 0.04s
[14400/16000]	[L1: 7.9393]	 timer_model: 59.64 + timer_data: 0.04s
[16000/16000]	[L1: 7.7046]	 timer_model: 59.64 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 33.629 (Best: 34.796 @epoch 30)
Total time: 3.12s
 Now time: Sat Mar 20 08:27:10 2021

2021-03-20 08:27:11  [Epoch 37]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.4833]	 timer_model: 59.63 + timer_data: 0.27s
[3200/16000]	[L1: 5.5587]	 timer_model: 59.52 + timer_data: 0.03s
[4800/16000]	[L1: 5.4965]	 timer_model: 59.52 + timer_data: 0.03s
[6400/16000]	[L1: 5.4586]	 timer_model: 59.53 + timer_data: 0.04s
[8000/16000]	[L1: 6.7413]	 timer_model: 59.62 + timer_data: 0.04s
[9600/16000]	[L1: 6.7644]	 timer_model: 59.60 + timer_data: 0.04s
[11200/16000]	[L1: 6.6427]	 timer_model: 59.70 + timer_data: 0.04s
[12800/16000]	[L1: 6.7085]	 timer_model: 59.72 + timer_data: 0.04s
[14400/16000]	[L1: 7.3310]	 timer_model: 59.66 + timer_data: 0.04s
[16000/16000]	[L1: 7.2166]	 timer_model: 59.51 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 32.961 (Best: 34.796 @epoch 30)
Total time: 2.98s
 Now time: Sat Mar 20 08:37:11 2021

2021-03-20 08:37:12  [Epoch 38]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.7461]	 timer_model: 59.56 + timer_data: 0.29s
[3200/16000]	[L1: 5.6651]	 timer_model: 59.54 + timer_data: 0.04s
[4800/16000]	[L1: 5.5716]	 timer_model: 59.57 + timer_data: 0.04s
[6400/16000]	[L1: 5.5230]	 timer_model: 59.71 + timer_data: 0.03s
[8000/16000]	[L1: 5.4767]	 timer_model: 59.68 + timer_data: 0.03s
[9600/16000]	[L1: 5.4267]	 timer_model: 59.65 + timer_data: 0.03s
[11200/16000]	[L1: 5.3888]	 timer_model: 59.62 + timer_data: 0.04s
[12800/16000]	[L1: 5.3664]	 timer_model: 59.60 + timer_data: 0.04s
[14400/16000]	[L1: 5.3387]	 timer_model: 59.56 + timer_data: 0.04s
[16000/16000]	[L1: 5.3339]	 timer_model: 59.76 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 33.976 (Best: 34.796 @epoch 30)
Total time: 3.13s
 Now time: Sat Mar 20 08:47:13 2021

2021-03-20 08:47:14  [Epoch 39]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.1856]	 timer_model: 59.63 + timer_data: 0.29s
[3200/16000]	[L1: 5.3028]	 timer_model: 59.62 + timer_data: 0.03s
[4800/16000]	[L1: 5.2334]	 timer_model: 59.66 + timer_data: 0.03s
[6400/16000]	[L1: 16.0712]	 timer_model: 59.64 + timer_data: 0.03s
[8000/16000]	[L1: 15.1595]	 timer_model: 59.58 + timer_data: 0.03s
[9600/16000]	[L1: 13.8772]	 timer_model: 59.63 + timer_data: 0.03s
[11200/16000]	[L1: 12.8509]	 timer_model: 59.60 + timer_data: 0.04s
[12800/16000]	[L1: 12.0452]	 timer_model: 59.53 + timer_data: 0.04s
[14400/16000]	[L1: 11.3629]	 timer_model: 59.55 + timer_data: 0.04s
[16000/16000]	[L1: 10.8189]	 timer_model: 59.67 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 33.322 (Best: 34.796 @epoch 30)
Total time: 3.39s
 Now time: Sat Mar 20 08:57:14 2021

2021-03-20 08:57:16  [Epoch 40]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.9120]	 timer_model: 59.51 + timer_data: 0.28s
[3200/16000]	[L1: 5.7377]	 timer_model: 59.53 + timer_data: 0.04s
[4800/16000]	[L1: 5.7253]	 timer_model: 59.54 + timer_data: 0.04s
[6400/16000]	[L1: 5.6673]	 timer_model: 59.63 + timer_data: 0.03s
[8000/16000]	[L1: 5.6077]	 timer_model: 59.58 + timer_data: 0.04s
[9600/16000]	[L1: 5.5841]	 timer_model: 59.64 + timer_data: 0.04s
[11200/16000]	[L1: 5.5309]	 timer_model: 59.65 + timer_data: 0.04s
[12800/16000]	[L1: 5.5150]	 timer_model: 59.81 + timer_data: 0.04s
[14400/16000]	[L1: 5.5098]	 timer_model: 59.71 + timer_data: 0.04s
[16000/16000]	[L1: 5.4855]	 timer_model: 59.60 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.111 (Best: 34.796 @epoch 30)
Total time: 3.19s
 Now time: Sat Mar 20 09:07:16 2021

2021-03-20 09:07:18  [Epoch 41]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.1488]	 timer_model: 59.80 + timer_data: 0.28s
[3200/16000]	[L1: 5.2192]	 timer_model: 59.57 + timer_data: 0.04s
[4800/16000]	[L1: 5.1469]	 timer_model: 59.69 + timer_data: 0.04s
[6400/16000]	[L1: 5.1577]	 timer_model: 59.76 + timer_data: 0.04s
[8000/16000]	[L1: 5.2079]	 timer_model: 59.46 + timer_data: 0.04s
[9600/16000]	[L1: 5.1916]	 timer_model: 59.46 + timer_data: 0.04s
[11200/16000]	[L1: 5.1494]	 timer_model: 59.57 + timer_data: 0.04s
[12800/16000]	[L1: 5.1359]	 timer_model: 59.52 + timer_data: 0.04s
[14400/16000]	[L1: 5.1196]	 timer_model: 59.65 + timer_data: 0.04s
[16000/16000]	[L1: 5.1391]	 timer_model: 59.50 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 33.306 (Best: 34.796 @epoch 30)
Total time: 3.01s
 Now time: Sat Mar 20 09:17:18 2021

2021-03-20 09:17:19  [Epoch 42]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.1902]	 timer_model: 59.61 + timer_data: 0.28s
[3200/16000]	[L1: 5.1493]	 timer_model: 59.48 + timer_data: 0.04s
[4800/16000]	[L1: 5.1074]	 timer_model: 59.51 + timer_data: 0.04s
[6400/16000]	[L1: 5.0690]	 timer_model: 59.71 + timer_data: 0.04s
[8000/16000]	[L1: 5.0615]	 timer_model: 59.62 + timer_data: 0.03s
[9600/16000]	[L1: 5.0364]	 timer_model: 59.55 + timer_data: 0.03s
[11200/16000]	[L1: 5.0936]	 timer_model: 59.55 + timer_data: 0.03s
[12800/16000]	[L1: 5.1421]	 timer_model: 59.60 + timer_data: 0.04s
[14400/16000]	[L1: 5.1203]	 timer_model: 59.69 + timer_data: 0.04s
[16000/16000]	[L1: 7.7248]	 timer_model: 59.60 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 24.224 (Best: 34.796 @epoch 30)
Total time: 2.83s
 Now time: Sat Mar 20 09:27:19 2021

2021-03-20 09:27:20  [Epoch 43]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 9.2676]	 timer_model: 59.75 + timer_data: 0.28s
[3200/16000]	[L1: 7.9894]	 timer_model: 59.52 + timer_data: 0.03s
[4800/16000]	[L1: 7.2962]	 timer_model: 59.57 + timer_data: 0.03s
[6400/16000]	[L1: 6.8601]	 timer_model: 59.54 + timer_data: 0.03s
[8000/16000]	[L1: 6.7481]	 timer_model: 59.56 + timer_data: 0.04s
[9600/16000]	[L1: 6.5546]	 timer_model: 59.49 + timer_data: 0.04s
[11200/16000]	[L1: 6.4093]	 timer_model: 59.63 + timer_data: 0.04s
[12800/16000]	[L1: 6.2708]	 timer_model: 59.57 + timer_data: 0.03s
[14400/16000]	[L1: 6.1645]	 timer_model: 59.68 + timer_data: 0.03s
[16000/16000]	[L1: 6.0488]	 timer_model: 59.57 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.436 (Best: 34.796 @epoch 30)
Total time: 3.20s
 Now time: Sat Mar 20 09:37:20 2021

2021-03-20 09:37:22  [Epoch 44]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.9087]	 timer_model: 59.60 + timer_data: 0.28s
[3200/16000]	[L1: 5.0261]	 timer_model: 59.60 + timer_data: 0.04s
[4800/16000]	[L1: 4.9644]	 timer_model: 59.64 + timer_data: 0.04s
[6400/16000]	[L1: 4.9655]	 timer_model: 59.77 + timer_data: 0.04s
[8000/16000]	[L1: 4.9826]	 timer_model: 59.55 + timer_data: 0.04s
[9600/16000]	[L1: 4.9822]	 timer_model: 59.46 + timer_data: 0.03s
[11200/16000]	[L1: 4.9816]	 timer_model: 59.49 + timer_data: 0.04s
[12800/16000]	[L1: 4.9719]	 timer_model: 59.58 + timer_data: 0.04s
[14400/16000]	[L1: 4.9438]	 timer_model: 59.68 + timer_data: 0.03s
[16000/16000]	[L1: 4.9627]	 timer_model: 59.69 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 34.140 (Best: 34.796 @epoch 30)
Total time: 2.99s
 Now time: Sat Mar 20 09:47:22 2021

2021-03-20 09:47:23  [Epoch 45]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.0623]	 timer_model: 59.62 + timer_data: 0.30s
[3200/16000]	[L1: 4.9662]	 timer_model: 59.54 + timer_data: 0.04s
[4800/16000]	[L1: 4.9024]	 timer_model: 59.64 + timer_data: 0.04s
[6400/16000]	[L1: 4.9357]	 timer_model: 59.54 + timer_data: 0.03s
[8000/16000]	[L1: 4.9853]	 timer_model: 59.65 + timer_data: 0.04s
[9600/16000]	[L1: 4.9282]	 timer_model: 59.49 + timer_data: 0.04s
[11200/16000]	[L1: 4.9184]	 timer_model: 59.65 + timer_data: 0.03s
[12800/16000]	[L1: 4.9349]	 timer_model: 59.59 + timer_data: 0.04s
[14400/16000]	[L1: 4.9315]	 timer_model: 59.52 + timer_data: 0.04s
[16000/16000]	[L1: 4.9188]	 timer_model: 59.51 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.687 (Best: 34.796 @epoch 30)
Total time: 2.97s
 Now time: Sat Mar 20 09:57:23 2021

2021-03-20 09:57:24  [Epoch 46]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.5892]	 timer_model: 59.56 + timer_data: 0.29s
[3200/16000]	[L1: 4.5583]	 timer_model: 59.47 + timer_data: 0.04s
[4800/16000]	[L1: 4.6967]	 timer_model: 59.67 + timer_data: 0.04s
[6400/16000]	[L1: 4.8503]	 timer_model: 59.78 + timer_data: 0.04s
[8000/16000]	[L1: 5.1555]	 timer_model: 59.59 + timer_data: 0.04s
[9600/16000]	[L1: 5.1105]	 timer_model: 59.52 + timer_data: 0.04s
[11200/16000]	[L1: 5.0910]	 timer_model: 59.59 + timer_data: 0.04s
[12800/16000]	[L1: 5.0828]	 timer_model: 59.62 + timer_data: 0.03s
[14400/16000]	[L1: 5.0449]	 timer_model: 59.59 + timer_data: 0.03s
[16000/16000]	[L1: 5.0218]	 timer_model: 59.52 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.754 (Best: 34.796 @epoch 30)
Total time: 2.97s
 Now time: Sat Mar 20 10:07:24 2021

2021-03-20 10:07:25  [Epoch 47]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.8588]	 timer_model: 59.61 + timer_data: 0.28s
[3200/16000]	[L1: 4.7392]	 timer_model: 59.50 + timer_data: 0.04s
[4800/16000]	[L1: 4.7379]	 timer_model: 59.56 + timer_data: 0.04s
[6400/16000]	[L1: 4.7649]	 timer_model: 59.86 + timer_data: 0.04s
[8000/16000]	[L1: 4.7684]	 timer_model: 59.64 + timer_data: 0.04s
[9600/16000]	[L1: 4.7623]	 timer_model: 59.59 + timer_data: 0.04s
[11200/16000]	[L1: 4.7409]	 timer_model: 59.72 + timer_data: 0.04s
[12800/16000]	[L1: 4.7459]	 timer_model: 59.59 + timer_data: 0.04s
[14400/16000]	[L1: 4.7389]	 timer_model: 59.81 + timer_data: 0.04s
[16000/16000]	[L1: 4.7339]	 timer_model: 59.48 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 35.133 (Best: 35.133 @epoch 47)
Total time: 3.14s
 Now time: Sat Mar 20 10:17:26 2021

2021-03-20 10:17:28  [Epoch 48]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.6121]	 timer_model: 59.43 + timer_data: 0.31s
[3200/16000]	[L1: 4.6118]	 timer_model: 59.42 + timer_data: 0.05s
[4800/16000]	[L1: 5.0297]	 timer_model: 59.57 + timer_data: 0.05s
[6400/16000]	[L1: 5.0985]	 timer_model: 59.56 + timer_data: 0.05s
[8000/16000]	[L1: 5.0765]	 timer_model: 59.89 + timer_data: 0.05s
[9600/16000]	[L1: 5.0233]	 timer_model: 59.45 + timer_data: 0.04s
[11200/16000]	[L1: 4.9858]	 timer_model: 59.45 + timer_data: 0.05s
[12800/16000]	[L1: 4.9317]	 timer_model: 59.55 + timer_data: 0.04s
[14400/16000]	[L1: 5.1263]	 timer_model: 59.57 + timer_data: 0.03s
[16000/16000]	[L1: 5.1159]	 timer_model: 59.55 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.793 (Best: 35.133 @epoch 47)
Total time: 3.04s
 Now time: Sat Mar 20 10:27:27 2021

2021-03-20 10:27:29  [Epoch 49]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.7829]	 timer_model: 59.65 + timer_data: 0.28s
[3200/16000]	[L1: 4.8099]	 timer_model: 59.59 + timer_data: 0.04s
[4800/16000]	[L1: 4.7926]	 timer_model: 59.55 + timer_data: 0.03s
[6400/16000]	[L1: 5.1270]	 timer_model: 59.46 + timer_data: 0.04s
[8000/16000]	[L1: 5.1391]	 timer_model: 59.27 + timer_data: 0.04s
[9600/16000]	[L1: 5.0943]	 timer_model: 59.65 + timer_data: 0.04s
[11200/16000]	[L1: 5.0257]	 timer_model: 59.41 + timer_data: 0.03s
[12800/16000]	[L1: 5.3218]	 timer_model: 59.48 + timer_data: 0.03s
[14400/16000]	[L1: 5.2890]	 timer_model: 59.62 + timer_data: 0.03s
[16000/16000]	[L1: 5.2502]	 timer_model: 59.61 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.746 (Best: 35.133 @epoch 47)
Total time: 3.05s
 Now time: Sat Mar 20 10:37:28 2021

2021-03-20 10:37:30  [Epoch 50]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 6.8280]	 timer_model: 59.67 + timer_data: 0.28s
[3200/16000]	[L1: 6.1755]	 timer_model: 59.46 + timer_data: 0.03s
[4800/16000]	[L1: 5.8002]	 timer_model: 60.01 + timer_data: 0.04s
[6400/16000]	[L1: 5.5322]	 timer_model: 59.82 + timer_data: 0.04s
[8000/16000]	[L1: 5.3806]	 timer_model: 60.54 + timer_data: 0.04s
[9600/16000]	[L1: 5.2629]	 timer_model: 59.88 + timer_data: 0.04s
[11200/16000]	[L1: 5.1892]	 timer_model: 59.79 + timer_data: 0.04s
[12800/16000]	[L1: 5.1319]	 timer_model: 59.81 + timer_data: 0.04s
[14400/16000]	[L1: 5.0976]	 timer_model: 59.60 + timer_data: 0.04s
[16000/16000]	[L1: 5.0646]	 timer_model: 59.48 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 34.174 (Best: 35.133 @epoch 47)
Total time: 3.10s
 Now time: Sat Mar 20 10:47:32 2021

2021-03-20 10:47:33  [Epoch 51]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.8658]	 timer_model: 59.52 + timer_data: 0.29s
[3200/16000]	[L1: 5.0926]	 timer_model: 59.80 + timer_data: 0.03s
[4800/16000]	[L1: 5.0302]	 timer_model: 59.73 + timer_data: 0.03s
[6400/16000]	[L1: 4.9605]	 timer_model: 59.53 + timer_data: 0.03s
[8000/16000]	[L1: 4.8991]	 timer_model: 59.48 + timer_data: 0.03s
[9600/16000]	[L1: 4.8696]	 timer_model: 59.49 + timer_data: 0.04s
[11200/16000]	[L1: 4.8083]	 timer_model: 59.65 + timer_data: 0.04s
[12800/16000]	[L1: 4.7809]	 timer_model: 59.63 + timer_data: 0.03s
[14400/16000]	[L1: 4.7503]	 timer_model: 59.72 + timer_data: 0.04s
[16000/16000]	[L1: 4.7196]	 timer_model: 59.48 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 35.268 (Best: 35.268 @epoch 51)
Total time: 3.32s
 Now time: Sat Mar 20 10:57:33 2021

2021-03-20 10:57:35  [Epoch 52]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 9.2245]	 timer_model: 59.52 + timer_data: 0.28s
[3200/16000]	[L1: 7.4385]	 timer_model: 59.51 + timer_data: 0.03s
[4800/16000]	[L1: 6.6246]	 timer_model: 59.58 + timer_data: 0.04s
[6400/16000]	[L1: 6.2282]	 timer_model: 59.73 + timer_data: 0.04s
[8000/16000]	[L1: 6.6778]	 timer_model: 59.46 + timer_data: 0.04s
[9600/16000]	[L1: 6.4295]	 timer_model: 59.48 + timer_data: 0.04s
[11200/16000]	[L1: 6.1921]	 timer_model: 59.39 + timer_data: 0.04s
[12800/16000]	[L1: 6.0202]	 timer_model: 59.49 + timer_data: 0.03s
[14400/16000]	[L1: 5.9282]	 timer_model: 59.72 + timer_data: 0.03s
[16000/16000]	[L1: 5.8491]	 timer_model: 59.49 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.519 (Best: 35.268 @epoch 51)
Total time: 3.09s
 Now time: Sat Mar 20 11:07:34 2021

2021-03-20 11:07:36  [Epoch 53]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 6.1036]	 timer_model: 59.62 + timer_data: 0.28s
[3200/16000]	[L1: 5.5393]	 timer_model: 59.53 + timer_data: 0.04s
[4800/16000]	[L1: 5.3589]	 timer_model: 59.52 + timer_data: 0.04s
[6400/16000]	[L1: 5.1688]	 timer_model: 59.60 + timer_data: 0.04s
[8000/16000]	[L1: 5.1103]	 timer_model: 59.38 + timer_data: 0.03s
[9600/16000]	[L1: 5.0230]	 timer_model: 59.52 + timer_data: 0.03s
[11200/16000]	[L1: 4.9895]	 timer_model: 59.49 + timer_data: 0.03s
[12800/16000]	[L1: 4.9709]	 timer_model: 59.51 + timer_data: 0.03s
[14400/16000]	[L1: 4.9278]	 timer_model: 59.67 + timer_data: 0.04s
[16000/16000]	[L1: 4.8842]	 timer_model: 59.69 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 35.028 (Best: 35.268 @epoch 51)
Total time: 3.09s
 Now time: Sat Mar 20 11:17:35 2021

2021-03-20 11:17:37  [Epoch 54]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.0084]	 timer_model: 59.60 + timer_data: 0.27s
[3200/16000]	[L1: 4.8793]	 timer_model: 59.46 + timer_data: 0.03s
[4800/16000]	[L1: 4.7947]	 timer_model: 59.58 + timer_data: 0.03s
[6400/16000]	[L1: 4.7304]	 timer_model: 59.68 + timer_data: 0.03s
[8000/16000]	[L1: 4.7011]	 timer_model: 59.51 + timer_data: 0.03s
[9600/16000]	[L1: 4.6650]	 timer_model: 59.53 + timer_data: 0.04s
[11200/16000]	[L1: 4.6126]	 timer_model: 59.53 + timer_data: 0.04s
[12800/16000]	[L1: 5.0015]	 timer_model: 59.59 + timer_data: 0.04s
[14400/16000]	[L1: 5.0329]	 timer_model: 59.65 + timer_data: 0.04s
[16000/16000]	[L1: 5.0064]	 timer_model: 59.65 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.906 (Best: 35.268 @epoch 51)
Total time: 3.12s
 Now time: Sat Mar 20 11:27:37 2021

2021-03-20 11:27:38  [Epoch 55]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.6250]	 timer_model: 59.66 + timer_data: 0.28s
[3200/16000]	[L1: 4.6594]	 timer_model: 59.72 + timer_data: 0.04s
[4800/16000]	[L1: 4.7033]	 timer_model: 59.74 + timer_data: 0.04s
[6400/16000]	[L1: 4.6814]	 timer_model: 59.74 + timer_data: 0.04s
[8000/16000]	[L1: 4.6535]	 timer_model: 59.66 + timer_data: 0.04s
[9600/16000]	[L1: 4.6358]	 timer_model: 59.71 + timer_data: 0.04s
[11200/16000]	[L1: 4.6371]	 timer_model: 59.54 + timer_data: 0.03s
[12800/16000]	[L1: 4.8960]	 timer_model: 59.68 + timer_data: 0.04s
[14400/16000]	[L1: 4.9858]	 timer_model: 59.52 + timer_data: 0.03s
[16000/16000]	[L1: 4.9785]	 timer_model: 59.47 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.777 (Best: 35.268 @epoch 51)
Total time: 3.28s
 Now time: Sat Mar 20 11:37:39 2021

2021-03-20 11:37:40  [Epoch 56]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.8625]	 timer_model: 59.83 + timer_data: 0.28s
[3200/16000]	[L1: 4.7380]	 timer_model: 64.30 + timer_data: 0.05s
[4800/16000]	[L1: 4.7516]	 timer_model: 61.63 + timer_data: 0.05s
[6400/16000]	[L1: 4.6985]	 timer_model: 59.86 + timer_data: 0.04s
[8000/16000]	[L1: 4.6513]	 timer_model: 59.70 + timer_data: 0.04s
[9600/16000]	[L1: 4.6425]	 timer_model: 59.67 + timer_data: 0.04s
[11200/16000]	[L1: 4.6546]	 timer_model: 59.40 + timer_data: 0.04s
[12800/16000]	[L1: 4.6311]	 timer_model: 59.53 + timer_data: 0.04s
[14400/16000]	[L1: 4.6725]	 timer_model: 59.75 + timer_data: 0.04s
[16000/16000]	[L1: 4.8107]	 timer_model: 59.51 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.660 (Best: 35.268 @epoch 51)
Total time: 3.24s
 Now time: Sat Mar 20 11:47:47 2021

2021-03-20 11:47:49  [Epoch 57]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.7696]	 timer_model: 59.76 + timer_data: 0.29s
[3200/16000]	[L1: 4.7266]	 timer_model: 59.60 + timer_data: 0.04s
[4800/16000]	[L1: 7.8226]	 timer_model: 59.52 + timer_data: 0.04s
[6400/16000]	[L1: 7.2469]	 timer_model: 59.77 + timer_data: 0.03s
[8000/16000]	[L1: 6.8419]	 timer_model: 59.55 + timer_data: 0.03s
[9600/16000]	[L1: 6.5339]	 timer_model: 59.50 + timer_data: 0.04s
[11200/16000]	[L1: 6.3085]	 timer_model: 59.40 + timer_data: 0.04s
[12800/16000]	[L1: 6.1247]	 timer_model: 59.52 + timer_data: 0.04s
[14400/16000]	[L1: 5.9863]	 timer_model: 59.65 + timer_data: 0.03s
[16000/16000]	[L1: 5.8755]	 timer_model: 59.47 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 34.767 (Best: 35.268 @epoch 51)
Total time: 3.08s
 Now time: Sat Mar 20 11:57:48 2021

2021-03-20 11:57:50  [Epoch 58]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 8.4396]	 timer_model: 59.71 + timer_data: 0.28s
[3200/16000]	[L1: 6.8411]	 timer_model: 59.48 + timer_data: 0.03s
[4800/16000]	[L1: 6.1721]	 timer_model: 59.71 + timer_data: 0.03s
[6400/16000]	[L1: 5.7884]	 timer_model: 59.76 + timer_data: 0.03s
[8000/16000]	[L1: 5.5807]	 timer_model: 59.49 + timer_data: 0.03s
[9600/16000]	[L1: 5.4110]	 timer_model: 59.59 + timer_data: 0.04s
[11200/16000]	[L1: 5.3006]	 timer_model: 59.49 + timer_data: 0.04s
[12800/16000]	[L1: 5.1900]	 timer_model: 59.46 + timer_data: 0.04s
[14400/16000]	[L1: 5.0971]	 timer_model: 59.53 + timer_data: 0.04s
[16000/16000]	[L1: 5.0394]	 timer_model: 59.56 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 34.945 (Best: 35.268 @epoch 51)
Total time: 3.07s
 Now time: Sat Mar 20 12:07:49 2021

2021-03-20 12:07:51  [Epoch 59]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.6037]	 timer_model: 59.53 + timer_data: 0.29s
[3200/16000]	[L1: 8.0955]	 timer_model: 59.49 + timer_data: 0.04s
[4800/16000]	[L1: 7.2668]	 timer_model: 59.66 + timer_data: 0.04s
[6400/16000]	[L1: 6.6899]	 timer_model: 59.73 + timer_data: 0.03s
[8000/16000]	[L1: 6.3243]	 timer_model: 59.44 + timer_data: 0.03s
[9600/16000]	[L1: 6.0504]	 timer_model: 59.43 + timer_data: 0.03s
[11200/16000]	[L1: 5.8691]	 timer_model: 59.40 + timer_data: 0.04s
[12800/16000]	[L1: 5.7234]	 timer_model: 59.78 + timer_data: 0.04s
[14400/16000]	[L1: 5.6046]	 timer_model: 59.63 + timer_data: 0.04s
[16000/16000]	[L1: 5.5171]	 timer_model: 59.61 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.904 (Best: 35.268 @epoch 51)
Total time: 3.08s
 Now time: Sat Mar 20 12:17:51 2021

2021-03-20 12:17:52  [Epoch 60]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.6832]	 timer_model: 59.59 + timer_data: 0.29s
[3200/16000]	[L1: 4.5471]	 timer_model: 59.56 + timer_data: 0.04s
[4800/16000]	[L1: 4.5256]	 timer_model: 59.43 + timer_data: 0.03s
[6400/16000]	[L1: 4.5419]	 timer_model: 59.56 + timer_data: 0.03s
[8000/16000]	[L1: 4.5309]	 timer_model: 59.75 + timer_data: 0.04s
[9600/16000]	[L1: 4.5478]	 timer_model: 59.66 + timer_data: 0.04s
[11200/16000]	[L1: 4.5433]	 timer_model: 59.64 + timer_data: 0.04s
[12800/16000]	[L1: 4.5658]	 timer_model: 59.52 + timer_data: 0.04s
[14400/16000]	[L1: 4.5731]	 timer_model: 59.61 + timer_data: 0.04s
[16000/16000]	[L1: 5.3796]	 timer_model: 59.47 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 33.937 (Best: 35.268 @epoch 51)
Total time: 2.99s
 Now time: Sat Mar 20 12:27:52 2021

2021-03-20 12:27:53  [Epoch 61]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.2101]	 timer_model: 59.78 + timer_data: 0.29s
[3200/16000]	[L1: 5.1695]	 timer_model: 59.68 + timer_data: 0.04s
[4800/16000]	[L1: 5.0826]	 timer_model: 59.59 + timer_data: 0.04s
[6400/16000]	[L1: 5.1355]	 timer_model: 59.78 + timer_data: 0.04s
[8000/16000]	[L1: 5.0246]	 timer_model: 59.59 + timer_data: 0.04s
[9600/16000]	[L1: 4.9824]	 timer_model: 59.57 + timer_data: 0.03s
[11200/16000]	[L1: 4.9500]	 timer_model: 59.64 + timer_data: 0.04s
[12800/16000]	[L1: 4.9027]	 timer_model: 59.57 + timer_data: 0.04s
[14400/16000]	[L1: 4.8819]	 timer_model: 59.64 + timer_data: 0.04s
[16000/16000]	[L1: 4.8581]	 timer_model: 59.56 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 35.110 (Best: 35.268 @epoch 51)
Total time: 3.03s
 Now time: Sat Mar 20 12:37:54 2021

2021-03-20 12:37:55  [Epoch 62]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.6098]	 timer_model: 59.64 + timer_data: 0.28s
[3200/16000]	[L1: 4.5059]	 timer_model: 59.71 + timer_data: 0.04s
[4800/16000]	[L1: 5.8750]	 timer_model: 59.54 + timer_data: 0.03s
[6400/16000]	[L1: 6.8523]	 timer_model: 59.82 + timer_data: 0.04s
[8000/16000]	[L1: 6.5719]	 timer_model: 59.62 + timer_data: 0.04s
[9600/16000]	[L1: 6.4274]	 timer_model: 59.54 + timer_data: 0.04s
[11200/16000]	[L1: 6.2232]	 timer_model: 59.44 + timer_data: 0.03s
[12800/16000]	[L1: 6.0562]	 timer_model: 59.72 + timer_data: 0.04s
[14400/16000]	[L1: 5.8979]	 timer_model: 59.61 + timer_data: 0.04s
[16000/16000]	[L1: 5.7713]	 timer_model: 59.57 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.661 (Best: 35.268 @epoch 51)
Total time: 3.06s
 Now time: Sat Mar 20 12:47:55 2021

2021-03-20 12:47:57  [Epoch 63]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.7142]	 timer_model: 59.52 + timer_data: 0.28s
[3200/16000]	[L1: 4.5969]	 timer_model: 59.36 + timer_data: 0.04s
[4800/16000]	[L1: 4.5939]	 timer_model: 59.48 + timer_data: 0.04s
[6400/16000]	[L1: 9.7364]	 timer_model: 59.61 + timer_data: 0.03s
[8000/16000]	[L1: 9.9790]	 timer_model: 59.45 + timer_data: 0.03s
[9600/16000]	[L1: 9.9100]	 timer_model: 59.62 + timer_data: 0.03s
[11200/16000]	[L1: 9.3488]	 timer_model: 59.43 + timer_data: 0.03s
[12800/16000]	[L1: 8.8343]	 timer_model: 59.60 + timer_data: 0.03s
[14400/16000]	[L1: 8.4172]	 timer_model: 59.62 + timer_data: 0.03s
[16000/16000]	[L1: 8.0763]	 timer_model: 59.53 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.351 (Best: 35.268 @epoch 51)
Total time: 3.24s
 Now time: Sat Mar 20 12:57:57 2021

2021-03-20 12:57:58  [Epoch 64]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.9535]	 timer_model: 59.92 + timer_data: 0.29s
[3200/16000]	[L1: 16.5747]	 timer_model: 59.66 + timer_data: 0.04s
[4800/16000]	[L1: 13.5554]	 timer_model: 59.77 + timer_data: 0.03s
[6400/16000]	[L1: 11.6110]	 timer_model: 59.64 + timer_data: 0.03s
[8000/16000]	[L1: 10.4051]	 timer_model: 59.41 + timer_data: 0.03s
[9600/16000]	[L1: 9.5442]	 timer_model: 59.55 + timer_data: 0.03s
[11200/16000]	[L1: 8.9028]	 timer_model: 59.44 + timer_data: 0.03s
[12800/16000]	[L1: 8.4445]	 timer_model: 59.63 + timer_data: 0.04s
[14400/16000]	[L1: 8.0729]	 timer_model: 59.66 + timer_data: 0.04s
[16000/16000]	[L1: 7.7570]	 timer_model: 59.72 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.572 (Best: 35.268 @epoch 51)
Total time: 3.17s
 Now time: Sat Mar 20 13:07:58 2021

2021-03-20 13:08:00  [Epoch 65]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.7844]	 timer_model: 60.46 + timer_data: 0.36s
[3200/16000]	[L1: 4.8011]	 timer_model: 59.80 + timer_data: 0.04s
[4800/16000]	[L1: 4.8038]	 timer_model: 59.56 + timer_data: 0.04s
[6400/16000]	[L1: 4.8110]	 timer_model: 59.80 + timer_data: 0.04s
[8000/16000]	[L1: 4.8198]	 timer_model: 59.65 + timer_data: 0.04s
[9600/16000]	[L1: 4.7982]	 timer_model: 59.53 + timer_data: 0.04s
[11200/16000]	[L1: 4.7676]	 timer_model: 59.51 + timer_data: 0.04s
[12800/16000]	[L1: 6.1654]	 timer_model: 59.66 + timer_data: 0.04s
[14400/16000]	[L1: 6.1302]	 timer_model: 59.59 + timer_data: 0.04s
[16000/16000]	[L1: 6.0557]	 timer_model: 59.63 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.012 (Best: 35.268 @epoch 51)
Total time: 3.21s
 Now time: Sat Mar 20 13:18:01 2021

2021-03-20 13:18:03  [Epoch 66]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.1785]	 timer_model: 59.60 + timer_data: 0.28s
[3200/16000]	[L1: 5.6304]	 timer_model: 59.55 + timer_data: 0.03s
[4800/16000]	[L1: 5.5708]	 timer_model: 59.50 + timer_data: 0.03s
[6400/16000]	[L1: 5.4796]	 timer_model: 60.47 + timer_data: 0.04s
[8000/16000]	[L1: 5.3751]	 timer_model: 59.51 + timer_data: 0.04s
[9600/16000]	[L1: 5.2977]	 timer_model: 59.42 + timer_data: 0.04s
[11200/16000]	[L1: 5.2390]	 timer_model: 59.51 + timer_data: 0.04s
[12800/16000]	[L1: 5.1925]	 timer_model: 59.59 + timer_data: 0.04s
[14400/16000]	[L1: 5.1343]	 timer_model: 59.59 + timer_data: 0.03s
[16000/16000]	[L1: 5.0849]	 timer_model: 59.56 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 35.006 (Best: 35.268 @epoch 51)
Total time: 3.06s
 Now time: Sat Mar 20 13:28:03 2021

2021-03-20 13:28:05  [Epoch 67]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.7431]	 timer_model: 59.60 + timer_data: 0.28s
[3200/16000]	[L1: 4.6786]	 timer_model: 59.60 + timer_data: 0.04s
[4800/16000]	[L1: 4.6467]	 timer_model: 59.53 + timer_data: 0.04s
[6400/16000]	[L1: 4.6681]	 timer_model: 59.67 + timer_data: 0.04s
[8000/16000]	[L1: 4.6361]	 timer_model: 59.65 + timer_data: 0.04s
[9600/16000]	[L1: 4.6300]	 timer_model: 59.65 + timer_data: 0.04s
[11200/16000]	[L1: 4.6257]	 timer_model: 59.59 + timer_data: 0.04s
[12800/16000]	[L1: 4.6443]	 timer_model: 59.78 + timer_data: 0.04s
[14400/16000]	[L1: 4.6317]	 timer_model: 59.67 + timer_data: 0.04s
[16000/16000]	[L1: 4.6112]	 timer_model: 59.47 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 35.093 (Best: 35.268 @epoch 51)
Total time: 3.21s
 Now time: Sat Mar 20 13:38:05 2021

2021-03-20 13:38:06  [Epoch 68]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.6702]	 timer_model: 59.48 + timer_data: 0.28s
[3200/16000]	[L1: 4.6232]	 timer_model: 59.41 + timer_data: 0.03s
[4800/16000]	[L1: 4.6458]	 timer_model: 59.58 + timer_data: 0.03s
[6400/16000]	[L1: 4.5822]	 timer_model: 59.90 + timer_data: 0.04s
[8000/16000]	[L1: 4.5729]	 timer_model: 59.54 + timer_data: 0.04s
[9600/16000]	[L1: 4.5597]	 timer_model: 59.55 + timer_data: 0.04s
[11200/16000]	[L1: 4.5706]	 timer_model: 59.61 + timer_data: 0.03s
[12800/16000]	[L1: 4.5687]	 timer_model: 59.64 + timer_data: 0.03s
[14400/16000]	[L1: 4.5623]	 timer_model: 59.71 + timer_data: 0.03s
[16000/16000]	[L1: 4.5987]	 timer_model: 59.64 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 34.722 (Best: 35.268 @epoch 51)
Total time: 3.14s
 Now time: Sat Mar 20 13:48:06 2021

2021-03-20 13:48:08  [Epoch 69]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.5630]	 timer_model: 60.16 + timer_data: 0.29s
[3200/16000]	[L1: 12.7676]	 timer_model: 59.71 + timer_data: 0.05s
[4800/16000]	[L1: 11.1392]	 timer_model: 59.61 + timer_data: 0.05s
[6400/16000]	[L1: 9.7074]	 timer_model: 63.39 + timer_data: 0.05s
[8000/16000]	[L1: 8.7460]	 timer_model: 64.22 + timer_data: 0.05s
[9600/16000]	[L1: 8.1479]	 timer_model: 63.40 + timer_data: 0.05s
[11200/16000]	[L1: 7.6794]	 timer_model: 59.99 + timer_data: 0.05s
[12800/16000]	[L1: 7.3262]	 timer_model: 59.92 + timer_data: 0.04s
[14400/16000]	[L1: 7.0456]	 timer_model: 59.63 + timer_data: 0.04s
[16000/16000]	[L1: 6.8100]	 timer_model: 59.58 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.962 (Best: 35.268 @epoch 51)
Total time: 3.07s
 Now time: Sat Mar 20 13:58:22 2021

2021-03-20 13:58:23  [Epoch 70]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.5843]	 timer_model: 59.73 + timer_data: 0.35s
[3200/16000]	[L1: 4.5717]	 timer_model: 59.69 + timer_data: 0.04s
[4800/16000]	[L1: 4.5316]	 timer_model: 59.61 + timer_data: 0.03s
[6400/16000]	[L1: 4.5547]	 timer_model: 59.64 + timer_data: 0.03s
[8000/16000]	[L1: 4.5682]	 timer_model: 59.52 + timer_data: 0.03s
[9600/16000]	[L1: 4.7875]	 timer_model: 59.51 + timer_data: 0.03s
[11200/16000]	[L1: 5.3353]	 timer_model: 59.65 + timer_data: 0.03s
[12800/16000]	[L1: 5.2970]	 timer_model: 59.61 + timer_data: 0.03s
[14400/16000]	[L1: 5.4370]	 timer_model: 59.55 + timer_data: 0.04s
[16000/16000]	[L1: 5.4047]	 timer_model: 59.55 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.596 (Best: 35.268 @epoch 51)
Total time: 3.14s
 Now time: Sat Mar 20 14:08:24 2021

2021-03-20 14:08:25  [Epoch 71]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.6809]	 timer_model: 60.44 + timer_data: 0.29s
[3200/16000]	[L1: 4.7503]	 timer_model: 60.01 + timer_data: 0.04s
[4800/16000]	[L1: 4.7217]	 timer_model: 59.65 + timer_data: 0.04s
[6400/16000]	[L1: 4.7116]	 timer_model: 59.49 + timer_data: 0.04s
[8000/16000]	[L1: 4.7106]	 timer_model: 59.59 + timer_data: 0.04s
[9600/16000]	[L1: 4.6841]	 timer_model: 59.58 + timer_data: 0.03s
[11200/16000]	[L1: 4.6943]	 timer_model: 59.57 + timer_data: 0.04s
[12800/16000]	[L1: 4.6771]	 timer_model: 59.63 + timer_data: 0.04s
[14400/16000]	[L1: 4.6708]	 timer_model: 59.64 + timer_data: 0.04s
[16000/16000]	[L1: 4.6407]	 timer_model: 59.64 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 35.188 (Best: 35.268 @epoch 51)
Total time: 3.05s
 Now time: Sat Mar 20 14:18:27 2021

2021-03-20 14:18:28  [Epoch 72]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 8.8129]	 timer_model: 60.34 + timer_data: 0.28s
[3200/16000]	[L1: 7.2096]	 timer_model: 60.01 + timer_data: 0.03s
[4800/16000]	[L1: 6.4161]	 timer_model: 59.76 + timer_data: 0.03s
[6400/16000]	[L1: 6.0450]	 timer_model: 59.51 + timer_data: 0.04s
[8000/16000]	[L1: 5.7668]	 timer_model: 59.65 + timer_data: 0.04s
[9600/16000]	[L1: 5.5712]	 timer_model: 59.55 + timer_data: 0.03s
[11200/16000]	[L1: 9.4359]	 timer_model: 59.66 + timer_data: 0.03s
[12800/16000]	[L1: 9.0090]	 timer_model: 59.69 + timer_data: 0.03s
[14400/16000]	[L1: 8.6216]	 timer_model: 59.59 + timer_data: 0.04s
[16000/16000]	[L1: 8.2929]	 timer_model: 59.59 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.189 (Best: 35.268 @epoch 51)
Total time: 3.10s
 Now time: Sat Mar 20 14:28:29 2021

2021-03-20 14:28:31  [Epoch 73]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 5.5857]	 timer_model: 59.63 + timer_data: 0.28s
[3200/16000]	[L1: 5.3694]	 timer_model: 59.63 + timer_data: 0.03s
[4800/16000]	[L1: 5.2122]	 timer_model: 59.62 + timer_data: 0.04s
[6400/16000]	[L1: 5.1008]	 timer_model: 59.48 + timer_data: 0.04s
[8000/16000]	[L1: 5.0475]	 timer_model: 59.51 + timer_data: 0.04s
[9600/16000]	[L1: 5.0190]	 timer_model: 59.44 + timer_data: 0.04s
[11200/16000]	[L1: 4.9776]	 timer_model: 59.52 + timer_data: 0.03s
[12800/16000]	[L1: 4.9515]	 timer_model: 59.65 + timer_data: 0.03s
[14400/16000]	[L1: 4.9397]	 timer_model: 59.65 + timer_data: 0.04s
[16000/16000]	[L1: 4.9224]	 timer_model: 59.65 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.691 (Best: 35.268 @epoch 51)
Total time: 3.04s
 Now time: Sat Mar 20 14:38:30 2021

2021-03-20 14:38:32  [Epoch 74]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.7417]	 timer_model: 60.04 + timer_data: 0.28s
[3200/16000]	[L1: 4.6201]	 timer_model: 60.54 + timer_data: 0.03s
[4800/16000]	[L1: 4.6746]	 timer_model: 59.78 + timer_data: 0.04s
[6400/16000]	[L1: 4.6593]	 timer_model: 59.49 + timer_data: 0.04s
[8000/16000]	[L1: 4.6455]	 timer_model: 59.41 + timer_data: 0.04s
[9600/16000]	[L1: 4.6502]	 timer_model: 59.35 + timer_data: 0.04s
[11200/16000]	[L1: 4.6589]	 timer_model: 59.45 + timer_data: 0.04s
[12800/16000]	[L1: 4.6467]	 timer_model: 59.59 + timer_data: 0.03s
[14400/16000]	[L1: 4.6378]	 timer_model: 59.59 + timer_data: 0.04s
[16000/16000]	[L1: 4.6312]	 timer_model: 59.46 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 34.952 (Best: 35.268 @epoch 51)
Total time: 3.02s
 Now time: Sat Mar 20 14:48:32 2021

2021-03-20 14:48:34  [Epoch 75]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.6695]	 timer_model: 59.97 + timer_data: 0.28s
[3200/16000]	[L1: 4.6516]	 timer_model: 59.83 + timer_data: 0.04s
[4800/16000]	[L1: 4.6409]	 timer_model: 59.69 + timer_data: 0.04s
[6400/16000]	[L1: 4.6660]	 timer_model: 59.70 + timer_data: 0.03s
[8000/16000]	[L1: 4.6856]	 timer_model: 59.59 + timer_data: 0.03s
[9600/16000]	[L1: 4.6501]	 timer_model: 59.53 + timer_data: 0.03s
[11200/16000]	[L1: 4.6423]	 timer_model: 59.58 + timer_data: 0.03s
[12800/16000]	[L1: 4.6188]	 timer_model: 59.62 + timer_data: 0.04s
[14400/16000]	[L1: 4.6199]	 timer_model: 59.42 + timer_data: 0.04s
[16000/16000]	[L1: 4.5998]	 timer_model: 59.49 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 35.234 (Best: 35.268 @epoch 51)
Total time: 3.06s
 Now time: Sat Mar 20 14:58:34 2021

2021-03-20 14:58:36  [Epoch 76]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.6669]	 timer_model: 59.51 + timer_data: 0.28s
[3200/16000]	[L1: 4.7014]	 timer_model: 59.51 + timer_data: 0.04s
[4800/16000]	[L1: 4.6283]	 timer_model: 59.67 + timer_data: 0.04s
[6400/16000]	[L1: 4.6407]	 timer_model: 59.39 + timer_data: 0.03s
[8000/16000]	[L1: 4.6366]	 timer_model: 59.47 + timer_data: 0.03s
[9600/16000]	[L1: 4.6442]	 timer_model: 59.42 + timer_data: 0.03s
[11200/16000]	[L1: 4.6454]	 timer_model: 59.53 + timer_data: 0.04s
[12800/16000]	[L1: 4.6396]	 timer_model: 59.62 + timer_data: 0.04s
[14400/16000]	[L1: 4.6116]	 timer_model: 59.58 + timer_data: 0.04s
[16000/16000]	[L1: 6.1218]	 timer_model: 59.42 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 29.764 (Best: 35.268 @epoch 51)
Total time: 2.92s
 Now time: Sat Mar 20 15:08:35 2021

2021-03-20 15:08:36  [Epoch 77]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 6.0163]	 timer_model: 59.69 + timer_data: 0.28s
[3200/16000]	[L1: 5.5799]	 timer_model: 59.51 + timer_data: 0.03s
[4800/16000]	[L1: 5.2520]	 timer_model: 59.54 + timer_data: 0.03s
[6400/16000]	[L1: 5.1139]	 timer_model: 59.77 + timer_data: 0.04s
[8000/16000]	[L1: 5.0414]	 timer_model: 59.52 + timer_data: 0.04s
[9600/16000]	[L1: 4.9706]	 timer_model: 59.62 + timer_data: 0.04s
[11200/16000]	[L1: 4.9189]	 timer_model: 59.69 + timer_data: 0.04s
[12800/16000]	[L1: 4.8828]	 timer_model: 59.84 + timer_data: 0.03s
[14400/16000]	[L1: 4.8367]	 timer_model: 59.49 + timer_data: 0.04s
[16000/16000]	[L1: 4.8189]	 timer_model: 59.59 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 35.245 (Best: 35.268 @epoch 51)
Total time: 3.03s
 Now time: Sat Mar 20 15:18:37 2021

2021-03-20 15:18:38  [Epoch 78]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.5312]	 timer_model: 59.58 + timer_data: 0.28s
[3200/16000]	[L1: 14.5502]	 timer_model: 59.65 + timer_data: 0.04s
[4800/16000]	[L1: 12.5611]	 timer_model: 59.64 + timer_data: 0.03s
[6400/16000]	[L1: 10.7839]	 timer_model: 59.49 + timer_data: 0.03s
[8000/16000]	[L1: 9.6672]	 timer_model: 59.56 + timer_data: 0.03s
[9600/16000]	[L1: 8.8922]	 timer_model: 59.51 + timer_data: 0.04s
[11200/16000]	[L1: 8.3187]	 timer_model: 59.53 + timer_data: 0.04s
[12800/16000]	[L1: 7.8876]	 timer_model: 59.67 + timer_data: 0.04s
[14400/16000]	[L1: 7.5392]	 timer_model: 59.66 + timer_data: 0.03s
[16000/16000]	[L1: 7.2576]	 timer_model: 59.51 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 35.013 (Best: 35.268 @epoch 51)
Total time: 3.19s
 Now time: Sat Mar 20 15:28:38 2021

2021-03-20 15:28:39  [Epoch 79]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.4926]	 timer_model: 59.63 + timer_data: 0.29s
[3200/16000]	[L1: 4.7403]	 timer_model: 59.75 + timer_data: 0.04s
[4800/16000]	[L1: 5.0282]	 timer_model: 59.53 + timer_data: 0.04s
[6400/16000]	[L1: 5.0637]	 timer_model: 59.44 + timer_data: 0.04s
[8000/16000]	[L1: 4.9950]	 timer_model: 59.57 + timer_data: 0.04s
[9600/16000]	[L1: 4.9222]	 timer_model: 59.59 + timer_data: 0.04s
[11200/16000]	[L1: 4.8948]	 timer_model: 59.50 + timer_data: 0.03s
[12800/16000]	[L1: 4.8710]	 timer_model: 59.57 + timer_data: 0.03s
[14400/16000]	[L1: 4.8427]	 timer_model: 59.59 + timer_data: 0.04s
[16000/16000]	[L1: 4.8209]	 timer_model: 59.43 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 35.070 (Best: 35.268 @epoch 51)
Total time: 3.07s
 Now time: Sat Mar 20 15:38:39 2021

2021-03-20 15:38:40  [Epoch 80]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.5648]	 timer_model: 59.74 + timer_data: 0.28s
[3200/16000]	[L1: 4.5807]	 timer_model: 59.64 + timer_data: 0.03s
[4800/16000]	[L1: 4.5926]	 timer_model: 59.67 + timer_data: 0.04s
[6400/16000]	[L1: 4.5475]	 timer_model: 59.59 + timer_data: 0.04s
[8000/16000]	[L1: 4.5151]	 timer_model: 59.52 + timer_data: 0.04s
[9600/16000]	[L1: 4.5016]	 timer_model: 59.58 + timer_data: 0.04s
[11200/16000]	[L1: 4.5077]	 timer_model: 59.67 + timer_data: 0.04s
[12800/16000]	[L1: 4.4913]	 timer_model: 59.60 + timer_data: 0.04s
[14400/16000]	[L1: 4.4790]	 timer_model: 59.54 + timer_data: 0.04s
[16000/16000]	[L1: 4.4806]	 timer_model: 59.58 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 35.206 (Best: 35.268 @epoch 51)
Total time: 3.25s
 Now time: Sat Mar 20 15:48:41 2021

2021-03-20 15:48:42  [Epoch 81]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4.3390]	 timer_model: 59.57 + timer_data: 0.28s
[3200/16000]	[L1: 4.4277]	 timer_model: 59.57 + timer_data: 0.04s
[4800/16000]	[L1: 4.4355]	 timer_model: 59.59 + timer_data: 0.04s
[6400/16000]	[L1: 17.8995]	 timer_model: 59.51 + timer_data: 0.04s
[8000/16000]	[L1: 169.1432]	 timer_model: 59.57 + timer_data: 0.04s
[9600/16000]	[L1: 331.2002]	 timer_model: 59.47 + timer_data: 0.03s
[11200/16000]	[L1: 469.7413]	 timer_model: 59.46 + timer_data: 0.03s
Skip this batch 733! (Loss: 5884124.5)
Skip this batch 734! (Loss: 5377202.0)
Skip this batch 735! (Loss: 5134220.0)
Skip this batch 736! (Loss: 5847725.5)
Skip this batch 737! (Loss: 6077322.0)
Skip this batch 738! (Loss: 5220323.5)
Skip this batch 739! (Loss: 6787023.0)
Skip this batch 740! (Loss: 5096475.5)
Skip this batch 741! (Loss: 5832822.5)
Skip this batch 742! (Loss: 5739758.5)
Skip this batch 743! (Loss: 5421304.5)
Skip this batch 744! (Loss: 5052743.5)
Skip this batch 745! (Loss: 5092186.0)
Skip this batch 746! (Loss: 5364786.5)
Skip this batch 747! (Loss: 6181563.5)
Skip this batch 749! (Loss: 4860330.0)
Skip this batch 788! (Loss: 10500037.0)
Skip this batch 789! (Loss: 9445510.0)
Skip this batch 790! (Loss: 11986257.0)
Skip this batch 791! (Loss: 12629541.0)
Skip this batch 792! (Loss: 9537348.0)
Skip this batch 793! (Loss: 10280029.0)
Skip this batch 794! (Loss: 13016744.0)
Skip this batch 795! (Loss: 8084449.0)
Skip this batch 796! (Loss: 9032512.0)
Skip this batch 797! (Loss: 12618808.0)
Skip this batch 798! (Loss: 15407600.0)
Skip this batch 799! (Loss: 9240507.0)
Skip this batch 800! (Loss: 10207727.0)
[12800/16000]	[L1: 305452.9062]	 timer_model: 46.48 + timer_data: 0.03s
Skip this batch 801! (Loss: 11078119.0)
Skip this batch 802! (Loss: 13554157.0)
Skip this batch 803! (Loss: 11837730.0)
Skip this batch 804! (Loss: 11286540.0)
Skip this batch 805! (Loss: 9156990.0)
Skip this batch 806! (Loss: 10194526.0)
Skip this batch 807! (Loss: 12550109.0)
Skip this batch 808! (Loss: 9299815.0)
Skip this batch 809! (Loss: 9708971.0)
Skip this batch 810! (Loss: 14939643.0)
Skip this batch 811! (Loss: 12280183.0)
Skip this batch 812! (Loss: 10252607.0)
Skip this batch 813! (Loss: 9703915.0)
Skip this batch 814! (Loss: 8949822.0)
Skip this batch 815! (Loss: 9169287.0)
Skip this batch 816! (Loss: 11443539.0)
Skip this batch 817! (Loss: 10144727.0)
Skip this batch 818! (Loss: 12091782.0)
Skip this batch 819! (Loss: 12247423.0)
Skip this batch 820! (Loss: 15707586.0)
Skip this batch 821! (Loss: 7314196.0)
Skip this batch 822! (Loss: 12708012.0)
Skip this batch 823! (Loss: 11419959.0)
Skip this batch 824! (Loss: 11250046.0)
Skip this batch 825! (Loss: 9902699.0)
Skip this batch 826! (Loss: 13798957.0)
Skip this batch 827! (Loss: 10028959.0)
Skip this batch 828! (Loss: 12599266.0)
Skip this batch 829! (Loss: 7738386.0)
Skip this batch 830! (Loss: 11295829.0)
Skip this batch 831! (Loss: 9309629.0)
Skip this batch 832! (Loss: 11708194.0)
Skip this batch 833! (Loss: 8816056.0)
Skip this batch 834! (Loss: 13476232.0)
Skip this batch 835! (Loss: 12074191.0)
Skip this batch 836! (Loss: 10620313.0)
Skip this batch 837! (Loss: 6959734.5)
Skip this batch 838! (Loss: 10775578.0)
Skip this batch 839! (Loss: 13395854.0)
Skip this batch 840! (Loss: 8843944.0)
Skip this batch 841! (Loss: 11234176.0)
Skip this batch 842! (Loss: 9039793.0)
Skip this batch 843! (Loss: 14732045.0)
Skip this batch 844! (Loss: 10961812.0)
Skip this batch 845! (Loss: 12079550.0)
Skip this batch 846! (Loss: 13888770.0)
Skip this batch 847! (Loss: 14726519.0)
Skip this batch 848! (Loss: 8610061.0)
Skip this batch 849! (Loss: 12466110.0)
Skip this batch 850! (Loss: 8432611.0)
Skip this batch 851! (Loss: 11289061.0)
Skip this batch 852! (Loss: 8146890.0)
Skip this batch 853! (Loss: 9451633.0)
Skip this batch 854! (Loss: 16090279.0)
Skip this batch 855! (Loss: 10576786.0)
Skip this batch 856! (Loss: 10373184.0)
Skip this batch 857! (Loss: 14247195.0)
Skip this batch 858! (Loss: 14523224.0)
Skip this batch 859! (Loss: 11820692.0)
Skip this batch 860! (Loss: 8959410.0)
Skip this batch 861! (Loss: 10055012.0)
Skip this batch 862! (Loss: 8936584.0)
Skip this batch 863! (Loss: 7353743.5)
Skip this batch 864! (Loss: 10768978.0)
Skip this batch 865! (Loss: 11029778.0)
Skip this batch 866! (Loss: 7906125.0)
Skip this batch 867! (Loss: 9061690.0)
Skip this batch 868! (Loss: 10945557.0)
Skip this batch 869! (Loss: 10428386.0)
Skip this batch 870! (Loss: 11293261.0)
Skip this batch 871! (Loss: 10971778.0)
Skip this batch 872! (Loss: 9448557.0)
Skip this batch 873! (Loss: 14476637.0)
Skip this batch 874! (Loss: 10395114.0)
Skip this batch 875! (Loss: 10875007.0)
Skip this batch 876! (Loss: 13348563.0)
Skip this batch 877! (Loss: 11743266.0)
Skip this batch 878! (Loss: 8722299.0)
Skip this batch 879! (Loss: 11251197.0)
Skip this batch 880! (Loss: 10129259.0)
Skip this batch 881! (Loss: 13418911.0)
Skip this batch 882! (Loss: 8851302.0)
Skip this batch 883! (Loss: 13703500.0)
Skip this batch 884! (Loss: 13243829.0)
Skip this batch 885! (Loss: 11958436.0)
Skip this batch 886! (Loss: 8721319.0)
Skip this batch 887! (Loss: 8688450.0)
Skip this batch 888! (Loss: 11777528.0)
Skip this batch 889! (Loss: 9533783.0)
Skip this batch 890! (Loss: 8269198.5)
Skip this batch 891! (Loss: 9960109.0)
Skip this batch 892! (Loss: 11090190.0)
Skip this batch 893! (Loss: 8347735.0)
Skip this batch 894! (Loss: 16172727.0)
Skip this batch 895! (Loss: 13086616.0)
Skip this batch 896! (Loss: 10593496.0)
Skip this batch 897! (Loss: 18934254.0)
Skip this batch 898! (Loss: 8970770.0)
Skip this batch 899! (Loss: 9965070.0)
Skip this batch 900! (Loss: 12042707.0)
[14400/16000]	[L1: 1502351.3750]	 timer_model: 13.62 + timer_data: 0.04s
Skip this batch 901! (Loss: 9517169.0)
Skip this batch 902! (Loss: 11253618.0)
Skip this batch 903! (Loss: 10261201.0)
Skip this batch 904! (Loss: 13522285.0)
Skip this batch 905! (Loss: 11531567.0)
Skip this batch 906! (Loss: 11028997.0)
Skip this batch 907! (Loss: 10232062.0)
Skip this batch 908! (Loss: 11483435.0)
Skip this batch 909! (Loss: 10117509.0)
Skip this batch 910! (Loss: 9470005.0)
Skip this batch 911! (Loss: 9269759.0)
Skip this batch 912! (Loss: 8216817.0)
Skip this batch 913! (Loss: 17043932.0)
Skip this batch 914! (Loss: 16030879.0)
Skip this batch 915! (Loss: 10026805.0)
Skip this batch 916! (Loss: 9210603.0)
Skip this batch 917! (Loss: 11963048.0)
Skip this batch 918! (Loss: 13693092.0)
Skip this batch 919! (Loss: 10623441.0)
Skip this batch 920! (Loss: 6627015.0)
Skip this batch 921! (Loss: 14011538.0)
Skip this batch 922! (Loss: 10160263.0)
Skip this batch 923! (Loss: 14575063.0)
Skip this batch 924! (Loss: 9148989.0)
Skip this batch 925! (Loss: 10525778.0)
Skip this batch 926! (Loss: 14887071.0)
Skip this batch 927! (Loss: 10572106.0)
Skip this batch 928! (Loss: 12431618.0)
Skip this batch 929! (Loss: 9362024.0)
Skip this batch 930! (Loss: 10472173.0)
Skip this batch 931! (Loss: 11190145.0)
Skip this batch 932! (Loss: 9257570.0)
Skip this batch 933! (Loss: 15212755.0)
Skip this batch 934! (Loss: 13257147.0)
Skip this batch 935! (Loss: 8337226.5)
Skip this batch 936! (Loss: 11269066.0)
Skip this batch 937! (Loss: 8895009.0)
Skip this batch 938! (Loss: 14471170.0)
Skip this batch 939! (Loss: 13075319.0)
Skip this batch 940! (Loss: 15024603.0)
Skip this batch 941! (Loss: 11616659.0)
Skip this batch 942! (Loss: 8120061.0)
Skip this batch 943! (Loss: 11681984.0)
Skip this batch 944! (Loss: 10300442.0)
Skip this batch 945! (Loss: 7919377.0)
Skip this batch 946! (Loss: 13111412.0)
Skip this batch 947! (Loss: 11738822.0)
Skip this batch 948! (Loss: 11658783.0)
Skip this batch 949! (Loss: 10111102.0)
Skip this batch 950! (Loss: 7554565.0)
Skip this batch 951! (Loss: 8789757.0)
Skip this batch 952! (Loss: 12757677.0)
Skip this batch 953! (Loss: 9146831.0)
Skip this batch 954! (Loss: 12438049.0)
Skip this batch 955! (Loss: 9186925.0)
Skip this batch 956! (Loss: 8480962.0)
Skip this batch 957! (Loss: 10676632.0)
Skip this batch 958! (Loss: 10390187.0)
Skip this batch 959! (Loss: 13736353.0)
Skip this batch 960! (Loss: 10431471.0)
Skip this batch 961! (Loss: 9736570.0)
Skip this batch 962! (Loss: 13728678.0)
Skip this batch 963! (Loss: 10229703.0)
Skip this batch 964! (Loss: 15710704.0)
Skip this batch 965! (Loss: 8478866.0)
Skip this batch 966! (Loss: 9927198.0)
Skip this batch 967! (Loss: 9790811.0)
Skip this batch 968! (Loss: 7774601.0)
Skip this batch 969! (Loss: 11007877.0)
Skip this batch 970! (Loss: 10979850.0)
Skip this batch 971! (Loss: 9488653.0)
Skip this batch 972! (Loss: 9870529.0)
Skip this batch 973! (Loss: 9809091.0)
Skip this batch 974! (Loss: 12040218.0)
Skip this batch 975! (Loss: 10272795.0)
Skip this batch 976! (Loss: 11316079.0)
Skip this batch 977! (Loss: 8834487.0)
Skip this batch 978! (Loss: 13353060.0)
Skip this batch 979! (Loss: 14254689.0)
Skip this batch 980! (Loss: 11892703.0)
Skip this batch 981! (Loss: 10373429.0)
Skip this batch 982! (Loss: 9110854.0)
Skip this batch 983! (Loss: 10450451.0)
Skip this batch 984! (Loss: 13870239.0)
Skip this batch 985! (Loss: 8632431.0)
Skip this batch 986! (Loss: 10764282.0)
Skip this batch 987! (Loss: 11049127.0)
Skip this batch 988! (Loss: 11153899.0)
Skip this batch 989! (Loss: 9813827.0)
Skip this batch 990! (Loss: 12687270.0)
Skip this batch 991! (Loss: 11485608.0)
Skip this batch 992! (Loss: 7604608.0)
Skip this batch 993! (Loss: 10360766.0)
Skip this batch 994! (Loss: 10063085.0)
Skip this batch 995! (Loss: 10882909.0)
Skip this batch 996! (Loss: 11001406.0)
Skip this batch 997! (Loss: 10026683.0)
Skip this batch 998! (Loss: 12671645.0)
Skip this batch 999! (Loss: 13980309.0)
Skip this batch 1000! (Loss: 10525186.0)
[16000/16000]	[L1: 2454229.2500]	 timer_model: 13.57 + timer_data: 0.05s

Evaluation:
[Set5 x2]	PSNR: 7.722 (Best: 35.268 @epoch 51)
Total time: 2.71s
 Now time: Sat Mar 20 15:56:56 2021

2021-03-20 15:56:58  [Epoch 82]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 313327.0938]	 timer_model: 59.60 + timer_data: 0.29s
[3200/16000]	[L1: 162371.0469]	 timer_model: 59.71 + timer_data: 0.04s
[4800/16000]	[L1: 134652.0781]	 timer_model: 59.66 + timer_data: 0.04s
[6400/16000]	[L1: 103996.7969]	 timer_model: 59.79 + timer_data: 0.04s
[8000/16000]	[L1: 83463.5703]	 timer_model: 59.74 + timer_data: 0.03s
[9600/16000]	[L1: 69986.0312]	 timer_model: 59.42 + timer_data: 0.03s
[11200/16000]	[L1: 60063.6445]	 timer_model: 59.56 + timer_data: 0.03s
[12800/16000]	[L1: 52580.7305]	 timer_model: 59.55 + timer_data: 0.03s
[14400/16000]	[L1: 46760.8672]	 timer_model: 59.63 + timer_data: 0.03s
[16000/16000]	[L1: 42234.3984]	 timer_model: 59.62 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 8.483 (Best: 35.268 @epoch 51)
Total time: 3.32s
 Now time: Sat Mar 20 16:06:58 2021

2021-03-20 16:07:00  [Epoch 83]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 340.7211]	 timer_model: 60.31 + timer_data: 0.29s
[3200/16000]	[L1: 282.9702]	 timer_model: 59.82 + timer_data: 0.04s
[4800/16000]	[L1: 236.7958]	 timer_model: 59.75 + timer_data: 0.04s
[6400/16000]	[L1: 208.5014]	 timer_model: 60.11 + timer_data: 0.03s
[8000/16000]	[L1: 187.6259]	 timer_model: 59.72 + timer_data: 0.03s
[9600/16000]	[L1: 285200.8438]	 timer_model: 59.84 + timer_data: 0.04s
[11200/16000]	[L1: 285241.2188]	 timer_model: 59.49 + timer_data: 0.04s
[12800/16000]	[L1: 254978.8125]	 timer_model: 59.49 + timer_data: 0.04s
[14400/16000]	[L1: 231403.6875]	 timer_model: 59.72 + timer_data: 0.04s
[16000/16000]	[L1: 208978.4375]	 timer_model: 59.75 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 9.553 (Best: 35.268 @epoch 51)
Total time: 2.76s
 Now time: Sat Mar 20 16:17:01 2021

2021-03-20 16:17:03  [Epoch 84]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 1064622.5000]	 timer_model: 59.74 + timer_data: 0.29s
[3200/16000]	[L1: 714947.2500]	 timer_model: 59.73 + timer_data: 0.04s
[4800/16000]	[L1: 481589.3438]	 timer_model: 59.87 + timer_data: 0.04s
[6400/16000]	[L1: 364931.9062]	 timer_model: 59.76 + timer_data: 0.04s
[8000/16000]	[L1: 292687.2188]	 timer_model: 59.74 + timer_data: 0.04s
[9600/16000]	[L1: 244265.4062]	 timer_model: 59.65 + timer_data: 0.03s
[11200/16000]	[L1: 210241.6250]	 timer_model: 59.59 + timer_data: 0.04s
[12800/16000]	[L1: 198292.4375]	 timer_model: 59.65 + timer_data: 0.04s
[14400/16000]	[L1: 178246.4844]	 timer_model: 59.65 + timer_data: 0.04s
[16000/16000]	[L1: 161094.5781]	 timer_model: 59.75 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 8.730 (Best: 35.268 @epoch 51)
Total time: 2.79s
 Now time: Sat Mar 20 16:27:04 2021

2021-03-20 16:27:05  [Epoch 85]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 2282.9092]	 timer_model: 59.78 + timer_data: 0.28s
[3200/16000]	[L1: 7235.0938]	 timer_model: 59.73 + timer_data: 0.04s
[4800/16000]	[L1: 6433.6079]	 timer_model: 59.77 + timer_data: 0.04s
[6400/16000]	[L1: 5596.9238]	 timer_model: 59.79 + timer_data: 0.04s
[8000/16000]	[L1: 4764.0479]	 timer_model: 59.75 + timer_data: 0.04s
[9600/16000]	[L1: 4138.8247]	 timer_model: 59.70 + timer_data: 0.04s
[11200/16000]	[L1: 3669.4329]	 timer_model: 59.81 + timer_data: 0.04s
[12800/16000]	[L1: 3298.1174]	 timer_model: 59.79 + timer_data: 0.04s
[14400/16000]	[L1: 2993.4478]	 timer_model: 59.79 + timer_data: 0.04s
[16000/16000]	[L1: 2749.4612]	 timer_model: 59.71 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 8.553 (Best: 35.268 @epoch 51)
Total time: 3.00s
 Now time: Sat Mar 20 16:37:07 2021

2021-03-20 16:37:08  [Epoch 86]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 430.5456]	 timer_model: 59.49 + timer_data: 0.28s
Skip this batch 200! (Loss: 12902905856.0)
[3200/16000]	[L1: 72436000.0000]	 timer_model: 59.06 + timer_data: 0.04s
Skip this batch 201! (Loss: 9396026368.0)
Skip this batch 202! (Loss: 8616810496.0)
Skip this batch 203! (Loss: 8069472256.0)
Skip this batch 204! (Loss: 10411213824.0)
Skip this batch 205! (Loss: 11177844736.0)
Skip this batch 206! (Loss: 7401599488.0)
Skip this batch 207! (Loss: 10321666048.0)
Skip this batch 208! (Loss: 8798986240.0)
Skip this batch 209! (Loss: 8215607808.0)
Skip this batch 210! (Loss: 7647180288.0)
Skip this batch 211! (Loss: 8383327744.0)
Skip this batch 212! (Loss: 8027962368.0)
Skip this batch 213! (Loss: 9155192832.0)
Skip this batch 214! (Loss: 7089156096.0)
Skip this batch 215! (Loss: 9108757504.0)
Skip this batch 216! (Loss: 10051433472.0)
Skip this batch 217! (Loss: 9185923072.0)
Skip this batch 218! (Loss: 8229283840.0)
Skip this batch 219! (Loss: 9462756352.0)
Skip this batch 220! (Loss: 10656806912.0)
Skip this batch 221! (Loss: 10258141184.0)
Skip this batch 222! (Loss: 7016364544.0)
Skip this batch 223! (Loss: 9537060864.0)
Skip this batch 224! (Loss: 9947932672.0)
Skip this batch 225! (Loss: 8322967552.0)
Skip this batch 226! (Loss: 12304018432.0)
Skip this batch 227! (Loss: 6791385088.0)
Skip this batch 228! (Loss: 8748089344.0)
Skip this batch 229! (Loss: 7047688192.0)
Skip this batch 230! (Loss: 9489183744.0)
Skip this batch 231! (Loss: 7307495936.0)
Skip this batch 232! (Loss: 9838720000.0)
Skip this batch 233! (Loss: 9447095296.0)
Skip this batch 234! (Loss: 10646648832.0)
Skip this batch 235! (Loss: 7395429888.0)
Skip this batch 236! (Loss: 11077280768.0)
Skip this batch 237! (Loss: 9404692480.0)
Skip this batch 238! (Loss: 8818439168.0)
Skip this batch 239! (Loss: 6630156800.0)
Skip this batch 240! (Loss: 7339032064.0)
Skip this batch 241! (Loss: 8698906624.0)
Skip this batch 242! (Loss: 8424618496.0)
Skip this batch 243! (Loss: 7237768704.0)
Skip this batch 244! (Loss: 9204318208.0)
Skip this batch 245! (Loss: 9371513856.0)
Skip this batch 246! (Loss: 8903535616.0)
Skip this batch 247! (Loss: 9511106560.0)
Skip this batch 248! (Loss: 10077526016.0)
Skip this batch 249! (Loss: 6846407680.0)
Skip this batch 250! (Loss: 11196243968.0)
Skip this batch 251! (Loss: 6138976768.0)
Skip this batch 252! (Loss: 7659827712.0)
Skip this batch 253! (Loss: 9857972224.0)
Skip this batch 254! (Loss: 8116115456.0)
Skip this batch 255! (Loss: 10316028928.0)
Skip this batch 256! (Loss: 8355905536.0)
Skip this batch 257! (Loss: 7859789824.0)
Skip this batch 258! (Loss: 10781520896.0)
Skip this batch 259! (Loss: 8988153856.0)
Skip this batch 260! (Loss: 13266348032.0)
Skip this batch 261! (Loss: 7673110528.0)
Skip this batch 262! (Loss: 9818419200.0)
Skip this batch 263! (Loss: 8612539392.0)
Skip this batch 264! (Loss: 11664770048.0)
Skip this batch 265! (Loss: 9912739840.0)
Skip this batch 266! (Loss: 6920260608.0)
Skip this batch 267! (Loss: 10415308800.0)
Skip this batch 268! (Loss: 10311794688.0)
Skip this batch 269! (Loss: 9756114944.0)
Skip this batch 270! (Loss: 9391257600.0)
Skip this batch 271! (Loss: 8851693568.0)
Skip this batch 272! (Loss: 8775310336.0)
Skip this batch 273! (Loss: 7020639232.0)
Skip this batch 274! (Loss: 8662732800.0)
Skip this batch 275! (Loss: 7628638208.0)
Skip this batch 276! (Loss: 7804877824.0)
Skip this batch 277! (Loss: 7727542784.0)
Skip this batch 278! (Loss: 8929989632.0)
Skip this batch 279! (Loss: 10269813760.0)
Skip this batch 280! (Loss: 8823962624.0)
Skip this batch 281! (Loss: 13394187264.0)
Skip this batch 282! (Loss: 10260714496.0)
Skip this batch 283! (Loss: 13157954560.0)
Skip this batch 284! (Loss: 7632309248.0)
Skip this batch 285! (Loss: 9584548864.0)
Skip this batch 286! (Loss: 9417890816.0)
Skip this batch 287! (Loss: 9338370048.0)
Skip this batch 288! (Loss: 7254026752.0)
Skip this batch 289! (Loss: 10269870080.0)
Skip this batch 290! (Loss: 12488983552.0)
Skip this batch 291! (Loss: 10686462976.0)
Skip this batch 292! (Loss: 9832555520.0)
Skip this batch 293! (Loss: 6961008640.0)
Skip this batch 294! (Loss: 8825919488.0)
Skip this batch 295! (Loss: 8200023552.0)
Skip this batch 296! (Loss: 9846902784.0)
Skip this batch 297! (Loss: 7238137344.0)
Skip this batch 298! (Loss: 8256663552.0)
Skip this batch 299! (Loss: 8912507904.0)
Skip this batch 300! (Loss: 8970982400.0)
[4800/16000]	[L1: 3071920128.0000]	 timer_model: 13.56 + timer_data: 0.04s
Skip this batch 301! (Loss: 6954306560.0)
Skip this batch 302! (Loss: 9303394304.0)
Skip this batch 303! (Loss: 10125772800.0)
Skip this batch 304! (Loss: 9033488384.0)
Skip this batch 305! (Loss: 7941046784.0)
Skip this batch 306! (Loss: 7654688768.0)
Skip this batch 307! (Loss: 7815894016.0)
Skip this batch 308! (Loss: 7617162752.0)
Skip this batch 309! (Loss: 7319397888.0)
Skip this batch 310! (Loss: 8629912576.0)
Skip this batch 311! (Loss: 8428496384.0)
Skip this batch 312! (Loss: 9381808128.0)
Skip this batch 313! (Loss: 8472263168.0)
Skip this batch 314! (Loss: 10561533952.0)
Skip this batch 315! (Loss: 8670406656.0)
Skip this batch 316! (Loss: 10048968704.0)
Skip this batch 317! (Loss: 7653089280.0)
Skip this batch 318! (Loss: 8695058432.0)
Skip this batch 319! (Loss: 11986022400.0)
Skip this batch 320! (Loss: 9117170688.0)
Skip this batch 321! (Loss: 11585906688.0)
Skip this batch 322! (Loss: 8073291264.0)
Skip this batch 323! (Loss: 11691056128.0)
Skip this batch 324! (Loss: 8119513088.0)
Skip this batch 325! (Loss: 7462252032.0)
Skip this batch 326! (Loss: 7645330432.0)
Skip this batch 327! (Loss: 7730054144.0)
Skip this batch 328! (Loss: 8401322496.0)
Skip this batch 329! (Loss: 7384188928.0)
Skip this batch 330! (Loss: 6362716160.0)
Skip this batch 331! (Loss: 10403418112.0)
Skip this batch 332! (Loss: 11452112896.0)
Skip this batch 333! (Loss: 6946489344.0)
Skip this batch 334! (Loss: 8564755456.0)
Skip this batch 335! (Loss: 8317172224.0)
Skip this batch 336! (Loss: 6177330176.0)
Skip this batch 337! (Loss: 7774649856.0)
Skip this batch 338! (Loss: 6974373888.0)
Skip this batch 339! (Loss: 9266180096.0)
Skip this batch 340! (Loss: 8193930752.0)
Skip this batch 341! (Loss: 10003486720.0)
Skip this batch 342! (Loss: 9316282368.0)
Skip this batch 343! (Loss: 6426581504.0)
Skip this batch 344! (Loss: 6749799424.0)
Skip this batch 345! (Loss: 10881827840.0)
Skip this batch 346! (Loss: 9568588800.0)
Skip this batch 347! (Loss: 10713104384.0)
Skip this batch 348! (Loss: 7905917440.0)
Skip this batch 349! (Loss: 7952040960.0)
Skip this batch 350! (Loss: 7869973504.0)
Skip this batch 351! (Loss: 7396463616.0)
Skip this batch 352! (Loss: 9443396608.0)
Skip this batch 353! (Loss: 9415957504.0)
Skip this batch 354! (Loss: 7991204352.0)
Skip this batch 355! (Loss: 8781736960.0)
Skip this batch 356! (Loss: 9920203776.0)
Skip this batch 357! (Loss: 7213626368.0)
Skip this batch 358! (Loss: 8158149632.0)
Skip this batch 359! (Loss: 9314205696.0)
Skip this batch 360! (Loss: 9600851968.0)
Skip this batch 361! (Loss: 15048363008.0)
Skip this batch 362! (Loss: 10958026752.0)
Skip this batch 363! (Loss: 7752186368.0)
Skip this batch 364! (Loss: 7070636544.0)
Skip this batch 365! (Loss: 9528570880.0)
Skip this batch 366! (Loss: 10490063872.0)
Skip this batch 367! (Loss: 6474157568.0)
Skip this batch 368! (Loss: 6867647488.0)
Skip this batch 369! (Loss: 11161487360.0)
Skip this batch 370! (Loss: 8083267584.0)
Skip this batch 371! (Loss: 9803207680.0)
Skip this batch 372! (Loss: 6866554368.0)
Skip this batch 373! (Loss: 9898961920.0)
Skip this batch 374! (Loss: 11790195712.0)
Skip this batch 375! (Loss: 7442247680.0)
Skip this batch 376! (Loss: 8356403712.0)
Skip this batch 377! (Loss: 8065315328.0)
Skip this batch 378! (Loss: 7804496384.0)
Skip this batch 379! (Loss: 11129877504.0)
Skip this batch 380! (Loss: 5804733952.0)
Skip this batch 381! (Loss: 7988197888.0)
Skip this batch 382! (Loss: 7025597440.0)
Skip this batch 383! (Loss: 9082700800.0)
Skip this batch 384! (Loss: 8065469440.0)
Skip this batch 385! (Loss: 15351645184.0)
Skip this batch 386! (Loss: 6305299456.0)
Skip this batch 387! (Loss: 7942559744.0)
Skip this batch 388! (Loss: 9014099968.0)
Skip this batch 389! (Loss: 9140990976.0)
Skip this batch 390! (Loss: 8693817344.0)
Skip this batch 391! (Loss: 7099835392.0)
Skip this batch 392! (Loss: 9522876416.0)
Skip this batch 393! (Loss: 7206577664.0)
Skip this batch 394! (Loss: 11731657728.0)
Skip this batch 395! (Loss: 12765383680.0)
Skip this batch 396! (Loss: 8312865280.0)
Skip this batch 397! (Loss: 9303215104.0)
Skip this batch 398! (Loss: 13349625856.0)
Skip this batch 399! (Loss: 12184176640.0)
Skip this batch 400! (Loss: 7884241920.0)
[6400/16000]	[L1: 4521256960.0000]	 timer_model: 13.51 + timer_data: 0.06s
Skip this batch 401! (Loss: 7212445184.0)
Skip this batch 402! (Loss: 9935197184.0)
Skip this batch 403! (Loss: 9232446464.0)
Skip this batch 404! (Loss: 9138481152.0)
Skip this batch 405! (Loss: 7104375296.0)
Skip this batch 406! (Loss: 10350274560.0)
Skip this batch 407! (Loss: 9907067904.0)
Skip this batch 408! (Loss: 8643889152.0)
Skip this batch 409! (Loss: 9041580032.0)
Skip this batch 410! (Loss: 7369509888.0)
Skip this batch 411! (Loss: 8680591360.0)
Skip this batch 412! (Loss: 9245763584.0)
Skip this batch 413! (Loss: 10648287232.0)
Skip this batch 414! (Loss: 8256598528.0)
Skip this batch 415! (Loss: 7244985344.0)
Skip this batch 416! (Loss: 9462199296.0)
Skip this batch 417! (Loss: 8582961152.0)
Skip this batch 418! (Loss: 6652972032.0)
Skip this batch 419! (Loss: 13859519488.0)
Skip this batch 420! (Loss: 9587354624.0)
Skip this batch 421! (Loss: 10375752704.0)
Skip this batch 422! (Loss: 10982337536.0)
Skip this batch 423! (Loss: 8518169088.0)
Skip this batch 424! (Loss: 9815880704.0)
Skip this batch 425! (Loss: 9933720576.0)
Skip this batch 426! (Loss: 6836023296.0)
Skip this batch 427! (Loss: 8809984000.0)
Skip this batch 428! (Loss: 7154721792.0)
Skip this batch 429! (Loss: 8613925888.0)
Skip this batch 430! (Loss: 10059009024.0)
Skip this batch 431! (Loss: 9919494144.0)
Skip this batch 432! (Loss: 9202740224.0)
Skip this batch 433! (Loss: 7811825664.0)
Skip this batch 434! (Loss: 10714383360.0)
Skip this batch 435! (Loss: 10136652800.0)
Skip this batch 436! (Loss: 11363436544.0)
Skip this batch 437! (Loss: 10802656256.0)
Skip this batch 438! (Loss: 9504031744.0)
Skip this batch 439! (Loss: 8545854464.0)
Skip this batch 440! (Loss: 11392614400.0)
Skip this batch 441! (Loss: 8505245184.0)
Skip this batch 442! (Loss: 10451676160.0)
Skip this batch 443! (Loss: 9161056256.0)
Skip this batch 444! (Loss: 8214934016.0)
Skip this batch 445! (Loss: 7014874624.0)
Skip this batch 446! (Loss: 6541199872.0)
Skip this batch 447! (Loss: 8895699968.0)
Skip this batch 448! (Loss: 11567444992.0)
Skip this batch 449! (Loss: 8785854464.0)
Skip this batch 450! (Loss: 8264936960.0)
Skip this batch 451! (Loss: 5952793088.0)
Skip this batch 452! (Loss: 8283092480.0)
Skip this batch 453! (Loss: 11595808768.0)
Skip this batch 454! (Loss: 8047499776.0)
Skip this batch 455! (Loss: 8856142848.0)
Skip this batch 456! (Loss: 8098319872.0)
Skip this batch 457! (Loss: 7832021504.0)
Skip this batch 458! (Loss: 8349067776.0)
Skip this batch 459! (Loss: 9612622848.0)
Skip this batch 460! (Loss: 7386460672.0)
Skip this batch 461! (Loss: 11690338304.0)
Skip this batch 462! (Loss: 7284846592.0)
Skip this batch 463! (Loss: 10077179904.0)
Skip this batch 464! (Loss: 8638937088.0)
Skip this batch 465! (Loss: 7651882496.0)
Skip this batch 466! (Loss: 8136857600.0)
Skip this batch 467! (Loss: 10460720128.0)
Skip this batch 468! (Loss: 10722017280.0)
Skip this batch 469! (Loss: 7611687424.0)
Skip this batch 470! (Loss: 9192916992.0)
Skip this batch 471! (Loss: 8438663168.0)
Skip this batch 472! (Loss: 10545951744.0)
Skip this batch 473! (Loss: 7049410048.0)
Skip this batch 474! (Loss: 8833213440.0)
Skip this batch 475! (Loss: 6273900032.0)
Skip this batch 476! (Loss: 8599776256.0)
Skip this batch 477! (Loss: 7274704896.0)
Skip this batch 478! (Loss: 10735335424.0)
Skip this batch 479! (Loss: 9971788800.0)
Skip this batch 480! (Loss: 6930886144.0)
Skip this batch 481! (Loss: 8925263872.0)
Skip this batch 482! (Loss: 8302447616.0)
Skip this batch 483! (Loss: 8089474048.0)
Skip this batch 484! (Loss: 8146704896.0)
Skip this batch 485! (Loss: 8310761984.0)
Skip this batch 486! (Loss: 9659264000.0)
Skip this batch 487! (Loss: 7260345344.0)
Skip this batch 488! (Loss: 12389104640.0)
Skip this batch 489! (Loss: 9782408192.0)
Skip this batch 490! (Loss: 9152688128.0)
Skip this batch 491! (Loss: 9357807616.0)
Skip this batch 492! (Loss: 7145171456.0)
Skip this batch 493! (Loss: 7055153152.0)
Skip this batch 494! (Loss: 9263407104.0)
Skip this batch 495! (Loss: 10383219712.0)
Skip this batch 496! (Loss: 7724893184.0)
Skip this batch 497! (Loss: 7727717376.0)
Skip this batch 498! (Loss: 9146479616.0)
Skip this batch 499! (Loss: 6668042752.0)
Skip this batch 500! (Loss: 8489304064.0)
[8000/16000]	[L1: 5399340032.0000]	 timer_model: 13.71 + timer_data: 0.05s
Skip this batch 501! (Loss: 8935983104.0)
Skip this batch 502! (Loss: 14198398976.0)
Skip this batch 503! (Loss: 7199669248.0)
Skip this batch 504! (Loss: 11801314304.0)
Skip this batch 505! (Loss: 6524109824.0)
Skip this batch 506! (Loss: 9165987840.0)
Skip this batch 507! (Loss: 6597129216.0)
Skip this batch 508! (Loss: 7001180672.0)
Skip this batch 509! (Loss: 9449033728.0)
Skip this batch 510! (Loss: 9287022592.0)
Skip this batch 511! (Loss: 8827965440.0)
Skip this batch 512! (Loss: 6118364160.0)
Skip this batch 513! (Loss: 9785833472.0)
Skip this batch 514! (Loss: 9217675264.0)
Skip this batch 515! (Loss: 8935904256.0)
Skip this batch 516! (Loss: 6614193664.0)
Skip this batch 517! (Loss: 8712331264.0)
Skip this batch 518! (Loss: 7658336768.0)
Skip this batch 519! (Loss: 9212945408.0)
Skip this batch 520! (Loss: 10820782080.0)
Skip this batch 521! (Loss: 11825648640.0)
Skip this batch 522! (Loss: 6842451968.0)
Skip this batch 523! (Loss: 10774490112.0)
Skip this batch 524! (Loss: 11847872512.0)
Skip this batch 525! (Loss: 9958838272.0)
Skip this batch 526! (Loss: 9262299136.0)
Skip this batch 527! (Loss: 8525819904.0)
Skip this batch 528! (Loss: 6992310272.0)
Skip this batch 529! (Loss: 9388613632.0)
Skip this batch 530! (Loss: 11629212672.0)
Skip this batch 531! (Loss: 9875918848.0)
Skip this batch 532! (Loss: 8161450496.0)
Skip this batch 533! (Loss: 12652562432.0)
Skip this batch 534! (Loss: 10600028160.0)
Skip this batch 535! (Loss: 9030554624.0)
Skip this batch 536! (Loss: 8935767040.0)
Skip this batch 537! (Loss: 14554293248.0)
Skip this batch 538! (Loss: 8602189824.0)
Skip this batch 539! (Loss: 7007509504.0)
Skip this batch 540! (Loss: 9478236160.0)
Skip this batch 541! (Loss: 7458976768.0)
Skip this batch 542! (Loss: 9196196864.0)
Skip this batch 543! (Loss: 8070616064.0)
Skip this batch 544! (Loss: 8326472704.0)
Skip this batch 545! (Loss: 7908098048.0)
Skip this batch 546! (Loss: 6683019264.0)
Skip this batch 547! (Loss: 8775277568.0)
Skip this batch 548! (Loss: 7379766272.0)
Skip this batch 549! (Loss: 7595421184.0)
Skip this batch 550! (Loss: 7027381760.0)
Skip this batch 551! (Loss: 12041123840.0)
Skip this batch 552! (Loss: 8330641408.0)
Skip this batch 553! (Loss: 7531019776.0)
Skip this batch 554! (Loss: 10128923648.0)
Skip this batch 555! (Loss: 9910674432.0)
Skip this batch 556! (Loss: 7462540800.0)
Skip this batch 557! (Loss: 5786028544.0)
Skip this batch 558! (Loss: 10085284864.0)
Skip this batch 559! (Loss: 9858449408.0)
Skip this batch 560! (Loss: 7141529088.0)
Skip this batch 561! (Loss: 5991782912.0)
Skip this batch 562! (Loss: 6721377280.0)
Skip this batch 563! (Loss: 8092578304.0)
Skip this batch 564! (Loss: 6426046464.0)
Skip this batch 565! (Loss: 7891795968.0)
Skip this batch 566! (Loss: 10701429760.0)
Skip this batch 567! (Loss: 10003619840.0)
Skip this batch 568! (Loss: 9227728896.0)
Skip this batch 569! (Loss: 8743863296.0)
Skip this batch 570! (Loss: 7888488448.0)
Skip this batch 571! (Loss: 8912633856.0)
Skip this batch 572! (Loss: 9638947840.0)
Skip this batch 573! (Loss: 9589763072.0)
Skip this batch 574! (Loss: 8955478016.0)
Skip this batch 575! (Loss: 7239725568.0)
Skip this batch 576! (Loss: 6617365504.0)
Skip this batch 577! (Loss: 8390434816.0)
Skip this batch 578! (Loss: 9016955904.0)
Skip this batch 579! (Loss: 7424557568.0)
Skip this batch 580! (Loss: 7360259584.0)
Skip this batch 581! (Loss: 12039598080.0)
Skip this batch 582! (Loss: 9896772608.0)
Skip this batch 583! (Loss: 8320338944.0)
Skip this batch 584! (Loss: 7448837120.0)
Skip this batch 585! (Loss: 11863824384.0)
Skip this batch 586! (Loss: 9498617856.0)
Skip this batch 587! (Loss: 8480879104.0)
Skip this batch 588! (Loss: 11584223232.0)
Skip this batch 589! (Loss: 11362066432.0)
Skip this batch 590! (Loss: 9212923904.0)
Skip this batch 591! (Loss: 9125673984.0)
Skip this batch 592! (Loss: 9691065344.0)
Skip this batch 593! (Loss: 8090839040.0)
Skip this batch 594! (Loss: 9045018624.0)
Skip this batch 595! (Loss: 7274631168.0)
Skip this batch 596! (Loss: 8495484928.0)
Skip this batch 597! (Loss: 8869653504.0)
Skip this batch 598! (Loss: 9486596096.0)
Skip this batch 599! (Loss: 9161276416.0)
Skip this batch 600! (Loss: 7475426816.0)
[9600/16000]	[L1: 5982727168.0000]	 timer_model: 13.74 + timer_data: 0.07s
Skip this batch 601! (Loss: 10001920000.0)
Skip this batch 602! (Loss: 6640463360.0)
Skip this batch 603! (Loss: 7218604544.0)
Skip this batch 604! (Loss: 6530453504.0)
Skip this batch 605! (Loss: 10030702592.0)
Skip this batch 606! (Loss: 10247917568.0)
Skip this batch 607! (Loss: 9105146880.0)
Skip this batch 608! (Loss: 6928112640.0)
Skip this batch 609! (Loss: 9365384192.0)
Skip this batch 610! (Loss: 7012338176.0)
Skip this batch 611! (Loss: 6574516224.0)
Skip this batch 612! (Loss: 9570327552.0)
Skip this batch 613! (Loss: 6599169536.0)
Skip this batch 614! (Loss: 8572286464.0)
Skip this batch 615! (Loss: 9018265600.0)
Skip this batch 616! (Loss: 12349469696.0)
Skip this batch 617! (Loss: 5524882432.0)
Skip this batch 618! (Loss: 7932673024.0)
Skip this batch 619! (Loss: 8700482560.0)
Skip this batch 620! (Loss: 11243067392.0)
Skip this batch 621! (Loss: 7550315008.0)
Skip this batch 622! (Loss: 7571497472.0)
Skip this batch 623! (Loss: 9329639424.0)
Skip this batch 624! (Loss: 7424524288.0)
Skip this batch 625! (Loss: 9094902784.0)
Skip this batch 626! (Loss: 9176862720.0)
Skip this batch 627! (Loss: 8458765312.0)
Skip this batch 628! (Loss: 10622217216.0)
Skip this batch 629! (Loss: 6147233280.0)
Skip this batch 630! (Loss: 9767179264.0)
Skip this batch 631! (Loss: 7894298112.0)
Skip this batch 632! (Loss: 6615589888.0)
Skip this batch 633! (Loss: 8055871488.0)
Skip this batch 634! (Loss: 9540024320.0)
Skip this batch 635! (Loss: 10181675008.0)
Skip this batch 636! (Loss: 10236776448.0)
Skip this batch 637! (Loss: 8551068160.0)
Skip this batch 638! (Loss: 12353097728.0)
Skip this batch 639! (Loss: 11509509120.0)
Skip this batch 640! (Loss: 7561304064.0)
Skip this batch 641! (Loss: 8625996800.0)
Skip this batch 642! (Loss: 11700171776.0)
Skip this batch 643! (Loss: 7366140928.0)
Skip this batch 644! (Loss: 7756972544.0)
Skip this batch 645! (Loss: 8387276800.0)
Skip this batch 646! (Loss: 8817240064.0)
Skip this batch 647! (Loss: 9994701824.0)
Skip this batch 648! (Loss: 10613463040.0)
Skip this batch 649! (Loss: 6021055488.0)
Skip this batch 650! (Loss: 8937834496.0)
Skip this batch 651! (Loss: 6813223424.0)
Skip this batch 652! (Loss: 7921110528.0)
Skip this batch 653! (Loss: 10556877824.0)
Skip this batch 654! (Loss: 6026768384.0)
Skip this batch 655! (Loss: 9437494272.0)
Skip this batch 656! (Loss: 7840071680.0)
Skip this batch 657! (Loss: 8891312128.0)
Skip this batch 658! (Loss: 11838382080.0)
Skip this batch 659! (Loss: 6820128768.0)
Skip this batch 660! (Loss: 8612963328.0)
Skip this batch 661! (Loss: 11134722048.0)
Skip this batch 662! (Loss: 8732975104.0)
Skip this batch 663! (Loss: 6404488704.0)
Skip this batch 664! (Loss: 10745178112.0)
Skip this batch 665! (Loss: 7400462848.0)
Skip this batch 666! (Loss: 8996132864.0)
Skip this batch 667! (Loss: 9185684480.0)
Skip this batch 668! (Loss: 9648669696.0)
Skip this batch 669! (Loss: 11749922816.0)
Skip this batch 670! (Loss: 7627951104.0)
Skip this batch 671! (Loss: 7435538432.0)
Skip this batch 672! (Loss: 10653027328.0)
Skip this batch 673! (Loss: 9376658432.0)
Skip this batch 674! (Loss: 8617857024.0)
Skip this batch 675! (Loss: 9929361408.0)
Skip this batch 676! (Loss: 10031982592.0)
Skip this batch 677! (Loss: 8741968896.0)
Skip this batch 678! (Loss: 14104907776.0)
Skip this batch 679! (Loss: 8471304192.0)
Skip this batch 680! (Loss: 8590069760.0)
Skip this batch 681! (Loss: 7875336192.0)
Skip this batch 682! (Loss: 9444499456.0)
Skip this batch 683! (Loss: 9884174336.0)
Skip this batch 684! (Loss: 10232808448.0)
Skip this batch 685! (Loss: 9606297600.0)
Skip this batch 686! (Loss: 7325811712.0)
Skip this batch 687! (Loss: 8172892672.0)
Skip this batch 688! (Loss: 7457974272.0)
Skip this batch 689! (Loss: 8345228288.0)
Skip this batch 690! (Loss: 7325238272.0)
Skip this batch 691! (Loss: 13460685824.0)
Skip this batch 692! (Loss: 9127946240.0)
Skip this batch 693! (Loss: 7902897664.0)
Skip this batch 694! (Loss: 7394627584.0)
Skip this batch 695! (Loss: 6968267776.0)
Skip this batch 696! (Loss: 6193303552.0)
Skip this batch 697! (Loss: 7971789312.0)
Skip this batch 698! (Loss: 9965109248.0)
Skip this batch 699! (Loss: 7785541120.0)
Skip this batch 700! (Loss: 7436648960.0)
[11200/16000]	[L1: 6381259776.0000]	 timer_model: 13.71 + timer_data: 0.05s
Skip this batch 701! (Loss: 11893329920.0)
Skip this batch 702! (Loss: 5608425984.0)
Skip this batch 703! (Loss: 5798677504.0)
Skip this batch 704! (Loss: 8040530432.0)
Skip this batch 705! (Loss: 9861042176.0)
Skip this batch 706! (Loss: 10501614592.0)
Skip this batch 707! (Loss: 8419746304.0)
Skip this batch 708! (Loss: 7677431808.0)
Skip this batch 709! (Loss: 9970948096.0)
Skip this batch 710! (Loss: 12319197184.0)
Skip this batch 711! (Loss: 10297005056.0)
Skip this batch 712! (Loss: 8350836736.0)
Skip this batch 713! (Loss: 10274388992.0)
Skip this batch 714! (Loss: 8632065024.0)
Skip this batch 715! (Loss: 9570889728.0)
Skip this batch 716! (Loss: 8942072832.0)
Skip this batch 717! (Loss: 7335988224.0)
Skip this batch 718! (Loss: 6678801920.0)
Skip this batch 719! (Loss: 9968397312.0)
Skip this batch 720! (Loss: 7796191232.0)
Skip this batch 721! (Loss: 6489360896.0)
Skip this batch 722! (Loss: 6102560768.0)
Skip this batch 723! (Loss: 10334280704.0)
Skip this batch 724! (Loss: 10475891712.0)
Skip this batch 725! (Loss: 8864882688.0)
Skip this batch 726! (Loss: 6743058432.0)
Skip this batch 727! (Loss: 6225519104.0)
Skip this batch 728! (Loss: 14719463424.0)
Skip this batch 729! (Loss: 6607184384.0)
Skip this batch 730! (Loss: 11594319872.0)
Skip this batch 731! (Loss: 9849937920.0)
Skip this batch 732! (Loss: 7646729216.0)
Skip this batch 733! (Loss: 8329311232.0)
Skip this batch 734! (Loss: 7245515776.0)
Skip this batch 735! (Loss: 8589246464.0)
Skip this batch 736! (Loss: 8396411904.0)
Skip this batch 737! (Loss: 8746923008.0)
Skip this batch 738! (Loss: 6620984832.0)
Skip this batch 739! (Loss: 9215205376.0)
Skip this batch 740! (Loss: 7338287104.0)
Skip this batch 741! (Loss: 12583060480.0)
Skip this batch 742! (Loss: 8604412928.0)
Skip this batch 743! (Loss: 9054464000.0)
Skip this batch 744! (Loss: 8458503168.0)
Skip this batch 745! (Loss: 10268984320.0)
Skip this batch 746! (Loss: 8292256768.0)
Skip this batch 747! (Loss: 7778450944.0)
Skip this batch 748! (Loss: 6759933952.0)
Skip this batch 749! (Loss: 9274575872.0)
Skip this batch 750! (Loss: 8307192832.0)
Skip this batch 751! (Loss: 6556699648.0)
Skip this batch 752! (Loss: 7831638016.0)
Skip this batch 753! (Loss: 6672919040.0)
Skip this batch 754! (Loss: 7099802624.0)
Skip this batch 755! (Loss: 10351766528.0)
Skip this batch 756! (Loss: 8422936576.0)
Skip this batch 757! (Loss: 6947157504.0)
Skip this batch 758! (Loss: 5634474496.0)
Skip this batch 759! (Loss: 8793498624.0)
Skip this batch 760! (Loss: 6638917120.0)
Skip this batch 761! (Loss: 11948324864.0)
Skip this batch 762! (Loss: 6379165184.0)
Skip this batch 763! (Loss: 10961218560.0)
Skip this batch 764! (Loss: 6611333632.0)
Skip this batch 765! (Loss: 12500317184.0)
Skip this batch 766! (Loss: 9007851520.0)
Skip this batch 767! (Loss: 8755908608.0)
Skip this batch 768! (Loss: 8714588160.0)
Skip this batch 769! (Loss: 7071326720.0)
Skip this batch 770! (Loss: 7216195072.0)
Skip this batch 771! (Loss: 5924782080.0)
Skip this batch 772! (Loss: 11458935808.0)
Skip this batch 773! (Loss: 7583450112.0)
Skip this batch 774! (Loss: 15017264128.0)
Skip this batch 775! (Loss: 10142505984.0)
Skip this batch 776! (Loss: 11058244608.0)
Skip this batch 777! (Loss: 9052304384.0)
Skip this batch 778! (Loss: 6179620864.0)
Skip this batch 779! (Loss: 10295685120.0)
Skip this batch 780! (Loss: 11652869120.0)
Skip this batch 781! (Loss: 8415116288.0)
Skip this batch 782! (Loss: 6901069312.0)
Skip this batch 783! (Loss: 9717772288.0)
Skip this batch 784! (Loss: 7186835456.0)
Skip this batch 785! (Loss: 8194805760.0)
Skip this batch 786! (Loss: 7519974912.0)
Skip this batch 787! (Loss: 8902262784.0)
Skip this batch 788! (Loss: 11248653312.0)
Skip this batch 789! (Loss: 10597383168.0)
Skip this batch 790! (Loss: 7843274752.0)
Skip this batch 791! (Loss: 8312201216.0)
Skip this batch 792! (Loss: 11347910656.0)
Skip this batch 793! (Loss: 8496574464.0)
Skip this batch 794! (Loss: 7209396736.0)
Skip this batch 795! (Loss: 9420602368.0)
Skip this batch 796! (Loss: 6867755520.0)
Skip this batch 797! (Loss: 7903884288.0)
Skip this batch 798! (Loss: 10804204544.0)
Skip this batch 799! (Loss: 11779597312.0)
Skip this batch 800! (Loss: 8256392192.0)
[12800/16000]	[L1: 6679677952.0000]	 timer_model: 13.53 + timer_data: 0.07s
Skip this batch 801! (Loss: 8129065984.0)
Skip this batch 802! (Loss: 12758209536.0)
Skip this batch 803! (Loss: 6823023616.0)
Skip this batch 804! (Loss: 9387179008.0)
Skip this batch 805! (Loss: 9205459968.0)
Skip this batch 806! (Loss: 12387560448.0)
Skip this batch 807! (Loss: 7760434688.0)
Skip this batch 808! (Loss: 7499960320.0)
Skip this batch 809! (Loss: 7999975424.0)
Skip this batch 810! (Loss: 7265068544.0)
Skip this batch 811! (Loss: 8699362304.0)
Skip this batch 812! (Loss: 8328920576.0)
Skip this batch 813! (Loss: 14169631744.0)
Skip this batch 814! (Loss: 6938248192.0)
Skip this batch 815! (Loss: 7550453760.0)
Skip this batch 816! (Loss: 9377277952.0)
Skip this batch 817! (Loss: 7897153536.0)
Skip this batch 818! (Loss: 11619908608.0)
Skip this batch 819! (Loss: 9447704576.0)
Skip this batch 820! (Loss: 9642221568.0)
Skip this batch 821! (Loss: 8597181440.0)
Skip this batch 822! (Loss: 10518781952.0)
Skip this batch 823! (Loss: 9173040128.0)
Skip this batch 824! (Loss: 7645358080.0)
Skip this batch 825! (Loss: 8096343552.0)
Skip this batch 826! (Loss: 8938717184.0)
Skip this batch 827! (Loss: 8528794624.0)
Skip this batch 828! (Loss: 8733654016.0)
Skip this batch 829! (Loss: 7702619648.0)
Skip this batch 830! (Loss: 9817062400.0)
Skip this batch 831! (Loss: 7537643008.0)
Skip this batch 832! (Loss: 8203680768.0)
Skip this batch 833! (Loss: 6702511104.0)
Skip this batch 834! (Loss: 9048193024.0)
Skip this batch 835! (Loss: 8572445184.0)
Skip this batch 836! (Loss: 6349533184.0)
Skip this batch 837! (Loss: 7203169280.0)
Skip this batch 838! (Loss: 6759648768.0)
Skip this batch 839! (Loss: 9873146880.0)
Skip this batch 840! (Loss: 9024372736.0)
Skip this batch 841! (Loss: 9091138560.0)
Skip this batch 842! (Loss: 6671280640.0)
Skip this batch 843! (Loss: 11546955776.0)
Skip this batch 844! (Loss: 8509137408.0)
Skip this batch 845! (Loss: 7646632448.0)
Skip this batch 846! (Loss: 11116512256.0)
Skip this batch 847! (Loss: 8936551424.0)
Skip this batch 848! (Loss: 8042480128.0)
Skip this batch 849! (Loss: 10786692096.0)
Skip this batch 850! (Loss: 9094047744.0)
Skip this batch 851! (Loss: 9999647744.0)
Skip this batch 852! (Loss: 7912521728.0)
Skip this batch 853! (Loss: 7335747584.0)
Skip this batch 854! (Loss: 8045046272.0)
Skip this batch 855! (Loss: 9886935040.0)
Skip this batch 856! (Loss: 8062424576.0)
Skip this batch 857! (Loss: 6981717504.0)
Skip this batch 858! (Loss: 8641231872.0)
Skip this batch 859! (Loss: 11751304192.0)
Skip this batch 860! (Loss: 10361005056.0)
Skip this batch 861! (Loss: 7704735744.0)
Skip this batch 862! (Loss: 9459520512.0)
Skip this batch 863! (Loss: 12257008640.0)
Skip this batch 864! (Loss: 9648587776.0)
Skip this batch 865! (Loss: 10260626432.0)
Skip this batch 866! (Loss: 6793131520.0)
Skip this batch 867! (Loss: 9482169344.0)
Skip this batch 868! (Loss: 9935960064.0)
Skip this batch 869! (Loss: 6806598144.0)
Skip this batch 870! (Loss: 9112903680.0)
Skip this batch 871! (Loss: 8364716032.0)
Skip this batch 872! (Loss: 7198798336.0)
Skip this batch 873! (Loss: 8666220544.0)
Skip this batch 874! (Loss: 8749314048.0)
Skip this batch 875! (Loss: 10087902208.0)
Skip this batch 876! (Loss: 7657539584.0)
Skip this batch 877! (Loss: 7761455616.0)
Skip this batch 878! (Loss: 10148172800.0)
Skip this batch 879! (Loss: 7214169600.0)
Skip this batch 880! (Loss: 7833479680.0)
Skip this batch 881! (Loss: 8246519296.0)
Skip this batch 882! (Loss: 6530025472.0)
Skip this batch 883! (Loss: 7734303744.0)
Skip this batch 884! (Loss: 6109264896.0)
Skip this batch 885! (Loss: 9248130048.0)
Skip this batch 886! (Loss: 7248428544.0)
Skip this batch 887! (Loss: 7861746176.0)
Skip this batch 888! (Loss: 9212042240.0)
Skip this batch 889! (Loss: 8518500864.0)
Skip this batch 890! (Loss: 10999362560.0)
Skip this batch 891! (Loss: 8084511744.0)
Skip this batch 892! (Loss: 8324743168.0)
Skip this batch 893! (Loss: 6917856256.0)
Skip this batch 894! (Loss: 9106862080.0)
Skip this batch 895! (Loss: 9790377984.0)
Skip this batch 896! (Loss: 10490660864.0)
Skip this batch 897! (Loss: 8355493376.0)
Skip this batch 898! (Loss: 10323623936.0)
Skip this batch 899! (Loss: 8811854848.0)
Skip this batch 900! (Loss: 7546139136.0)
[14400/16000]	[L1: 6909644800.0000]	 timer_model: 13.52 + timer_data: 0.06s
Skip this batch 901! (Loss: 11593703424.0)
Skip this batch 902! (Loss: 7292242432.0)
Skip this batch 903! (Loss: 9781687296.0)
Skip this batch 904! (Loss: 9913446400.0)
Skip this batch 905! (Loss: 6642501632.0)
Skip this batch 906! (Loss: 6217673216.0)
Skip this batch 907! (Loss: 9411800064.0)
Skip this batch 908! (Loss: 6931871232.0)
Skip this batch 909! (Loss: 7708695040.0)
Skip this batch 910! (Loss: 7149555200.0)
Skip this batch 911! (Loss: 9895404544.0)
Skip this batch 912! (Loss: 7846607360.0)
Skip this batch 913! (Loss: 7259494400.0)
Skip this batch 914! (Loss: 8453339648.0)
Skip this batch 915! (Loss: 7157462016.0)
Skip this batch 916! (Loss: 8649032704.0)
Skip this batch 917! (Loss: 11017805824.0)
Skip this batch 918! (Loss: 7625856512.0)
Skip this batch 919! (Loss: 10272093184.0)
Skip this batch 920! (Loss: 8387985408.0)
Skip this batch 921! (Loss: 7138308096.0)
Skip this batch 922! (Loss: 7736888832.0)
Skip this batch 923! (Loss: 9646598144.0)
Skip this batch 924! (Loss: 9675902976.0)
Skip this batch 925! (Loss: 8431986688.0)
Skip this batch 926! (Loss: 10686728192.0)
Skip this batch 927! (Loss: 7294541824.0)
Skip this batch 928! (Loss: 10159584256.0)
Skip this batch 929! (Loss: 10204526592.0)
Skip this batch 930! (Loss: 7684165120.0)
Skip this batch 931! (Loss: 9496724480.0)
Skip this batch 932! (Loss: 8632103936.0)
Skip this batch 933! (Loss: 7139934720.0)
Skip this batch 934! (Loss: 8253767680.0)
Skip this batch 935! (Loss: 9126334464.0)
Skip this batch 936! (Loss: 7076678144.0)
Skip this batch 937! (Loss: 6679106560.0)
Skip this batch 938! (Loss: 10100489216.0)
Skip this batch 939! (Loss: 6683888640.0)
Skip this batch 940! (Loss: 8942464000.0)
Skip this batch 941! (Loss: 7987090944.0)
Skip this batch 942! (Loss: 10021516288.0)
Skip this batch 943! (Loss: 8842629120.0)
Skip this batch 944! (Loss: 7264587776.0)
Skip this batch 945! (Loss: 9742030848.0)
Skip this batch 946! (Loss: 9307421696.0)
Skip this batch 947! (Loss: 6496215552.0)
Skip this batch 948! (Loss: 7767278080.0)
Skip this batch 949! (Loss: 13609242624.0)
Skip this batch 950! (Loss: 10267629568.0)
Skip this batch 951! (Loss: 11243938816.0)
Skip this batch 952! (Loss: 9114099712.0)
Skip this batch 953! (Loss: 8547004416.0)
Skip this batch 954! (Loss: 10195777536.0)
Skip this batch 955! (Loss: 9674303488.0)
Skip this batch 956! (Loss: 7757807104.0)
Skip this batch 957! (Loss: 10925035520.0)
Skip this batch 958! (Loss: 8798081024.0)
Skip this batch 959! (Loss: 7510936064.0)
Skip this batch 960! (Loss: 9476927488.0)
Skip this batch 961! (Loss: 6870260736.0)
Skip this batch 962! (Loss: 7721323008.0)
Skip this batch 963! (Loss: 7748345856.0)
Skip this batch 964! (Loss: 8169302528.0)
Skip this batch 965! (Loss: 8621093888.0)
Skip this batch 966! (Loss: 7508849152.0)
Skip this batch 967! (Loss: 13880597504.0)
Skip this batch 968! (Loss: 7054323712.0)
Skip this batch 969! (Loss: 8418554880.0)
Skip this batch 970! (Loss: 10828079104.0)
Skip this batch 971! (Loss: 6281186304.0)
Skip this batch 972! (Loss: 8874565632.0)
Skip this batch 973! (Loss: 9586924544.0)
Skip this batch 974! (Loss: 7024520704.0)
Skip this batch 975! (Loss: 7150044160.0)
Skip this batch 976! (Loss: 7438498816.0)
Skip this batch 977! (Loss: 8937850880.0)
Skip this batch 978! (Loss: 9764706304.0)
Skip this batch 979! (Loss: 17793642496.0)
Skip this batch 980! (Loss: 8541181952.0)
Skip this batch 981! (Loss: 9213340672.0)
Skip this batch 982! (Loss: 6949928960.0)
Skip this batch 983! (Loss: 7999008256.0)
Skip this batch 984! (Loss: 6921775616.0)
Skip this batch 985! (Loss: 10092932096.0)
Skip this batch 986! (Loss: 6771672064.0)
Skip this batch 987! (Loss: 6403618304.0)
Skip this batch 988! (Loss: 7969842688.0)
Skip this batch 989! (Loss: 8775979008.0)
Skip this batch 990! (Loss: 6029122048.0)
Skip this batch 991! (Loss: 8143653376.0)
Skip this batch 992! (Loss: 7837304832.0)
Skip this batch 993! (Loss: 9821460480.0)
Skip this batch 994! (Loss: 7314073600.0)
Skip this batch 995! (Loss: 7674278912.0)
Skip this batch 996! (Loss: 6696717824.0)
Skip this batch 997! (Loss: 8242982912.0)
Skip this batch 998! (Loss: 9681942528.0)
Skip this batch 999! (Loss: 8778796032.0)
Skip this batch 1000! (Loss: 8401160704.0)
[16000/16000]	[L1: 7081161728.0000]	 timer_model: 13.49 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: 10.821 (Best: 35.268 @epoch 51)
Total time: 2.75s
 Now time: Sat Mar 20 16:40:59 2021

2021-03-20 16:41:01  [Epoch 87]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 148534080.0000]	 timer_model: 59.62 + timer_data: 0.28s
[3200/16000]	[L1: 79177512.0000]	 timer_model: 59.55 + timer_data: 0.04s
[4800/16000]	[L1: 19282042880.0000]	 timer_model: 59.56 + timer_data: 0.03s
[6400/16000]	[L1: 14464094208.0000]	 timer_model: 59.58 + timer_data: 0.03s
[8000/16000]	[L1: 11572562944.0000]	 timer_model: 59.63 + timer_data: 0.03s
[9600/16000]	[L1: 9644362752.0000]	 timer_model: 59.67 + timer_data: 0.03s
[11200/16000]	[L1: 8268007424.0000]	 timer_model: 59.70 + timer_data: 0.04s
[12800/16000]	[L1: 7234781184.0000]	 timer_model: 59.56 + timer_data: 0.04s
[14400/16000]	[L1: 6431164928.0000]	 timer_model: 59.72 + timer_data: 0.04s
[16000/16000]	[L1: 5788175360.0000]	 timer_model: 59.76 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 9.254 (Best: 35.268 @epoch 51)
Total time: 2.88s
 Now time: Sat Mar 20 16:51:01 2021

2021-03-20 16:51:02  [Epoch 88]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4683738.0000]	 timer_model: 59.91 + timer_data: 0.29s
[3200/16000]	[L1: 3286241.5000]	 timer_model: 61.29 + timer_data: 0.04s
[4800/16000]	[L1: 3355227.0000]	 timer_model: 59.92 + timer_data: 0.04s
[6400/16000]	[L1: 3746237.0000]	 timer_model: 59.93 + timer_data: 0.04s
[8000/16000]	[L1: 4246313.5000]	 timer_model: 59.91 + timer_data: 0.04s
[9600/16000]	[L1: 3803887.7500]	 timer_model: 60.57 + timer_data: 0.04s
[11200/16000]	[L1: 9945342.0000]	 timer_model: 59.65 + timer_data: 0.03s
[12800/16000]	[L1: 9182869.0000]	 timer_model: 59.59 + timer_data: 0.03s
[14400/16000]	[L1: 8642432.0000]	 timer_model: 59.52 + timer_data: 0.03s
[16000/16000]	[L1: 8130799.5000]	 timer_model: 59.53 + timer_data: 0.03s

Evaluation:
[Set5 x2]	PSNR: 8.713 (Best: 35.268 @epoch 51)
Total time: 2.89s
 Now time: Sat Mar 20 17:01:06 2021

2021-03-20 17:01:08  [Epoch 89]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 8750271.0000]	 timer_model: 60.63 + timer_data: 0.30s
[3200/16000]	[L1: 6127422.0000]	 timer_model: 60.36 + timer_data: 0.05s
[4800/16000]	[L1: 11957947.0000]	 timer_model: 59.95 + timer_data: 0.04s
[6400/16000]	[L1: 12557930.0000]	 timer_model: 59.88 + timer_data: 0.04s
[8000/16000]	[L1: 11312055.0000]	 timer_model: 59.67 + timer_data: 0.04s
[9600/16000]	[L1: 9890063.0000]	 timer_model: 59.70 + timer_data: 0.04s
[11200/16000]	[L1: 8673709.0000]	 timer_model: 59.58 + timer_data: 0.04s
[12800/16000]	[L1: 7767394.5000]	 timer_model: 59.53 + timer_data: 0.04s
[14400/16000]	[L1: 6983196.5000]	 timer_model: 59.88 + timer_data: 0.04s
[16000/16000]	[L1: 6646768.5000]	 timer_model: 59.71 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 8.872 (Best: 35.268 @epoch 51)
Total time: 2.94s
 Now time: Sat Mar 20 17:11:10 2021

2021-03-20 17:11:12  [Epoch 90]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 3530585.2500]	 timer_model: 61.49 + timer_data: 0.30s
[3200/16000]	[L1: 3227371.7500]	 timer_model: 60.39 + timer_data: 0.05s
[4800/16000]	[L1: 3786744.0000]	 timer_model: 59.87 + timer_data: 0.04s
[6400/16000]	[L1: 6313233.5000]	 timer_model: 59.84 + timer_data: 0.05s
[8000/16000]	[L1: 6066627.0000]	 timer_model: 59.64 + timer_data: 0.04s
[9600/16000]	[L1: 6010244.5000]	 timer_model: 59.75 + timer_data: 0.04s
[11200/16000]	[L1: 6136432.0000]	 timer_model: 59.75 + timer_data: 0.04s
[12800/16000]	[L1: 149180416.0000]	 timer_model: 59.76 + timer_data: 0.05s
[14400/16000]	[L1: 181468400.0000]	 timer_model: 59.57 + timer_data: 0.04s
[16000/16000]	[L1: 163854416.0000]	 timer_model: 59.55 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 8.307 (Best: 35.268 @epoch 51)
Total time: 2.94s
 Now time: Sat Mar 20 17:21:16 2021

2021-03-20 17:21:17  [Epoch 91]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 12708942.0000]	 timer_model: 59.90 + timer_data: 0.28s
[3200/16000]	[L1: 7292035.0000]	 timer_model: 59.82 + timer_data: 0.04s
[4800/16000]	[L1: 6625206.0000]	 timer_model: 59.85 + timer_data: 0.04s
[6400/16000]	[L1: 8395281.0000]	 timer_model: 59.72 + timer_data: 0.03s
[8000/16000]	[L1: 8381561.0000]	 timer_model: 59.72 + timer_data: 0.03s
[9600/16000]	[L1: 9299977.0000]	 timer_model: 59.61 + timer_data: 0.03s
[11200/16000]	[L1: 8608788.0000]	 timer_model: 59.51 + timer_data: 0.03s
[12800/16000]	[L1: 8029033.0000]	 timer_model: 59.64 + timer_data: 0.03s
[14400/16000]	[L1: 7550803.5000]	 timer_model: 59.71 + timer_data: 0.03s
[16000/16000]	[L1: 7282752.5000]	 timer_model: 59.69 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 8.912 (Best: 35.268 @epoch 51)
Total time: 3.13s
 Now time: Sat Mar 20 17:31:18 2021

2021-03-20 17:31:20  [Epoch 92]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 4642921.5000]	 timer_model: 59.77 + timer_data: 0.28s
Skip this batch 198! (Loss: 31204199890944.0)
Skip this batch 199! (Loss: 48517561188352.0)
Skip this batch 200! (Loss: 28662950789120.0)
[3200/16000]	[L1: 565817049088.0000]	 timer_model: 58.24 + timer_data: 0.04s
Skip this batch 201! (Loss: 37974091759616.0)
Skip this batch 202! (Loss: 32241998626816.0)
Skip this batch 203! (Loss: 23430573326336.0)
Skip this batch 204! (Loss: 30073467961344.0)
Skip this batch 205! (Loss: 26747032567808.0)
Skip this batch 206! (Loss: 33161249554432.0)
Skip this batch 207! (Loss: 35399384694784.0)
Skip this batch 208! (Loss: 34694284443648.0)
Skip this batch 209! (Loss: 29195249909760.0)
Skip this batch 210! (Loss: 30264101175296.0)
Skip this batch 211! (Loss: 22130605424640.0)
Skip this batch 212! (Loss: 31384437522432.0)
Skip this batch 213! (Loss: 48298387832832.0)
Skip this batch 214! (Loss: 33249770340352.0)
Skip this batch 215! (Loss: 33712865542144.0)
Skip this batch 216! (Loss: 37039269478400.0)
Skip this batch 217! (Loss: 23640443715584.0)
Skip this batch 218! (Loss: 24938792615936.0)
Skip this batch 219! (Loss: 31063847993344.0)
Skip this batch 220! (Loss: 50433582170112.0)
Skip this batch 221! (Loss: 29331550109696.0)
Skip this batch 222! (Loss: 45363759153152.0)
Skip this batch 223! (Loss: 37868487573504.0)
Skip this batch 224! (Loss: 37726946590720.0)
Skip this batch 225! (Loss: 42168177655808.0)
Skip this batch 226! (Loss: 45042962006016.0)
Skip this batch 227! (Loss: 36517540003840.0)
Skip this batch 228! (Loss: 25362434097152.0)
Skip this batch 229! (Loss: 52749496483840.0)
Skip this batch 230! (Loss: 29518366507008.0)
Skip this batch 231! (Loss: 31405511802880.0)
Skip this batch 232! (Loss: 33957691260928.0)
Skip this batch 233! (Loss: 22530117074944.0)
Skip this batch 234! (Loss: 36294742769664.0)
Skip this batch 235! (Loss: 34750664278016.0)
Skip this batch 236! (Loss: 33512621080576.0)
Skip this batch 237! (Loss: 32055782014976.0)
Skip this batch 238! (Loss: 41057400127488.0)
Skip this batch 239! (Loss: 25826309439488.0)
Skip this batch 240! (Loss: 29617303846912.0)
Skip this batch 241! (Loss: 41822172741632.0)
Skip this batch 242! (Loss: 49325312835584.0)
Skip this batch 243! (Loss: 21803126751232.0)
Skip this batch 244! (Loss: 27999493685248.0)
Skip this batch 245! (Loss: 32805845204992.0)
Skip this batch 246! (Loss: 29027874111488.0)
Skip this batch 247! (Loss: 27290392068096.0)
Skip this batch 248! (Loss: 30399679954944.0)
Skip this batch 249! (Loss: 41807228436480.0)
Skip this batch 250! (Loss: 39475447070720.0)
Skip this batch 251! (Loss: 25362912247808.0)
Skip this batch 252! (Loss: 44307847315456.0)
Skip this batch 253! (Loss: 44030045978624.0)
Skip this batch 254! (Loss: 18559010865152.0)
Skip this batch 255! (Loss: 50172889399296.0)
Skip this batch 256! (Loss: 35102306336768.0)
Skip this batch 257! (Loss: 44797687496704.0)
Skip this batch 258! (Loss: 38097408491520.0)
Skip this batch 259! (Loss: 39190163095552.0)
Skip this batch 260! (Loss: 57486157545472.0)
Skip this batch 261! (Loss: 43704534433792.0)
Skip this batch 262! (Loss: 23197894311936.0)
Skip this batch 263! (Loss: 26831616999424.0)
Skip this batch 264! (Loss: 38946759245824.0)
Skip this batch 265! (Loss: 33891582738432.0)
Skip this batch 266! (Loss: 47125425553408.0)
Skip this batch 267! (Loss: 35747394486272.0)
Skip this batch 268! (Loss: 37488080977920.0)
Skip this batch 269! (Loss: 32413258350592.0)
Skip this batch 270! (Loss: 51301077483520.0)
Skip this batch 271! (Loss: 36803688005632.0)
Skip this batch 272! (Loss: 22636748865536.0)
Skip this batch 273! (Loss: 18032566992896.0)
Skip this batch 274! (Loss: 58155568463872.0)
Skip this batch 275! (Loss: 29613419921408.0)
Skip this batch 276! (Loss: 38157454147584.0)
Skip this batch 277! (Loss: 31190534848512.0)
Skip this batch 278! (Loss: 28113524228096.0)
Skip this batch 279! (Loss: 32436440268800.0)
Skip this batch 280! (Loss: 44306534498304.0)
Skip this batch 281! (Loss: 41486087356416.0)
Skip this batch 282! (Loss: 39707773763584.0)
Skip this batch 283! (Loss: 29344449691648.0)
Skip this batch 284! (Loss: 31067874525184.0)
Skip this batch 285! (Loss: 26594619949056.0)
Skip this batch 286! (Loss: 25109291073536.0)
Skip this batch 287! (Loss: 30545404755968.0)
Skip this batch 288! (Loss: 27382201188352.0)
Skip this batch 289! (Loss: 37484914278400.0)
Skip this batch 290! (Loss: 37963555667968.0)
Skip this batch 291! (Loss: 45438635868160.0)
Skip this batch 292! (Loss: 24117979906048.0)
Skip this batch 293! (Loss: 39859448184832.0)
Skip this batch 294! (Loss: 42365578379264.0)
Skip this batch 295! (Loss: 32376935677952.0)
Skip this batch 296! (Loss: 35952479174656.0)
Skip this batch 297! (Loss: 32528706568192.0)
Skip this batch 298! (Loss: 34954272571392.0)
Skip this batch 299! (Loss: 32914811125760.0)
Skip this batch 300! (Loss: 46334597922816.0)
[4800/16000]	[L1: 12024602951680.0000]	 timer_model: 13.61 + timer_data: 0.04s
Skip this batch 301! (Loss: 41293422002176.0)
Skip this batch 302! (Loss: 41622075080704.0)
Skip this batch 303! (Loss: 37551649849344.0)
Skip this batch 304! (Loss: 38790605307904.0)
Skip this batch 305! (Loss: 31950400126976.0)
Skip this batch 306! (Loss: 41068540198912.0)
Skip this batch 307! (Loss: 32099377610752.0)
Skip this batch 308! (Loss: 18697433382912.0)
Skip this batch 309! (Loss: 31473545510912.0)
Skip this batch 310! (Loss: 40019997753344.0)
Skip this batch 311! (Loss: 29426609815552.0)
Skip this batch 312! (Loss: 31680666533888.0)
Skip this batch 313! (Loss: 47520373800960.0)
Skip this batch 314! (Loss: 35624983724032.0)
Skip this batch 315! (Loss: 42548890435584.0)
Skip this batch 316! (Loss: 35445454929920.0)
Skip this batch 317! (Loss: 47896539955200.0)
Skip this batch 318! (Loss: 33048531828736.0)
Skip this batch 319! (Loss: 32657100505088.0)
Skip this batch 320! (Loss: 26628801429504.0)
Skip this batch 321! (Loss: 38781306535936.0)
Skip this batch 322! (Loss: 30892854607872.0)
Skip this batch 323! (Loss: 42733464977408.0)
Skip this batch 324! (Loss: 40532399095808.0)
Skip this batch 325! (Loss: 36621802012672.0)
Skip this batch 326! (Loss: 38872868192256.0)
Skip this batch 327! (Loss: 37648680878080.0)
Skip this batch 328! (Loss: 38524384444416.0)
Skip this batch 329! (Loss: 33123939123200.0)
Skip this batch 330! (Loss: 34561635385344.0)
Skip this batch 331! (Loss: 39752896086016.0)
Skip this batch 332! (Loss: 42883776249856.0)
Skip this batch 333! (Loss: 26763258232832.0)
Skip this batch 334! (Loss: 33198396407808.0)
Skip this batch 335! (Loss: 27236698685440.0)
Skip this batch 336! (Loss: 28342459826176.0)
Skip this batch 337! (Loss: 30639640281088.0)
Skip this batch 338! (Loss: 34909812948992.0)
Skip this batch 339! (Loss: 31239226523648.0)
Skip this batch 340! (Loss: 26668072697856.0)
Skip this batch 341! (Loss: 22836022345728.0)
Skip this batch 342! (Loss: 42706499796992.0)
Skip this batch 343! (Loss: 25491822084096.0)
Skip this batch 344! (Loss: 30096937189376.0)
Skip this batch 345! (Loss: 46068884570112.0)
Skip this batch 346! (Loss: 23299729915904.0)
Skip this batch 347! (Loss: 22996234272768.0)
Skip this batch 348! (Loss: 25766490275840.0)
Skip this batch 349! (Loss: 26820745363456.0)
Skip this batch 350! (Loss: 34931157762048.0)
Skip this batch 351! (Loss: 30019392897024.0)
Skip this batch 352! (Loss: 35801761054720.0)
Skip this batch 353! (Loss: 61005702889472.0)
Skip this batch 354! (Loss: 33152980484096.0)
Skip this batch 355! (Loss: 32156132835328.0)
Skip this batch 356! (Loss: 29711365308416.0)
Skip this batch 357! (Loss: 33770442850304.0)
Skip this batch 358! (Loss: 30392627232768.0)
Skip this batch 359! (Loss: 53252536139776.0)
Skip this batch 360! (Loss: 53518404681728.0)
Skip this batch 361! (Loss: 33813272985600.0)
Skip this batch 362! (Loss: 45591295950848.0)
Skip this batch 363! (Loss: 45661709926400.0)
Skip this batch 364! (Loss: 41169463541760.0)
Skip this batch 365! (Loss: 36215457841152.0)
Skip this batch 366! (Loss: 37351552188416.0)
Skip this batch 367! (Loss: 40625978212352.0)
Skip this batch 368! (Loss: 46261688336384.0)
Skip this batch 369! (Loss: 47533921402880.0)
Skip this batch 370! (Loss: 24531657818112.0)
Skip this batch 371! (Loss: 31379570032640.0)
Skip this batch 372! (Loss: 27449448464384.0)
Skip this batch 373! (Loss: 39222836723712.0)
Skip this batch 374! (Loss: 32586086744064.0)
Skip this batch 375! (Loss: 44414999199744.0)
Skip this batch 376! (Loss: 35511922065408.0)
Skip this batch 377! (Loss: 34738108628992.0)
Skip this batch 378! (Loss: 49425577672704.0)
Skip this batch 379! (Loss: 30472268677120.0)
Skip this batch 380! (Loss: 40626288590848.0)
Skip this batch 381! (Loss: 42637344112640.0)
Skip this batch 382! (Loss: 45525701230592.0)
Skip this batch 383! (Loss: 42142361714688.0)
Skip this batch 384! (Loss: 29846751150080.0)
Skip this batch 385! (Loss: 37181548658688.0)
Skip this batch 386! (Loss: 30409144401920.0)
Skip this batch 387! (Loss: 28034723741696.0)
Skip this batch 388! (Loss: 45915515650048.0)
Skip this batch 389! (Loss: 34620177383424.0)
Skip this batch 390! (Loss: 32507152039936.0)
Skip this batch 391! (Loss: 23560963751936.0)
Skip this batch 392! (Loss: 25773398294528.0)
Skip this batch 393! (Loss: 45347367813120.0)
Skip this batch 394! (Loss: 46278146785280.0)
Skip this batch 395! (Loss: 29550429863936.0)
Skip this batch 396! (Loss: 41098391060480.0)
Skip this batch 397! (Loss: 41028136468480.0)
Skip this batch 398! (Loss: 40923182399488.0)
Skip this batch 399! (Loss: 43324807315456.0)
Skip this batch 400! (Loss: 27458101313536.0)
[6400/16000]	[L1: 18024738324480.0000]	 timer_model: 13.61 + timer_data: 0.06s
Skip this batch 401! (Loss: 32329626025984.0)
Skip this batch 402! (Loss: 61614875213824.0)
Skip this batch 403! (Loss: 39028908883968.0)
Skip this batch 404! (Loss: 27435991040000.0)
Skip this batch 405! (Loss: 43365324292096.0)
Skip this batch 406! (Loss: 28991148785664.0)
Skip this batch 407! (Loss: 46174207737856.0)
Skip this batch 408! (Loss: 31963310194688.0)
Skip this batch 409! (Loss: 23928233787392.0)
Skip this batch 410! (Loss: 32023236313088.0)
Skip this batch 411! (Loss: 52249338314752.0)
Skip this batch 412! (Loss: 32486667059200.0)
Skip this batch 413! (Loss: 35785604595712.0)
Skip this batch 414! (Loss: 19116484198400.0)
Skip this batch 415! (Loss: 42578019876864.0)
Skip this batch 416! (Loss: 42751034916864.0)
Skip this batch 417! (Loss: 55986307989504.0)
Skip this batch 418! (Loss: 47395601645568.0)
Skip this batch 419! (Loss: 40616855601152.0)
Skip this batch 420! (Loss: 41272085577728.0)
Skip this batch 421! (Loss: 55701787377664.0)
Skip this batch 422! (Loss: 34083128213504.0)
Skip this batch 423! (Loss: 39296375455744.0)
Skip this batch 424! (Loss: 50507909431296.0)
Skip this batch 425! (Loss: 40927154405376.0)
Skip this batch 426! (Loss: 30790924632064.0)
Skip this batch 427! (Loss: 35146623352832.0)
Skip this batch 428! (Loss: 47340543016960.0)
Skip this batch 429! (Loss: 28597674835968.0)
Skip this batch 430! (Loss: 37467289812992.0)
Skip this batch 431! (Loss: 35500836519936.0)
Skip this batch 432! (Loss: 37165815824384.0)
Skip this batch 433! (Loss: 32464605020160.0)
Skip this batch 434! (Loss: 27023787425792.0)
Skip this batch 435! (Loss: 66646634921984.0)
Skip this batch 436! (Loss: 46109296689152.0)
Skip this batch 437! (Loss: 43753255469056.0)
Skip this batch 438! (Loss: 37269725511680.0)
Skip this batch 439! (Loss: 33818180321280.0)
Skip this batch 440! (Loss: 25032185085952.0)
Skip this batch 441! (Loss: 40151761813504.0)
Skip this batch 442! (Loss: 54663638417408.0)
Skip this batch 443! (Loss: 43041133953024.0)
Skip this batch 444! (Loss: 29915745353728.0)
Skip this batch 445! (Loss: 26482797707264.0)
Skip this batch 446! (Loss: 34583871488000.0)
Skip this batch 447! (Loss: 46680149852160.0)
Skip this batch 448! (Loss: 43735740055552.0)
Skip this batch 449! (Loss: 39071850168320.0)
Skip this batch 450! (Loss: 34961031692288.0)
Skip this batch 451! (Loss: 35253066399744.0)
Skip this batch 452! (Loss: 45941939765248.0)
Skip this batch 453! (Loss: 29763397746688.0)
Skip this batch 454! (Loss: 25398249259008.0)
Skip this batch 455! (Loss: 28748172754944.0)
Skip this batch 456! (Loss: 29230060535808.0)
Skip this batch 457! (Loss: 24693635547136.0)
Skip this batch 458! (Loss: 33334564487168.0)
Skip this batch 459! (Loss: 40582021906432.0)
Skip this batch 460! (Loss: 33516328845312.0)
Skip this batch 461! (Loss: 50020237705216.0)
Skip this batch 462! (Loss: 29061074124800.0)
Skip this batch 463! (Loss: 33696530825216.0)
Skip this batch 464! (Loss: 24630639198208.0)
Skip this batch 465! (Loss: 35190944563200.0)
Skip this batch 466! (Loss: 58118419513344.0)
Skip this batch 467! (Loss: 30201956270080.0)
Skip this batch 468! (Loss: 35156823900160.0)
Skip this batch 469! (Loss: 41068674416640.0)
Skip this batch 470! (Loss: 39070415716352.0)
Skip this batch 471! (Loss: 42862435631104.0)
Skip this batch 472! (Loss: 49443176972288.0)
Skip this batch 473! (Loss: 27267537305600.0)
Skip this batch 474! (Loss: 27383373496320.0)
Skip this batch 475! (Loss: 25031040040960.0)
Skip this batch 476! (Loss: 54568578711552.0)
Skip this batch 477! (Loss: 36709999837184.0)
Skip this batch 478! (Loss: 51697099472896.0)
Skip this batch 479! (Loss: 41413676892160.0)
Skip this batch 480! (Loss: 30755828793344.0)
Skip this batch 481! (Loss: 24005299929088.0)
Skip this batch 482! (Loss: 22133742764032.0)
Skip this batch 483! (Loss: 38449394483200.0)
Skip this batch 484! (Loss: 53499911995392.0)
Skip this batch 485! (Loss: 34068798373888.0)
Skip this batch 486! (Loss: 25081422020608.0)
Skip this batch 487! (Loss: 36646774898688.0)
Skip this batch 488! (Loss: 39533487849472.0)
Skip this batch 489! (Loss: 44250314047488.0)
Skip this batch 490! (Loss: 38318666416128.0)
Skip this batch 491! (Loss: 29004425854976.0)
Skip this batch 492! (Loss: 43390058102784.0)
Skip this batch 493! (Loss: 25111748935680.0)
Skip this batch 494! (Loss: 33704535654400.0)
Skip this batch 495! (Loss: 35511838179328.0)
Skip this batch 496! (Loss: 33365394718720.0)
Skip this batch 497! (Loss: 36933564628992.0)
Skip this batch 498! (Loss: 43916497780736.0)
Skip this batch 499! (Loss: 49358309425152.0)
Skip this batch 500! (Loss: 45385309487104.0)
[8000/16000]	[L1: 21972796833792.0000]	 timer_model: 13.52 + timer_data: 0.05s
Skip this batch 501! (Loss: 42254227996672.0)
Skip this batch 502! (Loss: 47621313921024.0)
Skip this batch 503! (Loss: 41335130161152.0)
Skip this batch 504! (Loss: 33775006253056.0)
Skip this batch 505! (Loss: 32242334171136.0)
Skip this batch 506! (Loss: 38169785401344.0)
Skip this batch 507! (Loss: 32521844686848.0)
Skip this batch 508! (Loss: 24899500376064.0)
Skip this batch 509! (Loss: 40180161445888.0)
Skip this batch 510! (Loss: 26456793022464.0)
Skip this batch 511! (Loss: 33111802904576.0)
Skip this batch 512! (Loss: 30602730405888.0)
Skip this batch 513! (Loss: 27734008922112.0)
Skip this batch 514! (Loss: 34888338112512.0)
Skip this batch 515! (Loss: 29565627924480.0)
Skip this batch 516! (Loss: 35581593649152.0)
Skip this batch 517! (Loss: 37052343123968.0)
Skip this batch 518! (Loss: 37700681859072.0)
Skip this batch 519! (Loss: 36734398103552.0)
Skip this batch 520! (Loss: 46577506844672.0)
Skip this batch 521! (Loss: 32063015092224.0)
Skip this batch 522! (Loss: 35429764038656.0)
Skip this batch 523! (Loss: 26310856409088.0)
Skip this batch 524! (Loss: 35796484620288.0)
Skip this batch 525! (Loss: 26771103678464.0)
Skip this batch 526! (Loss: 35594763763712.0)
Skip this batch 527! (Loss: 31895660265472.0)
Skip this batch 528! (Loss: 35409883037696.0)
Skip this batch 529! (Loss: 31936904953856.0)
Skip this batch 530! (Loss: 39130583007232.0)
Skip this batch 531! (Loss: 30766486519808.0)
Skip this batch 532! (Loss: 58505281142784.0)
Skip this batch 533! (Loss: 24012224724992.0)
Skip this batch 534! (Loss: 40810053632000.0)
Skip this batch 535! (Loss: 53617327341568.0)
Skip this batch 536! (Loss: 29743158132736.0)
Skip this batch 537! (Loss: 37618632884224.0)
Skip this batch 538! (Loss: 26633977200640.0)
Skip this batch 539! (Loss: 29663713820672.0)
Skip this batch 540! (Loss: 34489080217600.0)
Skip this batch 541! (Loss: 51825843634176.0)
Skip this batch 542! (Loss: 47712003162112.0)
Skip this batch 543! (Loss: 36515694510080.0)
Skip this batch 544! (Loss: 27274931863552.0)
Skip this batch 545! (Loss: 26652444721152.0)
Skip this batch 546! (Loss: 38790672416768.0)
Skip this batch 547! (Loss: 37838116618240.0)
Skip this batch 548! (Loss: 40624816390144.0)
Skip this batch 549! (Loss: 27904647888896.0)
Skip this batch 550! (Loss: 49225530343424.0)
Skip this batch 551! (Loss: 31097884770304.0)
Skip this batch 552! (Loss: 30969568428032.0)
Skip this batch 553! (Loss: 39239139983360.0)
Skip this batch 554! (Loss: 39886161707008.0)
Skip this batch 555! (Loss: 39718800588800.0)
Skip this batch 556! (Loss: 37449237528576.0)
Skip this batch 557! (Loss: 36289621524480.0)
Skip this batch 558! (Loss: 32116135952384.0)
Skip this batch 559! (Loss: 51689553920000.0)
Skip this batch 560! (Loss: 50284990562304.0)
Skip this batch 561! (Loss: 45980447670272.0)
Skip this batch 562! (Loss: 27220164739072.0)
Skip this batch 563! (Loss: 37740783599616.0)
Skip this batch 564! (Loss: 33321532784640.0)
Skip this batch 565! (Loss: 44470871523328.0)
Skip this batch 566! (Loss: 46589540302848.0)
Skip this batch 567! (Loss: 38195890749440.0)
Skip this batch 568! (Loss: 47670299197440.0)
Skip this batch 569! (Loss: 27167094210560.0)
Skip this batch 570! (Loss: 31748320657408.0)
Skip this batch 571! (Loss: 30140782346240.0)
Skip this batch 572! (Loss: 37618918096896.0)
Skip this batch 573! (Loss: 37799013122048.0)
Skip this batch 574! (Loss: 32333363150848.0)
Skip this batch 575! (Loss: 52899770007552.0)
Skip this batch 576! (Loss: 38439277821952.0)
Skip this batch 577! (Loss: 30497220591616.0)
Skip this batch 578! (Loss: 25953629634560.0)
Skip this batch 579! (Loss: 35093305360384.0)
Skip this batch 580! (Loss: 45762759098368.0)
Skip this batch 581! (Loss: 34957546225664.0)
Skip this batch 582! (Loss: 32702864556032.0)
Skip this batch 583! (Loss: 38664503558144.0)
Skip this batch 584! (Loss: 46961436655616.0)
Skip this batch 585! (Loss: 52882615304192.0)
Skip this batch 586! (Loss: 49976407228416.0)
Skip this batch 587! (Loss: 39719098384384.0)
Skip this batch 588! (Loss: 23776318193664.0)
Skip this batch 589! (Loss: 40054588178432.0)
Skip this batch 590! (Loss: 22042212564992.0)
Skip this batch 591! (Loss: 38973221109760.0)
Skip this batch 592! (Loss: 36397700349952.0)
Skip this batch 593! (Loss: 36785509892096.0)
Skip this batch 594! (Loss: 49188326866944.0)
Skip this batch 595! (Loss: 39568355098624.0)
Skip this batch 596! (Loss: 38657494876160.0)
Skip this batch 597! (Loss: 15652089757696.0)
Skip this batch 598! (Loss: 30463208980480.0)
Skip this batch 599! (Loss: 35639655399424.0)
Skip this batch 600! (Loss: 47324503998464.0)
[9600/16000]	[L1: 24436226916352.0000]	 timer_model: 13.57 + timer_data: 0.06s
Skip this batch 601! (Loss: 42728687665152.0)
Skip this batch 602! (Loss: 35843129475072.0)
Skip this batch 603! (Loss: 30996107886592.0)
Skip this batch 604! (Loss: 32524052987904.0)
Skip this batch 605! (Loss: 48230532382720.0)
Skip this batch 606! (Loss: 48671773163520.0)
Skip this batch 607! (Loss: 48342620962816.0)
Skip this batch 608! (Loss: 42350038482944.0)
Skip this batch 609! (Loss: 30121100574720.0)
Skip this batch 610! (Loss: 28753801510912.0)
Skip this batch 611! (Loss: 45734833422336.0)
Skip this batch 612! (Loss: 30205445931008.0)
Skip this batch 613! (Loss: 45791511052288.0)
Skip this batch 614! (Loss: 42044974170112.0)
Skip this batch 615! (Loss: 32848534831104.0)
Skip this batch 616! (Loss: 37866541416448.0)
Skip this batch 617! (Loss: 32053898772480.0)
Skip this batch 618! (Loss: 36645889900544.0)
Skip this batch 619! (Loss: 15401879601152.0)
Skip this batch 620! (Loss: 47199245303808.0)
Skip this batch 621! (Loss: 30605926465536.0)
Skip this batch 622! (Loss: 34844721545216.0)
Skip this batch 623! (Loss: 40623956557824.0)
Skip this batch 624! (Loss: 26203150876672.0)
Skip this batch 625! (Loss: 31170312011776.0)
Skip this batch 626! (Loss: 42206324850688.0)
Skip this batch 627! (Loss: 26129154965504.0)
Skip this batch 628! (Loss: 25393595678720.0)
Skip this batch 629! (Loss: 43233182744576.0)
Skip this batch 630! (Loss: 34916540612608.0)
Skip this batch 631! (Loss: 28363238408192.0)
Skip this batch 632! (Loss: 39287961681920.0)
Skip this batch 633! (Loss: 26309040275456.0)
Skip this batch 634! (Loss: 26824184692736.0)
Skip this batch 635! (Loss: 29973989556224.0)
Skip this batch 636! (Loss: 23726900903936.0)
Skip this batch 637! (Loss: 31245417316352.0)
Skip this batch 638! (Loss: 39695048245248.0)
Skip this batch 639! (Loss: 49493315682304.0)
Skip this batch 640! (Loss: 35589105647616.0)
Skip this batch 641! (Loss: 35220753481728.0)
Skip this batch 642! (Loss: 42268648013824.0)
Skip this batch 643! (Loss: 42509161988096.0)
Skip this batch 644! (Loss: 47507686031360.0)
Skip this batch 645! (Loss: 32975934717952.0)
Skip this batch 646! (Loss: 25956091691008.0)
Skip this batch 647! (Loss: 34544939958272.0)
Skip this batch 648! (Loss: 41228636782592.0)
Skip this batch 649! (Loss: 38964538900480.0)
Skip this batch 650! (Loss: 47639429120000.0)
Skip this batch 651! (Loss: 40041900408832.0)
Skip this batch 652! (Loss: 39838371807232.0)
Skip this batch 653! (Loss: 45143243620352.0)
Skip this batch 654! (Loss: 33905663016960.0)
Skip this batch 655! (Loss: 27466211000320.0)
Skip this batch 656! (Loss: 28282045071360.0)
Skip this batch 657! (Loss: 63797532295168.0)
Skip this batch 658! (Loss: 54912838795264.0)
Skip this batch 659! (Loss: 52468603944960.0)
Skip this batch 660! (Loss: 45933920256000.0)
Skip this batch 661! (Loss: 27040549961728.0)
Skip this batch 662! (Loss: 38379374772224.0)
Skip this batch 663! (Loss: 33253765414912.0)
Skip this batch 664! (Loss: 17671137525760.0)
Skip this batch 665! (Loss: 31892329988096.0)
Skip this batch 666! (Loss: 31316552712192.0)
Skip this batch 667! (Loss: 34728503672832.0)
Skip this batch 668! (Loss: 38558169563136.0)
Skip this batch 669! (Loss: 30838330753024.0)
Skip this batch 670! (Loss: 32952224317440.0)
Skip this batch 671! (Loss: 30729171894272.0)
Skip this batch 672! (Loss: 33452527190016.0)
Skip this batch 673! (Loss: 33529886932992.0)
Skip this batch 674! (Loss: 35470977269760.0)
Skip this batch 675! (Loss: 41682112348160.0)
Skip this batch 676! (Loss: 35734035628032.0)
Skip this batch 677! (Loss: 42617639272448.0)
Skip this batch 678! (Loss: 32967604830208.0)
Skip this batch 679! (Loss: 44446800412672.0)
Skip this batch 680! (Loss: 46614932619264.0)
Skip this batch 681! (Loss: 50705419206656.0)
Skip this batch 682! (Loss: 43013153751040.0)
Skip this batch 683! (Loss: 44627105153024.0)
Skip this batch 684! (Loss: 34613082718208.0)
Skip this batch 685! (Loss: 30417883234304.0)
Skip this batch 686! (Loss: 46347629625344.0)
Skip this batch 687! (Loss: 32365694943232.0)
Skip this batch 688! (Loss: 34022453411840.0)
Skip this batch 689! (Loss: 34028570804224.0)
Skip this batch 690! (Loss: 24528411426816.0)
Skip this batch 691! (Loss: 49578032234496.0)
Skip this batch 692! (Loss: 27646878547968.0)
Skip this batch 693! (Loss: 40247605854208.0)
Skip this batch 694! (Loss: 37277875044352.0)
Skip this batch 695! (Loss: 27926623944704.0)
Skip this batch 696! (Loss: 31793713512448.0)
Skip this batch 697! (Loss: 36753268277248.0)
Skip this batch 698! (Loss: 36382445666304.0)
Skip this batch 699! (Loss: 31179587715072.0)
Skip this batch 700! (Loss: 32996021239808.0)
[11200/16000]	[L1: 26175518801920.0000]	 timer_model: 13.57 + timer_data: 0.06s
Skip this batch 701! (Loss: 51038652465152.0)
Skip this batch 702! (Loss: 33742351499264.0)
Skip this batch 703! (Loss: 51203975151616.0)
Skip this batch 704! (Loss: 47247144255488.0)
Skip this batch 705! (Loss: 55832876154880.0)
Skip this batch 706! (Loss: 40946871828480.0)
Skip this batch 707! (Loss: 38101145616384.0)
Skip this batch 708! (Loss: 49626933624832.0)
Skip this batch 709! (Loss: 30349690142720.0)
Skip this batch 710! (Loss: 28670718640128.0)
Skip this batch 711! (Loss: 30679972708352.0)
Skip this batch 712! (Loss: 48419443834880.0)
Skip this batch 713! (Loss: 38984528953344.0)
Skip this batch 714! (Loss: 49965929857024.0)
Skip this batch 715! (Loss: 50772813283328.0)
Skip this batch 716! (Loss: 27998470275072.0)
Skip this batch 717! (Loss: 47309769408512.0)
Skip this batch 718! (Loss: 57469015425024.0)
Skip this batch 719! (Loss: 31070743429120.0)
Skip this batch 720! (Loss: 32896685441024.0)
Skip this batch 721! (Loss: 50212252942336.0)
Skip this batch 722! (Loss: 39718163054592.0)
Skip this batch 723! (Loss: 30535877394432.0)
Skip this batch 724! (Loss: 26565832343552.0)
Skip this batch 725! (Loss: 44552501067776.0)
Skip this batch 726! (Loss: 26518178758656.0)
Skip this batch 727! (Loss: 32086872293376.0)
Skip this batch 728! (Loss: 51222388146176.0)
Skip this batch 729! (Loss: 37069724319744.0)
Skip this batch 730! (Loss: 36278976380928.0)
Skip this batch 731! (Loss: 52311363682304.0)
Skip this batch 732! (Loss: 47583376441344.0)
Skip this batch 733! (Loss: 31658084401152.0)
Skip this batch 734! (Loss: 19930630062080.0)
Skip this batch 735! (Loss: 38000859807744.0)
Skip this batch 736! (Loss: 46574491140096.0)
Skip this batch 737! (Loss: 39186920898560.0)
Skip this batch 738! (Loss: 35265838055424.0)
Skip this batch 739! (Loss: 51592229289984.0)
Skip this batch 740! (Loss: 32186371670016.0)
Skip this batch 741! (Loss: 31558626967552.0)
Skip this batch 742! (Loss: 49513087631360.0)
Skip this batch 743! (Loss: 45050167820288.0)
Skip this batch 744! (Loss: 43623399817216.0)
Skip this batch 745! (Loss: 37007673786368.0)
Skip this batch 746! (Loss: 43078995935232.0)
Skip this batch 747! (Loss: 35575495131136.0)
Skip this batch 748! (Loss: 42249287106560.0)
Skip this batch 749! (Loss: 31864809062400.0)
Skip this batch 750! (Loss: 34858583719936.0)
Skip this batch 751! (Loss: 38863066103808.0)
Skip this batch 752! (Loss: 37565616881664.0)
Skip this batch 753! (Loss: 36005709086720.0)
Skip this batch 754! (Loss: 29067185225728.0)
Skip this batch 755! (Loss: 35838276665344.0)
Skip this batch 756! (Loss: 47737827491840.0)
Skip this batch 757! (Loss: 47825027072000.0)
Skip this batch 758! (Loss: 33499379662848.0)
Skip this batch 759! (Loss: 38068543291392.0)
Skip this batch 760! (Loss: 34374135316480.0)
Skip this batch 761! (Loss: 25768228814848.0)
Skip this batch 762! (Loss: 37239824318464.0)
Skip this batch 763! (Loss: 34690559901696.0)
Skip this batch 764! (Loss: 39604749074432.0)
Skip this batch 765! (Loss: 48475727200256.0)
Skip this batch 766! (Loss: 41086940610560.0)
Skip this batch 767! (Loss: 60525610270720.0)
Skip this batch 768! (Loss: 45641254305792.0)
Skip this batch 769! (Loss: 23653668356096.0)
Skip this batch 770! (Loss: 44978822709248.0)
Skip this batch 771! (Loss: 67212308119552.0)
Skip this batch 772! (Loss: 36776525692928.0)
Skip this batch 773! (Loss: 52273858215936.0)
Skip this batch 774! (Loss: 34522011795456.0)
Skip this batch 775! (Loss: 36007411974144.0)
Skip this batch 776! (Loss: 43422417158144.0)
Skip this batch 777! (Loss: 52441361940480.0)
Skip this batch 778! (Loss: 36027402027008.0)
Skip this batch 779! (Loss: 38097806950400.0)
Skip this batch 780! (Loss: 31264788709376.0)
Skip this batch 781! (Loss: 47915816976384.0)
Skip this batch 782! (Loss: 48885183545344.0)
Skip this batch 783! (Loss: 34827239686144.0)
Skip this batch 784! (Loss: 30735297675264.0)
Skip this batch 785! (Loss: 40660983873536.0)
Skip this batch 786! (Loss: 31683413803008.0)
Skip this batch 787! (Loss: 46400117145600.0)
Skip this batch 788! (Loss: 29564986195968.0)
Skip this batch 789! (Loss: 38755331211264.0)
Skip this batch 790! (Loss: 30797708918784.0)
Skip this batch 791! (Loss: 59046178586624.0)
Skip this batch 792! (Loss: 30274524020736.0)
Skip this batch 793! (Loss: 41086055612416.0)
Skip this batch 794! (Loss: 49010400296960.0)
Skip this batch 795! (Loss: 30265462226944.0)
Skip this batch 796! (Loss: 35235186081792.0)
Skip this batch 797! (Loss: 35722912333824.0)
Skip this batch 798! (Loss: 22961675304960.0)
Skip this batch 799! (Loss: 37685460729856.0)
Skip this batch 800! (Loss: 45689304252416.0)
[12800/16000]	[L1: 27880474017792.0000]	 timer_model: 13.62 + timer_data: 0.06s
Skip this batch 801! (Loss: 49912888688640.0)
Skip this batch 802! (Loss: 33921393754112.0)
Skip this batch 803! (Loss: 55047756972032.0)
Skip this batch 804! (Loss: 30035792625664.0)
Skip this batch 805! (Loss: 35869851385856.0)
Skip this batch 806! (Loss: 32364990300160.0)
Skip this batch 807! (Loss: 59435074453504.0)
Skip this batch 808! (Loss: 32929958854656.0)
Skip this batch 809! (Loss: 33613550714880.0)
Skip this batch 810! (Loss: 29374942281728.0)
Skip this batch 811! (Loss: 39930340311040.0)
Skip this batch 812! (Loss: 36458102521856.0)
Skip this batch 813! (Loss: 36813083246592.0)
Skip this batch 814! (Loss: 52134066257920.0)
Skip this batch 815! (Loss: 44725969092608.0)
Skip this batch 816! (Loss: 27717118459904.0)
Skip this batch 817! (Loss: 45362878349312.0)
Skip this batch 818! (Loss: 41113704464384.0)
Skip this batch 819! (Loss: 32869692997632.0)
Skip this batch 820! (Loss: 40505450692608.0)
Skip this batch 821! (Loss: 35790117666816.0)
Skip this batch 822! (Loss: 47467215192064.0)
Skip this batch 823! (Loss: 25283419701248.0)
Skip this batch 824! (Loss: 28489482764288.0)
Skip this batch 825! (Loss: 54956363087872.0)
Skip this batch 826! (Loss: 25382864551936.0)
Skip this batch 827! (Loss: 30748109176832.0)
Skip this batch 828! (Loss: 39050455023616.0)
Skip this batch 829! (Loss: 46095858139136.0)
Skip this batch 830! (Loss: 26307614212096.0)
Skip this batch 831! (Loss: 46297876791296.0)
Skip this batch 832! (Loss: 36418457960448.0)
Skip this batch 833! (Loss: 50310248660992.0)
Skip this batch 834! (Loss: 38090961846272.0)
Skip this batch 835! (Loss: 32468845461504.0)
Skip this batch 836! (Loss: 30444852609024.0)
Skip this batch 837! (Loss: 46723577675776.0)
Skip this batch 838! (Loss: 22331189624832.0)
Skip this batch 839! (Loss: 35447916986368.0)
Skip this batch 840! (Loss: 49623385243648.0)
Skip this batch 841! (Loss: 32063679889408.0)
Skip this batch 842! (Loss: 36802744287232.0)
Skip this batch 843! (Loss: 28651754094592.0)
Skip this batch 844! (Loss: 32020325466112.0)
Skip this batch 845! (Loss: 39357973004288.0)
Skip this batch 846! (Loss: 29789983342592.0)
Skip this batch 847! (Loss: 35132249473024.0)
Skip this batch 848! (Loss: 32708226973696.0)
Skip this batch 849! (Loss: 27863266885632.0)
Skip this batch 850! (Loss: 34502929809408.0)
Skip this batch 851! (Loss: 48071715061760.0)
Skip this batch 852! (Loss: 44046680588288.0)
Skip this batch 853! (Loss: 62156410191872.0)
Skip this batch 854! (Loss: 36741985599488.0)
Skip this batch 855! (Loss: 46074660126720.0)
Skip this batch 856! (Loss: 28749173096448.0)
Skip this batch 857! (Loss: 58833531568128.0)
Skip this batch 858! (Loss: 46525698801664.0)
Skip this batch 859! (Loss: 29497965412352.0)
Skip this batch 860! (Loss: 31373150650368.0)
Skip this batch 861! (Loss: 36463773220864.0)
Skip this batch 862! (Loss: 30979815112704.0)
Skip this batch 863! (Loss: 24914230771712.0)
Skip this batch 864! (Loss: 32886853992448.0)
Skip this batch 865! (Loss: 35511544578048.0)
Skip this batch 866! (Loss: 41078661054464.0)
Skip this batch 867! (Loss: 51793815928832.0)
Skip this batch 868! (Loss: 40167024885760.0)
Skip this batch 869! (Loss: 34347425988608.0)
Skip this batch 870! (Loss: 47641429803008.0)
Skip this batch 871! (Loss: 42929670324224.0)
Skip this batch 872! (Loss: 28376173641728.0)
Skip this batch 873! (Loss: 44439657512960.0)
Skip this batch 874! (Loss: 48256344129536.0)
Skip this batch 875! (Loss: 43900467150848.0)
Skip this batch 876! (Loss: 36839742242816.0)
Skip this batch 877! (Loss: 49889148928000.0)
Skip this batch 878! (Loss: 38530654928896.0)
Skip this batch 879! (Loss: 21968558489600.0)
Skip this batch 880! (Loss: 29838400290816.0)
Skip this batch 881! (Loss: 39997092659200.0)
Skip this batch 882! (Loss: 29971326173184.0)
Skip this batch 883! (Loss: 47961660719104.0)
Skip this batch 884! (Loss: 48141436977152.0)
Skip this batch 885! (Loss: 40487243218944.0)
Skip this batch 886! (Loss: 31402642898944.0)
Skip this batch 887! (Loss: 27012949344256.0)
Skip this batch 888! (Loss: 28158170497024.0)
Skip this batch 889! (Loss: 28616626798592.0)
Skip this batch 890! (Loss: 55635206995968.0)
Skip this batch 891! (Loss: 35763211206656.0)
Skip this batch 892! (Loss: 30397442293760.0)
Skip this batch 893! (Loss: 41405934206976.0)
Skip this batch 894! (Loss: 25986452160512.0)
Skip this batch 895! (Loss: 36345342853120.0)
Skip this batch 896! (Loss: 35103409438720.0)
Skip this batch 897! (Loss: 28339322486784.0)
Skip this batch 898! (Loss: 35963375976448.0)
Skip this batch 899! (Loss: 36297175465984.0)
Skip this batch 900! (Loss: 36085665103872.0)
[14400/16000]	[L1: 28981153759232.0000]	 timer_model: 13.53 + timer_data: 0.06s
Skip this batch 901! (Loss: 44516887232512.0)
Skip this batch 902! (Loss: 31744759693312.0)
Skip this batch 903! (Loss: 39447886299136.0)
Skip this batch 904! (Loss: 31432810430464.0)
Skip this batch 905! (Loss: 45423465070592.0)
Skip this batch 906! (Loss: 39615478104064.0)
Skip this batch 907! (Loss: 32823526293504.0)
Skip this batch 908! (Loss: 37221813977088.0)
Skip this batch 909! (Loss: 35115061215232.0)
Skip this batch 910! (Loss: 35796203601920.0)
Skip this batch 911! (Loss: 27817492348928.0)
Skip this batch 912! (Loss: 35100028829696.0)
Skip this batch 913! (Loss: 40980849885184.0)
Skip this batch 914! (Loss: 40122972110848.0)
Skip this batch 915! (Loss: 30711981539328.0)
Skip this batch 916! (Loss: 43850978557952.0)
Skip this batch 917! (Loss: 34245519081472.0)
Skip this batch 918! (Loss: 38040714084352.0)
Skip this batch 919! (Loss: 43548531490816.0)
Skip this batch 920! (Loss: 28196634361856.0)
Skip this batch 921! (Loss: 32683220533248.0)
Skip this batch 922! (Loss: 32999026458624.0)
Skip this batch 923! (Loss: 44480044466176.0)
Skip this batch 924! (Loss: 43750822772736.0)
Skip this batch 925! (Loss: 32543543918592.0)
Skip this batch 926! (Loss: 31857756340224.0)
Skip this batch 927! (Loss: 50970423721984.0)
Skip this batch 928! (Loss: 34311851999232.0)
Skip this batch 929! (Loss: 38552859574272.0)
Skip this batch 930! (Loss: 46256458039296.0)
Skip this batch 931! (Loss: 40035403431936.0)
Skip this batch 932! (Loss: 33757111255040.0)
Skip this batch 933! (Loss: 34386900680704.0)
Skip this batch 934! (Loss: 27154926534656.0)
Skip this batch 935! (Loss: 33169426350080.0)
Skip this batch 936! (Loss: 32544145801216.0)
Skip this batch 937! (Loss: 31644750708736.0)
Skip this batch 938! (Loss: 27789004636160.0)
Skip this batch 939! (Loss: 32263798521856.0)
Skip this batch 940! (Loss: 30813854892032.0)
Skip this batch 941! (Loss: 40256824934400.0)
Skip this batch 942! (Loss: 33104381083648.0)
Skip this batch 943! (Loss: 25882911571968.0)
Skip this batch 944! (Loss: 42135206232064.0)
Skip this batch 945! (Loss: 44199873347584.0)
Skip this batch 946! (Loss: 42264734728192.0)
Skip this batch 947! (Loss: 38869835710464.0)
Skip this batch 948! (Loss: 34226743279616.0)
Skip this batch 949! (Loss: 42809201524736.0)
Skip this batch 950! (Loss: 40055796137984.0)
Skip this batch 951! (Loss: 44017236574208.0)
Skip this batch 952! (Loss: 29645827211264.0)
Skip this batch 953! (Loss: 55204615553024.0)
Skip this batch 954! (Loss: 21446789169152.0)
Skip this batch 955! (Loss: 23440947937280.0)
Skip this batch 956! (Loss: 23503786999808.0)
Skip this batch 957! (Loss: 41367145283584.0)
Skip this batch 958! (Loss: 64491907710976.0)
Skip this batch 959! (Loss: 25659078344704.0)
Skip this batch 960! (Loss: 44554975707136.0)
Skip this batch 961! (Loss: 44872098643968.0)
Skip this batch 962! (Loss: 33820709486592.0)
Skip this batch 963! (Loss: 32977694228480.0)
Skip this batch 964! (Loss: 32523040063488.0)
Skip this batch 965! (Loss: 39781136334848.0)
Skip this batch 966! (Loss: 29410604351488.0)
Skip this batch 967! (Loss: 37698454683648.0)
Skip this batch 968! (Loss: 27545332350976.0)
Skip this batch 969! (Loss: 46537686122496.0)
Skip this batch 970! (Loss: 38075057045504.0)
Skip this batch 971! (Loss: 36583545765888.0)
Skip this batch 972! (Loss: 34409170337792.0)
Skip this batch 973! (Loss: 43629355728896.0)
Skip this batch 974! (Loss: 37014292398080.0)
Skip this batch 975! (Loss: 35892609679360.0)
Skip this batch 976! (Loss: 37781145387008.0)
Skip this batch 977! (Loss: 45048271994880.0)
Skip this batch 978! (Loss: 53484284018688.0)
Skip this batch 979! (Loss: 49047670882304.0)
Skip this batch 980! (Loss: 29384968765440.0)
Skip this batch 981! (Loss: 36310479798272.0)
Skip this batch 982! (Loss: 34469150982144.0)
Skip this batch 983! (Loss: 50269052207104.0)
Skip this batch 984! (Loss: 42979934863360.0)
Skip this batch 985! (Loss: 40047269117952.0)
Skip this batch 986! (Loss: 30328544559104.0)
Skip this batch 987! (Loss: 29810145361920.0)
Skip this batch 988! (Loss: 23711539265536.0)
Skip this batch 989! (Loss: 47470990065664.0)
Skip this batch 990! (Loss: 38244154605568.0)
Skip this batch 991! (Loss: 33512147124224.0)
Skip this batch 992! (Loss: 38924814647296.0)
Skip this batch 993! (Loss: 31331503308800.0)
Skip this batch 994! (Loss: 25456264871936.0)
Skip this batch 995! (Loss: 39442056216576.0)
Skip this batch 996! (Loss: 29443676438528.0)
Skip this batch 997! (Loss: 38542986182656.0)
Skip this batch 998! (Loss: 20092460990464.0)
Skip this batch 999! (Loss: 32603331624960.0)
Skip this batch 1000! (Loss: 43000872828928.0)
[16000/16000]	[L1: 29759408963584.0000]	 timer_model: 13.67 + timer_data: 0.06s

Evaluation:
[Set5 x2]	PSNR: 9.326 (Best: 35.268 @epoch 51)
Total time: 2.73s
 Now time: Sat Mar 20 17:35:10 2021

2021-03-20 17:35:12  [Epoch 93]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 2468644126720.0000]	 timer_model: 59.68 + timer_data: 0.28s
[3200/16000]	[L1: 1234658525184.0000]	 timer_model: 59.88 + timer_data: 0.04s
[4800/16000]	[L1: 823596023808.0000]	 timer_model: 59.57 + timer_data: 0.04s
[6400/16000]	[L1: 618942562304.0000]	 timer_model: 59.73 + timer_data: 0.04s
[8000/16000]	[L1: 495205679104.0000]	 timer_model: 59.98 + timer_data: 0.04s
[9600/16000]	[L1: 413184786432.0000]	 timer_model: 61.88 + timer_data: 0.04s
[11200/16000]	[L1: 354223816704.0000]	 timer_model: 60.20 + timer_data: 0.04s
[12800/16000]	[L1: 309964800000.0000]	 timer_model: 60.08 + timer_data: 0.03s
[14400/16000]	[L1: 275539296256.0000]	 timer_model: 60.15 + timer_data: 0.03s
[16000/16000]	[L1: 247994253312.0000]	 timer_model: 59.88 + timer_data: 0.04s

Evaluation:
[Set5 x2]	PSNR: 9.026 (Best: 35.268 @epoch 51)
Total time: 2.89s
 Now time: Sat Mar 20 17:45:17 2021

2021-03-20 17:45:18  [Epoch 94]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
[1600/16000]	[L1: 139971904.0000]	 timer_model: 60.30 + timer_data: 0.28s
[3200/16000]	[L1: 110980984.0000]	 timer_model: 59.64 + timer_data: 0.04s
[4800/16000]	[L1: 114741272.0000]	 timer_model: 59.49 + timer_data: 0.04s
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 56.83 + timer_data: 0.04s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.63 + timer_data: 0.04s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.63 + timer_data: 0.05s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.06s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.65 + timer_data: 0.06s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.59 + timer_data: 0.07s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.60 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.91s
 Now time: Sat Mar 20 17:50:40 2021

2021-03-20 17:50:42  [Epoch 95]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 14.51 + timer_data: 0.29s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 14.86 + timer_data: 0.05s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.67 + timer_data: 0.06s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.63 + timer_data: 0.04s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.07s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.60 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.60 + timer_data: 0.06s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.52 + timer_data: 0.06s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.07s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.73s
 Now time: Sat Mar 20 17:53:04 2021

2021-03-20 17:53:05  [Epoch 96]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.81 + timer_data: 0.28s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.67 + timer_data: 0.05s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.61 + timer_data: 0.05s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.63 + timer_data: 0.06s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.07s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.61 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.67 + timer_data: 0.06s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.65 + timer_data: 0.07s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.71 + timer_data: 0.07s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.88s
 Now time: Sat Mar 20 17:55:26 2021

2021-03-20 17:55:27  [Epoch 97]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.65 + timer_data: 0.29s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.05s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.60 + timer_data: 0.05s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.82 + timer_data: 0.05s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.54 + timer_data: 0.06s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.63 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.57 + timer_data: 0.06s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.05s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.61 + timer_data: 0.06s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.69s
 Now time: Sat Mar 20 17:57:47 2021

2021-03-20 17:57:49  [Epoch 98]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.59 + timer_data: 0.29s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.05s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.51 + timer_data: 0.05s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.49 + timer_data: 0.06s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.06s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.64 + timer_data: 0.05s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.74 + timer_data: 0.06s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.71 + timer_data: 0.05s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.78 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.94s
 Now time: Sat Mar 20 18:00:09 2021

2021-03-20 18:00:11  [Epoch 99]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.67 + timer_data: 0.29s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.60 + timer_data: 0.04s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.49 + timer_data: 0.05s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.51 + timer_data: 0.04s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.74 + timer_data: 0.06s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.63 + timer_data: 0.05s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.60 + timer_data: 0.05s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.56 + timer_data: 0.07s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.53 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.79s
 Now time: Sat Mar 20 18:02:31 2021

2021-03-20 18:02:32  [Epoch 100]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.78 + timer_data: 0.28s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.56 + timer_data: 0.05s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.68 + timer_data: 0.06s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.59 + timer_data: 0.06s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.60 + timer_data: 0.06s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.61 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.56 + timer_data: 0.06s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.52 + timer_data: 0.06s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.52 + timer_data: 0.06s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.55 + timer_data: 0.05s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.70s
 Now time: Sat Mar 20 18:04:52 2021

2021-03-20 18:04:53  [Epoch 101]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 14.04 + timer_data: 0.28s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.61 + timer_data: 0.05s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.06s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.65 + timer_data: 0.05s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.06s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.94 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.75 + timer_data: 0.04s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.59 + timer_data: 0.05s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.61 + timer_data: 0.07s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.51 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.71s
 Now time: Sat Mar 20 18:07:14 2021

2021-03-20 18:07:15  [Epoch 102]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.64 + timer_data: 0.28s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.65 + timer_data: 0.05s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.67 + timer_data: 0.06s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.75 + timer_data: 0.06s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.51 + timer_data: 0.07s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.07s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.64 + timer_data: 0.06s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.67 + timer_data: 0.06s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.63 + timer_data: 0.07s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.63 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.68s
 Now time: Sat Mar 20 18:09:36 2021

2021-03-20 18:09:37  [Epoch 103]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.59 + timer_data: 0.29s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.64 + timer_data: 0.04s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.59 + timer_data: 0.05s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.52 + timer_data: 0.04s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.49 + timer_data: 0.06s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.52 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.64 + timer_data: 0.05s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.79 + timer_data: 0.06s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.52 + timer_data: 0.06s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.64 + timer_data: 0.06s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.79s
 Now time: Sat Mar 20 18:11:57 2021

2021-03-20 18:11:58  [Epoch 104]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.63 + timer_data: 0.29s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.69 + timer_data: 0.05s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 14.45 + timer_data: 0.06s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.56 + timer_data: 0.06s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.59 + timer_data: 0.06s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.52 + timer_data: 0.07s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.57 + timer_data: 0.05s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.57 + timer_data: 0.05s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.60 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.67s
 Now time: Sat Mar 20 18:14:19 2021

2021-03-20 18:14:20  [Epoch 105]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.30s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.53 + timer_data: 0.04s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.64 + timer_data: 0.05s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.68 + timer_data: 0.05s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.56 + timer_data: 0.06s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.61 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.55 + timer_data: 0.06s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.52 + timer_data: 0.05s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.75 + timer_data: 0.06s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.06s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.68s
 Now time: Sat Mar 20 18:16:40 2021

2021-03-20 18:16:41  [Epoch 106]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.29s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.05s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.80 + timer_data: 0.05s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.67 + timer_data: 0.06s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.06s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.61 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.67 + timer_data: 0.06s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.80 + timer_data: 0.07s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.60 + timer_data: 0.07s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.72 + timer_data: 0.08s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.76s
 Now time: Sat Mar 20 18:19:02 2021

2021-03-20 18:19:03  [Epoch 107]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 14.25 + timer_data: 0.31s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.88 + timer_data: 0.05s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.79 + timer_data: 0.06s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.71 + timer_data: 0.04s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.70 + timer_data: 0.04s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.66 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.64 + timer_data: 0.06s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.60 + timer_data: 0.06s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.06s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.77 + timer_data: 0.05s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.78s
 Now time: Sat Mar 20 18:21:25 2021

2021-03-20 18:21:26  [Epoch 108]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.55 + timer_data: 0.29s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.51 + timer_data: 0.05s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 14.42 + timer_data: 0.06s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.84 + timer_data: 0.06s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.69 + timer_data: 0.07s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 14.86 + timer_data: 0.08s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.86 + timer_data: 0.06s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.48 + timer_data: 0.05s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.44 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.69s
 Now time: Sat Mar 20 18:23:48 2021

2021-03-20 18:23:50  [Epoch 109]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.63 + timer_data: 0.29s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.93 + timer_data: 0.05s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.66 + timer_data: 0.05s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.70 + timer_data: 0.05s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.63 + timer_data: 0.07s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.74 + timer_data: 0.05s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.57 + timer_data: 0.04s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.67 + timer_data: 0.04s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.61 + timer_data: 0.06s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.57 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.71s
 Now time: Sat Mar 20 18:26:10 2021

2021-03-20 18:26:12  [Epoch 110]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.89 + timer_data: 0.29s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.60 + timer_data: 0.04s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.53 + timer_data: 0.05s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.59 + timer_data: 0.06s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.53 + timer_data: 0.06s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.78 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.64 + timer_data: 0.06s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.61 + timer_data: 0.06s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.70 + timer_data: 0.06s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.63 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.72s
 Now time: Sat Mar 20 18:28:32 2021

2021-03-20 18:28:33  [Epoch 111]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.63 + timer_data: 0.30s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.60 + timer_data: 0.04s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.63 + timer_data: 0.06s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.65 + timer_data: 0.05s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.65 + timer_data: 0.07s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.71 + timer_data: 0.06s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.07s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.06s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.59 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.73s
 Now time: Sat Mar 20 18:30:54 2021

2021-03-20 18:30:55  [Epoch 112]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.65 + timer_data: 0.29s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.89 + timer_data: 0.05s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.60 + timer_data: 0.05s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.75 + timer_data: 0.06s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.82 + timer_data: 0.06s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.65 + timer_data: 0.07s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.75 + timer_data: 0.07s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.76 + timer_data: 0.07s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.80 + timer_data: 0.08s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.65 + timer_data: 0.08s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.74s
 Now time: Sat Mar 20 18:33:16 2021

2021-03-20 18:33:18  [Epoch 113]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.56 + timer_data: 0.29s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.65 + timer_data: 0.04s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.05s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.55 + timer_data: 0.05s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.61 + timer_data: 0.06s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.07s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.67 + timer_data: 0.05s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.73 + timer_data: 0.06s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.06s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.61 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.72s
 Now time: Sat Mar 20 18:35:38 2021

2021-03-20 18:35:39  [Epoch 114]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.55 + timer_data: 0.28s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.60 + timer_data: 0.04s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.75 + timer_data: 0.05s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.06s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.59 + timer_data: 0.06s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.59 + timer_data: 0.05s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.06s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.61 + timer_data: 0.06s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.73s
 Now time: Sat Mar 20 18:37:59 2021

2021-03-20 18:38:01  [Epoch 115]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
Skip this batch 1! (Loss: inf)
Skip this batch 2! (Loss: inf)
Skip this batch 3! (Loss: inf)
Skip this batch 4! (Loss: inf)
Skip this batch 5! (Loss: inf)
Skip this batch 6! (Loss: inf)
Skip this batch 7! (Loss: inf)
Skip this batch 8! (Loss: inf)
Skip this batch 9! (Loss: inf)
Skip this batch 10! (Loss: inf)
Skip this batch 11! (Loss: inf)
Skip this batch 12! (Loss: inf)
Skip this batch 13! (Loss: inf)
Skip this batch 14! (Loss: inf)
Skip this batch 15! (Loss: inf)
Skip this batch 16! (Loss: inf)
Skip this batch 17! (Loss: inf)
Skip this batch 18! (Loss: inf)
Skip this batch 19! (Loss: inf)
Skip this batch 20! (Loss: inf)
Skip this batch 21! (Loss: inf)
Skip this batch 22! (Loss: inf)
Skip this batch 23! (Loss: inf)
Skip this batch 24! (Loss: inf)
Skip this batch 25! (Loss: inf)
Skip this batch 26! (Loss: inf)
Skip this batch 27! (Loss: inf)
Skip this batch 28! (Loss: inf)
Skip this batch 29! (Loss: inf)
Skip this batch 30! (Loss: inf)
Skip this batch 31! (Loss: inf)
Skip this batch 32! (Loss: inf)
Skip this batch 33! (Loss: inf)
Skip this batch 34! (Loss: inf)
Skip this batch 35! (Loss: inf)
Skip this batch 36! (Loss: inf)
Skip this batch 37! (Loss: inf)
Skip this batch 38! (Loss: inf)
Skip this batch 39! (Loss: inf)
Skip this batch 40! (Loss: inf)
Skip this batch 41! (Loss: inf)
Skip this batch 42! (Loss: inf)
Skip this batch 43! (Loss: inf)
Skip this batch 44! (Loss: inf)
Skip this batch 45! (Loss: inf)
Skip this batch 46! (Loss: inf)
Skip this batch 47! (Loss: inf)
Skip this batch 48! (Loss: inf)
Skip this batch 49! (Loss: inf)
Skip this batch 50! (Loss: inf)
Skip this batch 51! (Loss: inf)
Skip this batch 52! (Loss: inf)
Skip this batch 53! (Loss: inf)
Skip this batch 54! (Loss: inf)
Skip this batch 55! (Loss: inf)
Skip this batch 56! (Loss: inf)
Skip this batch 57! (Loss: inf)
Skip this batch 58! (Loss: inf)
Skip this batch 59! (Loss: inf)
Skip this batch 60! (Loss: inf)
Skip this batch 61! (Loss: inf)
Skip this batch 62! (Loss: inf)
Skip this batch 63! (Loss: inf)
Skip this batch 64! (Loss: inf)
Skip this batch 65! (Loss: inf)
Skip this batch 66! (Loss: inf)
Skip this batch 67! (Loss: inf)
Skip this batch 68! (Loss: inf)
Skip this batch 69! (Loss: inf)
Skip this batch 70! (Loss: inf)
Skip this batch 71! (Loss: inf)
Skip this batch 72! (Loss: inf)
Skip this batch 73! (Loss: inf)
Skip this batch 74! (Loss: inf)
Skip this batch 75! (Loss: inf)
Skip this batch 76! (Loss: inf)
Skip this batch 77! (Loss: inf)
Skip this batch 78! (Loss: inf)
Skip this batch 79! (Loss: inf)
Skip this batch 80! (Loss: inf)
Skip this batch 81! (Loss: inf)
Skip this batch 82! (Loss: inf)
Skip this batch 83! (Loss: inf)
Skip this batch 84! (Loss: inf)
Skip this batch 85! (Loss: inf)
Skip this batch 86! (Loss: inf)
Skip this batch 87! (Loss: inf)
Skip this batch 88! (Loss: inf)
Skip this batch 89! (Loss: inf)
Skip this batch 90! (Loss: inf)
Skip this batch 91! (Loss: inf)
Skip this batch 92! (Loss: inf)
Skip this batch 93! (Loss: inf)
Skip this batch 94! (Loss: inf)
Skip this batch 95! (Loss: inf)
Skip this batch 96! (Loss: inf)
Skip this batch 97! (Loss: inf)
Skip this batch 98! (Loss: inf)
Skip this batch 99! (Loss: inf)
Skip this batch 100! (Loss: inf)
[1600/16000]	[L1: inf]	 timer_model: 13.59 + timer_data: 0.29s
Skip this batch 101! (Loss: inf)
Skip this batch 102! (Loss: inf)
Skip this batch 103! (Loss: inf)
Skip this batch 104! (Loss: inf)
Skip this batch 105! (Loss: inf)
Skip this batch 106! (Loss: inf)
Skip this batch 107! (Loss: inf)
Skip this batch 108! (Loss: inf)
Skip this batch 109! (Loss: inf)
Skip this batch 110! (Loss: inf)
Skip this batch 111! (Loss: inf)
Skip this batch 112! (Loss: inf)
Skip this batch 113! (Loss: inf)
Skip this batch 114! (Loss: inf)
Skip this batch 115! (Loss: inf)
Skip this batch 116! (Loss: inf)
Skip this batch 117! (Loss: inf)
Skip this batch 118! (Loss: inf)
Skip this batch 119! (Loss: inf)
Skip this batch 120! (Loss: inf)
Skip this batch 121! (Loss: inf)
Skip this batch 122! (Loss: inf)
Skip this batch 123! (Loss: inf)
Skip this batch 124! (Loss: inf)
Skip this batch 125! (Loss: inf)
Skip this batch 126! (Loss: inf)
Skip this batch 127! (Loss: inf)
Skip this batch 128! (Loss: inf)
Skip this batch 129! (Loss: inf)
Skip this batch 130! (Loss: inf)
Skip this batch 131! (Loss: inf)
Skip this batch 132! (Loss: inf)
Skip this batch 133! (Loss: inf)
Skip this batch 134! (Loss: inf)
Skip this batch 135! (Loss: inf)
Skip this batch 136! (Loss: inf)
Skip this batch 137! (Loss: inf)
Skip this batch 138! (Loss: inf)
Skip this batch 139! (Loss: inf)
Skip this batch 140! (Loss: inf)
Skip this batch 141! (Loss: inf)
Skip this batch 142! (Loss: inf)
Skip this batch 143! (Loss: inf)
Skip this batch 144! (Loss: inf)
Skip this batch 145! (Loss: inf)
Skip this batch 146! (Loss: inf)
Skip this batch 147! (Loss: inf)
Skip this batch 148! (Loss: inf)
Skip this batch 149! (Loss: inf)
Skip this batch 150! (Loss: inf)
Skip this batch 151! (Loss: inf)
Skip this batch 152! (Loss: inf)
Skip this batch 153! (Loss: inf)
Skip this batch 154! (Loss: inf)
Skip this batch 155! (Loss: inf)
Skip this batch 156! (Loss: inf)
Skip this batch 157! (Loss: inf)
Skip this batch 158! (Loss: inf)
Skip this batch 159! (Loss: inf)
Skip this batch 160! (Loss: inf)
Skip this batch 161! (Loss: inf)
Skip this batch 162! (Loss: inf)
Skip this batch 163! (Loss: inf)
Skip this batch 164! (Loss: inf)
Skip this batch 165! (Loss: inf)
Skip this batch 166! (Loss: inf)
Skip this batch 167! (Loss: inf)
Skip this batch 168! (Loss: inf)
Skip this batch 169! (Loss: inf)
Skip this batch 170! (Loss: inf)
Skip this batch 171! (Loss: inf)
Skip this batch 172! (Loss: inf)
Skip this batch 173! (Loss: inf)
Skip this batch 174! (Loss: inf)
Skip this batch 175! (Loss: inf)
Skip this batch 176! (Loss: inf)
Skip this batch 177! (Loss: inf)
Skip this batch 178! (Loss: inf)
Skip this batch 179! (Loss: inf)
Skip this batch 180! (Loss: inf)
Skip this batch 181! (Loss: inf)
Skip this batch 182! (Loss: inf)
Skip this batch 183! (Loss: inf)
Skip this batch 184! (Loss: inf)
Skip this batch 185! (Loss: inf)
Skip this batch 186! (Loss: inf)
Skip this batch 187! (Loss: inf)
Skip this batch 188! (Loss: inf)
Skip this batch 189! (Loss: inf)
Skip this batch 190! (Loss: inf)
Skip this batch 191! (Loss: inf)
Skip this batch 192! (Loss: inf)
Skip this batch 193! (Loss: inf)
Skip this batch 194! (Loss: inf)
Skip this batch 195! (Loss: inf)
Skip this batch 196! (Loss: inf)
Skip this batch 197! (Loss: inf)
Skip this batch 198! (Loss: inf)
Skip this batch 199! (Loss: inf)
Skip this batch 200! (Loss: inf)
[3200/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.05s
Skip this batch 201! (Loss: inf)
Skip this batch 202! (Loss: inf)
Skip this batch 203! (Loss: inf)
Skip this batch 204! (Loss: inf)
Skip this batch 205! (Loss: inf)
Skip this batch 206! (Loss: inf)
Skip this batch 207! (Loss: inf)
Skip this batch 208! (Loss: inf)
Skip this batch 209! (Loss: inf)
Skip this batch 210! (Loss: inf)
Skip this batch 211! (Loss: inf)
Skip this batch 212! (Loss: inf)
Skip this batch 213! (Loss: inf)
Skip this batch 214! (Loss: inf)
Skip this batch 215! (Loss: inf)
Skip this batch 216! (Loss: inf)
Skip this batch 217! (Loss: inf)
Skip this batch 218! (Loss: inf)
Skip this batch 219! (Loss: inf)
Skip this batch 220! (Loss: inf)
Skip this batch 221! (Loss: inf)
Skip this batch 222! (Loss: inf)
Skip this batch 223! (Loss: inf)
Skip this batch 224! (Loss: inf)
Skip this batch 225! (Loss: inf)
Skip this batch 226! (Loss: inf)
Skip this batch 227! (Loss: inf)
Skip this batch 228! (Loss: inf)
Skip this batch 229! (Loss: inf)
Skip this batch 230! (Loss: inf)
Skip this batch 231! (Loss: inf)
Skip this batch 232! (Loss: inf)
Skip this batch 233! (Loss: inf)
Skip this batch 234! (Loss: inf)
Skip this batch 235! (Loss: inf)
Skip this batch 236! (Loss: inf)
Skip this batch 237! (Loss: inf)
Skip this batch 238! (Loss: inf)
Skip this batch 239! (Loss: inf)
Skip this batch 240! (Loss: inf)
Skip this batch 241! (Loss: inf)
Skip this batch 242! (Loss: inf)
Skip this batch 243! (Loss: inf)
Skip this batch 244! (Loss: inf)
Skip this batch 245! (Loss: inf)
Skip this batch 246! (Loss: inf)
Skip this batch 247! (Loss: inf)
Skip this batch 248! (Loss: inf)
Skip this batch 249! (Loss: inf)
Skip this batch 250! (Loss: inf)
Skip this batch 251! (Loss: inf)
Skip this batch 252! (Loss: inf)
Skip this batch 253! (Loss: inf)
Skip this batch 254! (Loss: inf)
Skip this batch 255! (Loss: inf)
Skip this batch 256! (Loss: inf)
Skip this batch 257! (Loss: inf)
Skip this batch 258! (Loss: inf)
Skip this batch 259! (Loss: inf)
Skip this batch 260! (Loss: inf)
Skip this batch 261! (Loss: inf)
Skip this batch 262! (Loss: inf)
Skip this batch 263! (Loss: inf)
Skip this batch 264! (Loss: inf)
Skip this batch 265! (Loss: inf)
Skip this batch 266! (Loss: inf)
Skip this batch 267! (Loss: inf)
Skip this batch 268! (Loss: inf)
Skip this batch 269! (Loss: inf)
Skip this batch 270! (Loss: inf)
Skip this batch 271! (Loss: inf)
Skip this batch 272! (Loss: inf)
Skip this batch 273! (Loss: inf)
Skip this batch 274! (Loss: inf)
Skip this batch 275! (Loss: inf)
Skip this batch 276! (Loss: inf)
Skip this batch 277! (Loss: inf)
Skip this batch 278! (Loss: inf)
Skip this batch 279! (Loss: inf)
Skip this batch 280! (Loss: inf)
Skip this batch 281! (Loss: inf)
Skip this batch 282! (Loss: inf)
Skip this batch 283! (Loss: inf)
Skip this batch 284! (Loss: inf)
Skip this batch 285! (Loss: inf)
Skip this batch 286! (Loss: inf)
Skip this batch 287! (Loss: inf)
Skip this batch 288! (Loss: inf)
Skip this batch 289! (Loss: inf)
Skip this batch 290! (Loss: inf)
Skip this batch 291! (Loss: inf)
Skip this batch 292! (Loss: inf)
Skip this batch 293! (Loss: inf)
Skip this batch 294! (Loss: inf)
Skip this batch 295! (Loss: inf)
Skip this batch 296! (Loss: inf)
Skip this batch 297! (Loss: inf)
Skip this batch 298! (Loss: inf)
Skip this batch 299! (Loss: inf)
Skip this batch 300! (Loss: inf)
[4800/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.06s
Skip this batch 301! (Loss: inf)
Skip this batch 302! (Loss: inf)
Skip this batch 303! (Loss: inf)
Skip this batch 304! (Loss: inf)
Skip this batch 305! (Loss: inf)
Skip this batch 306! (Loss: inf)
Skip this batch 307! (Loss: inf)
Skip this batch 308! (Loss: inf)
Skip this batch 309! (Loss: inf)
Skip this batch 310! (Loss: inf)
Skip this batch 311! (Loss: inf)
Skip this batch 312! (Loss: inf)
Skip this batch 313! (Loss: inf)
Skip this batch 314! (Loss: inf)
Skip this batch 315! (Loss: inf)
Skip this batch 316! (Loss: inf)
Skip this batch 317! (Loss: inf)
Skip this batch 318! (Loss: inf)
Skip this batch 319! (Loss: inf)
Skip this batch 320! (Loss: inf)
Skip this batch 321! (Loss: inf)
Skip this batch 322! (Loss: inf)
Skip this batch 323! (Loss: inf)
Skip this batch 324! (Loss: inf)
Skip this batch 325! (Loss: inf)
Skip this batch 326! (Loss: inf)
Skip this batch 327! (Loss: inf)
Skip this batch 328! (Loss: inf)
Skip this batch 329! (Loss: inf)
Skip this batch 330! (Loss: inf)
Skip this batch 331! (Loss: inf)
Skip this batch 332! (Loss: inf)
Skip this batch 333! (Loss: inf)
Skip this batch 334! (Loss: inf)
Skip this batch 335! (Loss: inf)
Skip this batch 336! (Loss: inf)
Skip this batch 337! (Loss: inf)
Skip this batch 338! (Loss: inf)
Skip this batch 339! (Loss: inf)
Skip this batch 340! (Loss: inf)
Skip this batch 341! (Loss: inf)
Skip this batch 342! (Loss: inf)
Skip this batch 343! (Loss: inf)
Skip this batch 344! (Loss: inf)
Skip this batch 345! (Loss: inf)
Skip this batch 346! (Loss: inf)
Skip this batch 347! (Loss: inf)
Skip this batch 348! (Loss: inf)
Skip this batch 349! (Loss: inf)
Skip this batch 350! (Loss: inf)
Skip this batch 351! (Loss: inf)
Skip this batch 352! (Loss: inf)
Skip this batch 353! (Loss: inf)
Skip this batch 354! (Loss: inf)
Skip this batch 355! (Loss: inf)
Skip this batch 356! (Loss: inf)
Skip this batch 357! (Loss: inf)
Skip this batch 358! (Loss: inf)
Skip this batch 359! (Loss: inf)
Skip this batch 360! (Loss: inf)
Skip this batch 361! (Loss: inf)
Skip this batch 362! (Loss: inf)
Skip this batch 363! (Loss: inf)
Skip this batch 364! (Loss: inf)
Skip this batch 365! (Loss: inf)
Skip this batch 366! (Loss: inf)
Skip this batch 367! (Loss: inf)
Skip this batch 368! (Loss: inf)
Skip this batch 369! (Loss: inf)
Skip this batch 370! (Loss: inf)
Skip this batch 371! (Loss: inf)
Skip this batch 372! (Loss: inf)
Skip this batch 373! (Loss: inf)
Skip this batch 374! (Loss: inf)
Skip this batch 375! (Loss: inf)
Skip this batch 376! (Loss: inf)
Skip this batch 377! (Loss: inf)
Skip this batch 378! (Loss: inf)
Skip this batch 379! (Loss: inf)
Skip this batch 380! (Loss: inf)
Skip this batch 381! (Loss: inf)
Skip this batch 382! (Loss: inf)
Skip this batch 383! (Loss: inf)
Skip this batch 384! (Loss: inf)
Skip this batch 385! (Loss: inf)
Skip this batch 386! (Loss: inf)
Skip this batch 387! (Loss: inf)
Skip this batch 388! (Loss: inf)
Skip this batch 389! (Loss: inf)
Skip this batch 390! (Loss: inf)
Skip this batch 391! (Loss: inf)
Skip this batch 392! (Loss: inf)
Skip this batch 393! (Loss: inf)
Skip this batch 394! (Loss: inf)
Skip this batch 395! (Loss: inf)
Skip this batch 396! (Loss: inf)
Skip this batch 397! (Loss: inf)
Skip this batch 398! (Loss: inf)
Skip this batch 399! (Loss: inf)
Skip this batch 400! (Loss: inf)
[6400/16000]	[L1: inf]	 timer_model: 13.62 + timer_data: 0.05s
Skip this batch 401! (Loss: inf)
Skip this batch 402! (Loss: inf)
Skip this batch 403! (Loss: inf)
Skip this batch 404! (Loss: inf)
Skip this batch 405! (Loss: inf)
Skip this batch 406! (Loss: inf)
Skip this batch 407! (Loss: inf)
Skip this batch 408! (Loss: inf)
Skip this batch 409! (Loss: inf)
Skip this batch 410! (Loss: inf)
Skip this batch 411! (Loss: inf)
Skip this batch 412! (Loss: inf)
Skip this batch 413! (Loss: inf)
Skip this batch 414! (Loss: inf)
Skip this batch 415! (Loss: inf)
Skip this batch 416! (Loss: inf)
Skip this batch 417! (Loss: inf)
Skip this batch 418! (Loss: inf)
Skip this batch 419! (Loss: inf)
Skip this batch 420! (Loss: inf)
Skip this batch 421! (Loss: inf)
Skip this batch 422! (Loss: inf)
Skip this batch 423! (Loss: inf)
Skip this batch 424! (Loss: inf)
Skip this batch 425! (Loss: inf)
Skip this batch 426! (Loss: inf)
Skip this batch 427! (Loss: inf)
Skip this batch 428! (Loss: inf)
Skip this batch 429! (Loss: inf)
Skip this batch 430! (Loss: inf)
Skip this batch 431! (Loss: inf)
Skip this batch 432! (Loss: inf)
Skip this batch 433! (Loss: inf)
Skip this batch 434! (Loss: inf)
Skip this batch 435! (Loss: inf)
Skip this batch 436! (Loss: inf)
Skip this batch 437! (Loss: inf)
Skip this batch 438! (Loss: inf)
Skip this batch 439! (Loss: inf)
Skip this batch 440! (Loss: inf)
Skip this batch 441! (Loss: inf)
Skip this batch 442! (Loss: inf)
Skip this batch 443! (Loss: inf)
Skip this batch 444! (Loss: inf)
Skip this batch 445! (Loss: inf)
Skip this batch 446! (Loss: inf)
Skip this batch 447! (Loss: inf)
Skip this batch 448! (Loss: inf)
Skip this batch 449! (Loss: inf)
Skip this batch 450! (Loss: inf)
Skip this batch 451! (Loss: inf)
Skip this batch 452! (Loss: inf)
Skip this batch 453! (Loss: inf)
Skip this batch 454! (Loss: inf)
Skip this batch 455! (Loss: inf)
Skip this batch 456! (Loss: inf)
Skip this batch 457! (Loss: inf)
Skip this batch 458! (Loss: inf)
Skip this batch 459! (Loss: inf)
Skip this batch 460! (Loss: inf)
Skip this batch 461! (Loss: inf)
Skip this batch 462! (Loss: inf)
Skip this batch 463! (Loss: inf)
Skip this batch 464! (Loss: inf)
Skip this batch 465! (Loss: inf)
Skip this batch 466! (Loss: inf)
Skip this batch 467! (Loss: inf)
Skip this batch 468! (Loss: inf)
Skip this batch 469! (Loss: inf)
Skip this batch 470! (Loss: inf)
Skip this batch 471! (Loss: inf)
Skip this batch 472! (Loss: inf)
Skip this batch 473! (Loss: inf)
Skip this batch 474! (Loss: inf)
Skip this batch 475! (Loss: inf)
Skip this batch 476! (Loss: inf)
Skip this batch 477! (Loss: inf)
Skip this batch 478! (Loss: inf)
Skip this batch 479! (Loss: inf)
Skip this batch 480! (Loss: inf)
Skip this batch 481! (Loss: inf)
Skip this batch 482! (Loss: inf)
Skip this batch 483! (Loss: inf)
Skip this batch 484! (Loss: inf)
Skip this batch 485! (Loss: inf)
Skip this batch 486! (Loss: inf)
Skip this batch 487! (Loss: inf)
Skip this batch 488! (Loss: inf)
Skip this batch 489! (Loss: inf)
Skip this batch 490! (Loss: inf)
Skip this batch 491! (Loss: inf)
Skip this batch 492! (Loss: inf)
Skip this batch 493! (Loss: inf)
Skip this batch 494! (Loss: inf)
Skip this batch 495! (Loss: inf)
Skip this batch 496! (Loss: inf)
Skip this batch 497! (Loss: inf)
Skip this batch 498! (Loss: inf)
Skip this batch 499! (Loss: inf)
Skip this batch 500! (Loss: inf)
[8000/16000]	[L1: inf]	 timer_model: 13.68 + timer_data: 0.06s
Skip this batch 501! (Loss: inf)
Skip this batch 502! (Loss: inf)
Skip this batch 503! (Loss: inf)
Skip this batch 504! (Loss: inf)
Skip this batch 505! (Loss: inf)
Skip this batch 506! (Loss: inf)
Skip this batch 507! (Loss: inf)
Skip this batch 508! (Loss: inf)
Skip this batch 509! (Loss: inf)
Skip this batch 510! (Loss: inf)
Skip this batch 511! (Loss: inf)
Skip this batch 512! (Loss: inf)
Skip this batch 513! (Loss: inf)
Skip this batch 514! (Loss: inf)
Skip this batch 515! (Loss: inf)
Skip this batch 516! (Loss: inf)
Skip this batch 517! (Loss: inf)
Skip this batch 518! (Loss: inf)
Skip this batch 519! (Loss: inf)
Skip this batch 520! (Loss: inf)
Skip this batch 521! (Loss: inf)
Skip this batch 522! (Loss: inf)
Skip this batch 523! (Loss: inf)
Skip this batch 524! (Loss: inf)
Skip this batch 525! (Loss: inf)
Skip this batch 526! (Loss: inf)
Skip this batch 527! (Loss: inf)
Skip this batch 528! (Loss: inf)
Skip this batch 529! (Loss: inf)
Skip this batch 530! (Loss: inf)
Skip this batch 531! (Loss: inf)
Skip this batch 532! (Loss: inf)
Skip this batch 533! (Loss: inf)
Skip this batch 534! (Loss: inf)
Skip this batch 535! (Loss: inf)
Skip this batch 536! (Loss: inf)
Skip this batch 537! (Loss: inf)
Skip this batch 538! (Loss: inf)
Skip this batch 539! (Loss: inf)
Skip this batch 540! (Loss: inf)
Skip this batch 541! (Loss: inf)
Skip this batch 542! (Loss: inf)
Skip this batch 543! (Loss: inf)
Skip this batch 544! (Loss: inf)
Skip this batch 545! (Loss: inf)
Skip this batch 546! (Loss: inf)
Skip this batch 547! (Loss: inf)
Skip this batch 548! (Loss: inf)
Skip this batch 549! (Loss: inf)
Skip this batch 550! (Loss: inf)
Skip this batch 551! (Loss: inf)
Skip this batch 552! (Loss: inf)
Skip this batch 553! (Loss: inf)
Skip this batch 554! (Loss: inf)
Skip this batch 555! (Loss: inf)
Skip this batch 556! (Loss: inf)
Skip this batch 557! (Loss: inf)
Skip this batch 558! (Loss: inf)
Skip this batch 559! (Loss: inf)
Skip this batch 560! (Loss: inf)
Skip this batch 561! (Loss: inf)
Skip this batch 562! (Loss: inf)
Skip this batch 563! (Loss: inf)
Skip this batch 564! (Loss: inf)
Skip this batch 565! (Loss: inf)
Skip this batch 566! (Loss: inf)
Skip this batch 567! (Loss: inf)
Skip this batch 568! (Loss: inf)
Skip this batch 569! (Loss: inf)
Skip this batch 570! (Loss: inf)
Skip this batch 571! (Loss: inf)
Skip this batch 572! (Loss: inf)
Skip this batch 573! (Loss: inf)
Skip this batch 574! (Loss: inf)
Skip this batch 575! (Loss: inf)
Skip this batch 576! (Loss: inf)
Skip this batch 577! (Loss: inf)
Skip this batch 578! (Loss: inf)
Skip this batch 579! (Loss: inf)
Skip this batch 580! (Loss: inf)
Skip this batch 581! (Loss: inf)
Skip this batch 582! (Loss: inf)
Skip this batch 583! (Loss: inf)
Skip this batch 584! (Loss: inf)
Skip this batch 585! (Loss: inf)
Skip this batch 586! (Loss: inf)
Skip this batch 587! (Loss: inf)
Skip this batch 588! (Loss: inf)
Skip this batch 589! (Loss: inf)
Skip this batch 590! (Loss: inf)
Skip this batch 591! (Loss: inf)
Skip this batch 592! (Loss: inf)
Skip this batch 593! (Loss: inf)
Skip this batch 594! (Loss: inf)
Skip this batch 595! (Loss: inf)
Skip this batch 596! (Loss: inf)
Skip this batch 597! (Loss: inf)
Skip this batch 598! (Loss: inf)
Skip this batch 599! (Loss: inf)
Skip this batch 600! (Loss: inf)
[9600/16000]	[L1: inf]	 timer_model: 13.59 + timer_data: 0.06s
Skip this batch 601! (Loss: inf)
Skip this batch 602! (Loss: inf)
Skip this batch 603! (Loss: inf)
Skip this batch 604! (Loss: inf)
Skip this batch 605! (Loss: inf)
Skip this batch 606! (Loss: inf)
Skip this batch 607! (Loss: inf)
Skip this batch 608! (Loss: inf)
Skip this batch 609! (Loss: inf)
Skip this batch 610! (Loss: inf)
Skip this batch 611! (Loss: inf)
Skip this batch 612! (Loss: inf)
Skip this batch 613! (Loss: inf)
Skip this batch 614! (Loss: inf)
Skip this batch 615! (Loss: inf)
Skip this batch 616! (Loss: inf)
Skip this batch 617! (Loss: inf)
Skip this batch 618! (Loss: inf)
Skip this batch 619! (Loss: inf)
Skip this batch 620! (Loss: inf)
Skip this batch 621! (Loss: inf)
Skip this batch 622! (Loss: inf)
Skip this batch 623! (Loss: inf)
Skip this batch 624! (Loss: inf)
Skip this batch 625! (Loss: inf)
Skip this batch 626! (Loss: inf)
Skip this batch 627! (Loss: inf)
Skip this batch 628! (Loss: inf)
Skip this batch 629! (Loss: inf)
Skip this batch 630! (Loss: inf)
Skip this batch 631! (Loss: inf)
Skip this batch 632! (Loss: inf)
Skip this batch 633! (Loss: inf)
Skip this batch 634! (Loss: inf)
Skip this batch 635! (Loss: inf)
Skip this batch 636! (Loss: inf)
Skip this batch 637! (Loss: inf)
Skip this batch 638! (Loss: inf)
Skip this batch 639! (Loss: inf)
Skip this batch 640! (Loss: inf)
Skip this batch 641! (Loss: inf)
Skip this batch 642! (Loss: inf)
Skip this batch 643! (Loss: inf)
Skip this batch 644! (Loss: inf)
Skip this batch 645! (Loss: inf)
Skip this batch 646! (Loss: inf)
Skip this batch 647! (Loss: inf)
Skip this batch 648! (Loss: inf)
Skip this batch 649! (Loss: inf)
Skip this batch 650! (Loss: inf)
Skip this batch 651! (Loss: inf)
Skip this batch 652! (Loss: inf)
Skip this batch 653! (Loss: inf)
Skip this batch 654! (Loss: inf)
Skip this batch 655! (Loss: inf)
Skip this batch 656! (Loss: inf)
Skip this batch 657! (Loss: inf)
Skip this batch 658! (Loss: inf)
Skip this batch 659! (Loss: inf)
Skip this batch 660! (Loss: inf)
Skip this batch 661! (Loss: inf)
Skip this batch 662! (Loss: inf)
Skip this batch 663! (Loss: inf)
Skip this batch 664! (Loss: inf)
Skip this batch 665! (Loss: inf)
Skip this batch 666! (Loss: inf)
Skip this batch 667! (Loss: inf)
Skip this batch 668! (Loss: inf)
Skip this batch 669! (Loss: inf)
Skip this batch 670! (Loss: inf)
Skip this batch 671! (Loss: inf)
Skip this batch 672! (Loss: inf)
Skip this batch 673! (Loss: inf)
Skip this batch 674! (Loss: inf)
Skip this batch 675! (Loss: inf)
Skip this batch 676! (Loss: inf)
Skip this batch 677! (Loss: inf)
Skip this batch 678! (Loss: inf)
Skip this batch 679! (Loss: inf)
Skip this batch 680! (Loss: inf)
Skip this batch 681! (Loss: inf)
Skip this batch 682! (Loss: inf)
Skip this batch 683! (Loss: inf)
Skip this batch 684! (Loss: inf)
Skip this batch 685! (Loss: inf)
Skip this batch 686! (Loss: inf)
Skip this batch 687! (Loss: inf)
Skip this batch 688! (Loss: inf)
Skip this batch 689! (Loss: inf)
Skip this batch 690! (Loss: inf)
Skip this batch 691! (Loss: inf)
Skip this batch 692! (Loss: inf)
Skip this batch 693! (Loss: inf)
Skip this batch 694! (Loss: inf)
Skip this batch 695! (Loss: inf)
Skip this batch 696! (Loss: inf)
Skip this batch 697! (Loss: inf)
Skip this batch 698! (Loss: inf)
Skip this batch 699! (Loss: inf)
Skip this batch 700! (Loss: inf)
[11200/16000]	[L1: inf]	 timer_model: 13.87 + timer_data: 0.05s
Skip this batch 701! (Loss: inf)
Skip this batch 702! (Loss: inf)
Skip this batch 703! (Loss: inf)
Skip this batch 704! (Loss: inf)
Skip this batch 705! (Loss: inf)
Skip this batch 706! (Loss: inf)
Skip this batch 707! (Loss: inf)
Skip this batch 708! (Loss: inf)
Skip this batch 709! (Loss: inf)
Skip this batch 710! (Loss: inf)
Skip this batch 711! (Loss: inf)
Skip this batch 712! (Loss: inf)
Skip this batch 713! (Loss: inf)
Skip this batch 714! (Loss: inf)
Skip this batch 715! (Loss: inf)
Skip this batch 716! (Loss: inf)
Skip this batch 717! (Loss: inf)
Skip this batch 718! (Loss: inf)
Skip this batch 719! (Loss: inf)
Skip this batch 720! (Loss: inf)
Skip this batch 721! (Loss: inf)
Skip this batch 722! (Loss: inf)
Skip this batch 723! (Loss: inf)
Skip this batch 724! (Loss: inf)
Skip this batch 725! (Loss: inf)
Skip this batch 726! (Loss: inf)
Skip this batch 727! (Loss: inf)
Skip this batch 728! (Loss: inf)
Skip this batch 729! (Loss: inf)
Skip this batch 730! (Loss: inf)
Skip this batch 731! (Loss: inf)
Skip this batch 732! (Loss: inf)
Skip this batch 733! (Loss: inf)
Skip this batch 734! (Loss: inf)
Skip this batch 735! (Loss: inf)
Skip this batch 736! (Loss: inf)
Skip this batch 737! (Loss: inf)
Skip this batch 738! (Loss: inf)
Skip this batch 739! (Loss: inf)
Skip this batch 740! (Loss: inf)
Skip this batch 741! (Loss: inf)
Skip this batch 742! (Loss: inf)
Skip this batch 743! (Loss: inf)
Skip this batch 744! (Loss: inf)
Skip this batch 745! (Loss: inf)
Skip this batch 746! (Loss: inf)
Skip this batch 747! (Loss: inf)
Skip this batch 748! (Loss: inf)
Skip this batch 749! (Loss: inf)
Skip this batch 750! (Loss: inf)
Skip this batch 751! (Loss: inf)
Skip this batch 752! (Loss: inf)
Skip this batch 753! (Loss: inf)
Skip this batch 754! (Loss: inf)
Skip this batch 755! (Loss: inf)
Skip this batch 756! (Loss: inf)
Skip this batch 757! (Loss: inf)
Skip this batch 758! (Loss: inf)
Skip this batch 759! (Loss: inf)
Skip this batch 760! (Loss: inf)
Skip this batch 761! (Loss: inf)
Skip this batch 762! (Loss: inf)
Skip this batch 763! (Loss: inf)
Skip this batch 764! (Loss: inf)
Skip this batch 765! (Loss: inf)
Skip this batch 766! (Loss: inf)
Skip this batch 767! (Loss: inf)
Skip this batch 768! (Loss: inf)
Skip this batch 769! (Loss: inf)
Skip this batch 770! (Loss: inf)
Skip this batch 771! (Loss: inf)
Skip this batch 772! (Loss: inf)
Skip this batch 773! (Loss: inf)
Skip this batch 774! (Loss: inf)
Skip this batch 775! (Loss: inf)
Skip this batch 776! (Loss: inf)
Skip this batch 777! (Loss: inf)
Skip this batch 778! (Loss: inf)
Skip this batch 779! (Loss: inf)
Skip this batch 780! (Loss: inf)
Skip this batch 781! (Loss: inf)
Skip this batch 782! (Loss: inf)
Skip this batch 783! (Loss: inf)
Skip this batch 784! (Loss: inf)
Skip this batch 785! (Loss: inf)
Skip this batch 786! (Loss: inf)
Skip this batch 787! (Loss: inf)
Skip this batch 788! (Loss: inf)
Skip this batch 789! (Loss: inf)
Skip this batch 790! (Loss: inf)
Skip this batch 791! (Loss: inf)
Skip this batch 792! (Loss: inf)
Skip this batch 793! (Loss: inf)
Skip this batch 794! (Loss: inf)
Skip this batch 795! (Loss: inf)
Skip this batch 796! (Loss: inf)
Skip this batch 797! (Loss: inf)
Skip this batch 798! (Loss: inf)
Skip this batch 799! (Loss: inf)
Skip this batch 800! (Loss: inf)
[12800/16000]	[L1: inf]	 timer_model: 13.58 + timer_data: 0.05s
Skip this batch 801! (Loss: inf)
Skip this batch 802! (Loss: inf)
Skip this batch 803! (Loss: inf)
Skip this batch 804! (Loss: inf)
Skip this batch 805! (Loss: inf)
Skip this batch 806! (Loss: inf)
Skip this batch 807! (Loss: inf)
Skip this batch 808! (Loss: inf)
Skip this batch 809! (Loss: inf)
Skip this batch 810! (Loss: inf)
Skip this batch 811! (Loss: inf)
Skip this batch 812! (Loss: inf)
Skip this batch 813! (Loss: inf)
Skip this batch 814! (Loss: inf)
Skip this batch 815! (Loss: inf)
Skip this batch 816! (Loss: inf)
Skip this batch 817! (Loss: inf)
Skip this batch 818! (Loss: inf)
Skip this batch 819! (Loss: inf)
Skip this batch 820! (Loss: inf)
Skip this batch 821! (Loss: inf)
Skip this batch 822! (Loss: inf)
Skip this batch 823! (Loss: inf)
Skip this batch 824! (Loss: inf)
Skip this batch 825! (Loss: inf)
Skip this batch 826! (Loss: inf)
Skip this batch 827! (Loss: inf)
Skip this batch 828! (Loss: inf)
Skip this batch 829! (Loss: inf)
Skip this batch 830! (Loss: inf)
Skip this batch 831! (Loss: inf)
Skip this batch 832! (Loss: inf)
Skip this batch 833! (Loss: inf)
Skip this batch 834! (Loss: inf)
Skip this batch 835! (Loss: inf)
Skip this batch 836! (Loss: inf)
Skip this batch 837! (Loss: inf)
Skip this batch 838! (Loss: inf)
Skip this batch 839! (Loss: inf)
Skip this batch 840! (Loss: inf)
Skip this batch 841! (Loss: inf)
Skip this batch 842! (Loss: inf)
Skip this batch 843! (Loss: inf)
Skip this batch 844! (Loss: inf)
Skip this batch 845! (Loss: inf)
Skip this batch 846! (Loss: inf)
Skip this batch 847! (Loss: inf)
Skip this batch 848! (Loss: inf)
Skip this batch 849! (Loss: inf)
Skip this batch 850! (Loss: inf)
Skip this batch 851! (Loss: inf)
Skip this batch 852! (Loss: inf)
Skip this batch 853! (Loss: inf)
Skip this batch 854! (Loss: inf)
Skip this batch 855! (Loss: inf)
Skip this batch 856! (Loss: inf)
Skip this batch 857! (Loss: inf)
Skip this batch 858! (Loss: inf)
Skip this batch 859! (Loss: inf)
Skip this batch 860! (Loss: inf)
Skip this batch 861! (Loss: inf)
Skip this batch 862! (Loss: inf)
Skip this batch 863! (Loss: inf)
Skip this batch 864! (Loss: inf)
Skip this batch 865! (Loss: inf)
Skip this batch 866! (Loss: inf)
Skip this batch 867! (Loss: inf)
Skip this batch 868! (Loss: inf)
Skip this batch 869! (Loss: inf)
Skip this batch 870! (Loss: inf)
Skip this batch 871! (Loss: inf)
Skip this batch 872! (Loss: inf)
Skip this batch 873! (Loss: inf)
Skip this batch 874! (Loss: inf)
Skip this batch 875! (Loss: inf)
Skip this batch 876! (Loss: inf)
Skip this batch 877! (Loss: inf)
Skip this batch 878! (Loss: inf)
Skip this batch 879! (Loss: inf)
Skip this batch 880! (Loss: inf)
Skip this batch 881! (Loss: inf)
Skip this batch 882! (Loss: inf)
Skip this batch 883! (Loss: inf)
Skip this batch 884! (Loss: inf)
Skip this batch 885! (Loss: inf)
Skip this batch 886! (Loss: inf)
Skip this batch 887! (Loss: inf)
Skip this batch 888! (Loss: inf)
Skip this batch 889! (Loss: inf)
Skip this batch 890! (Loss: inf)
Skip this batch 891! (Loss: inf)
Skip this batch 892! (Loss: inf)
Skip this batch 893! (Loss: inf)
Skip this batch 894! (Loss: inf)
Skip this batch 895! (Loss: inf)
Skip this batch 896! (Loss: inf)
Skip this batch 897! (Loss: inf)
Skip this batch 898! (Loss: inf)
Skip this batch 899! (Loss: inf)
Skip this batch 900! (Loss: inf)
[14400/16000]	[L1: inf]	 timer_model: 13.65 + timer_data: 0.07s
Skip this batch 901! (Loss: inf)
Skip this batch 902! (Loss: inf)
Skip this batch 903! (Loss: inf)
Skip this batch 904! (Loss: inf)
Skip this batch 905! (Loss: inf)
Skip this batch 906! (Loss: inf)
Skip this batch 907! (Loss: inf)
Skip this batch 908! (Loss: inf)
Skip this batch 909! (Loss: inf)
Skip this batch 910! (Loss: inf)
Skip this batch 911! (Loss: inf)
Skip this batch 912! (Loss: inf)
Skip this batch 913! (Loss: inf)
Skip this batch 914! (Loss: inf)
Skip this batch 915! (Loss: inf)
Skip this batch 916! (Loss: inf)
Skip this batch 917! (Loss: inf)
Skip this batch 918! (Loss: inf)
Skip this batch 919! (Loss: inf)
Skip this batch 920! (Loss: inf)
Skip this batch 921! (Loss: inf)
Skip this batch 922! (Loss: inf)
Skip this batch 923! (Loss: inf)
Skip this batch 924! (Loss: inf)
Skip this batch 925! (Loss: inf)
Skip this batch 926! (Loss: inf)
Skip this batch 927! (Loss: inf)
Skip this batch 928! (Loss: inf)
Skip this batch 929! (Loss: inf)
Skip this batch 930! (Loss: inf)
Skip this batch 931! (Loss: inf)
Skip this batch 932! (Loss: inf)
Skip this batch 933! (Loss: inf)
Skip this batch 934! (Loss: inf)
Skip this batch 935! (Loss: inf)
Skip this batch 936! (Loss: inf)
Skip this batch 937! (Loss: inf)
Skip this batch 938! (Loss: inf)
Skip this batch 939! (Loss: inf)
Skip this batch 940! (Loss: inf)
Skip this batch 941! (Loss: inf)
Skip this batch 942! (Loss: inf)
Skip this batch 943! (Loss: inf)
Skip this batch 944! (Loss: inf)
Skip this batch 945! (Loss: inf)
Skip this batch 946! (Loss: inf)
Skip this batch 947! (Loss: inf)
Skip this batch 948! (Loss: inf)
Skip this batch 949! (Loss: inf)
Skip this batch 950! (Loss: inf)
Skip this batch 951! (Loss: inf)
Skip this batch 952! (Loss: inf)
Skip this batch 953! (Loss: inf)
Skip this batch 954! (Loss: inf)
Skip this batch 955! (Loss: inf)
Skip this batch 956! (Loss: inf)
Skip this batch 957! (Loss: inf)
Skip this batch 958! (Loss: inf)
Skip this batch 959! (Loss: inf)
Skip this batch 960! (Loss: inf)
Skip this batch 961! (Loss: inf)
Skip this batch 962! (Loss: inf)
Skip this batch 963! (Loss: inf)
Skip this batch 964! (Loss: inf)
Skip this batch 965! (Loss: inf)
Skip this batch 966! (Loss: inf)
Skip this batch 967! (Loss: inf)
Skip this batch 968! (Loss: inf)
Skip this batch 969! (Loss: inf)
Skip this batch 970! (Loss: inf)
Skip this batch 971! (Loss: inf)
Skip this batch 972! (Loss: inf)
Skip this batch 973! (Loss: inf)
Skip this batch 974! (Loss: inf)
Skip this batch 975! (Loss: inf)
Skip this batch 976! (Loss: inf)
Skip this batch 977! (Loss: inf)
Skip this batch 978! (Loss: inf)
Skip this batch 979! (Loss: inf)
Skip this batch 980! (Loss: inf)
Skip this batch 981! (Loss: inf)
Skip this batch 982! (Loss: inf)
Skip this batch 983! (Loss: inf)
Skip this batch 984! (Loss: inf)
Skip this batch 985! (Loss: inf)
Skip this batch 986! (Loss: inf)
Skip this batch 987! (Loss: inf)
Skip this batch 988! (Loss: inf)
Skip this batch 989! (Loss: inf)
Skip this batch 990! (Loss: inf)
Skip this batch 991! (Loss: inf)
Skip this batch 992! (Loss: inf)
Skip this batch 993! (Loss: inf)
Skip this batch 994! (Loss: inf)
Skip this batch 995! (Loss: inf)
Skip this batch 996! (Loss: inf)
Skip this batch 997! (Loss: inf)
Skip this batch 998! (Loss: inf)
Skip this batch 999! (Loss: inf)
Skip this batch 1000! (Loss: inf)
[16000/16000]	[L1: inf]	 timer_model: 13.63 + timer_data: 0.07s

Evaluation:
[Set5 x2]	PSNR: nan (Best: nan @epoch 94)
Total time: 2.77s
 Now time: Sat Mar 20 18:40:21 2021

2021-03-20 18:40:22  [Epoch 116]	Learning rate: 1.00e-4
Current value of gamma:	CSA.gamma	tensor([1.], device='cuda:0')
Current value of gamma:	LA.gamma	tensor([1.], device='cuda:0')
